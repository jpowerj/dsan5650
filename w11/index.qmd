---
title: "Week 11: Sensitivity Analysis"
subtitle: "*DSAN 5650: Causal Inference for Computational Social Science*<br><span class='subsubtitle'>Summer 2025, Georgetown University</span>"
author: "Jeff Jacobs"
institute: "[`jj1088@georgetown.edu`](mailto:jj1088@georgetown.edu)"
bibliography: "../_DSAN5650.bib"
date: 2025-07-30
date-format: full
lecnum: 11
categories:
  - "Class Sessions"
tbl-cap-location: bottom
crossref:
  fig-title: "Fig"
  title-delim: ". "
format:
  revealjs:
    df-print: kable
    footer: "DSAN 5650 Week 11: {{< var w11.footer >}}"
    output-file: "slides.html"
    html-math-method: mathjax
    scrollable: true
    link-external-icon: true
    link-external-newwindow: true
    echo: true
    code-fold: true
    theme: [default, "../dsan-globals/jjquarto.scss"]
    include-in-header:
      text: "<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css'><link rel='stylesheet' type='text/css' href='https://cdn.jsdelivr.net/gh/dreampulse/computer-modern-web-font@master/fonts.css'>"
    slide-number: true
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar'><span style='position: absolute; left: 8; padding-left: 8px;'><a href='./index.html'>&larr; Return to Notes</a></span><ul class='menu'></ul></div>"
      scale: 0.5
    revealjs-plugins:
      - simplemenu
  html:
    df-print: kable
    output-file: "index.html"
    html-math-method: mathjax
    link-external-icon: true
    link-external-newwindow: true
    echo: true
    code-fold: true
    include-in-header:
      text: "<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css'><link rel='stylesheet' type='text/css' href='https://cdn.jsdelivr.net/gh/dreampulse/computer-modern-web-font@master/fonts.css'>"
---

::: {.content-visible unless-format="revealjs"}

<center>
<a class="h2" href="./slides.html" target="_blank">Open slides in new window &rarr;</a>
</center>

:::

# Final Project Drafts: Friday, 5:59pm EDT {data-name="Final Projects"}

* On Canvas!
* Think of it as a **skeleton** of the final project
* Final submission = draft + missing pieces filled in

# Topic Models Over Space and Time {data-stack-name="Topic Models"}

## Topic Models {.smaller .crunch-title .crunch-ul .table-90}

* Intuition is just: let's model **latent topics** "underlying" **observed words**

| Section | Keywords |
| - | - |
| U.S. News | state, court, federal, republican |
| World News | government, country, officials, minister |
| Arts | music, show, art, dance |
| Sports | game, league, team, coach |
| Real Estate | home, bedrooms, bathrooms, building |

* Already, by just classifying articles based on these keyword counts:

| | Arts | Real Estate | Sports | U.S. News | World News | Total |
|-:|:-:|:-:|:-:|:-:|:-:|
| **Correct** | 3020 | 690 | 4860 | 1330 | 1730 | 11630 |
| **Incorrect** | 750 | 60 | 370 | 1100 | 590 | 2870 |
| **Accuracy** | 0.801 | 0.920 | 0.929 | 0.547 | 0.746 | 0.802 |

## Topic Models as PGMs {.smaller .crunch-title .crunch-img}

![From @blei_probabilistic_2012](images/lda.jpg){fig-align="center"}

<center>

*...Unlocks a world of social modeling through text!*

</center>

## Cross-Sectional Analysis

@blaydes_mirrors_2018

## Influence Over Time

![From @barron_individuals_2018](images/novelty_transience.png){fig-align="center"}

## Textual Influence Over Time

![](images/novelty-resonance-marxism.svg){fig-align="center"}

# Sensitivity Analysis 1: Sensitivity to Priors {data-stack-name="Sensitivity to Priors"}

```{python}
#| label: py-imports
#| fig-align: center
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
cb_palette = ['#e69f00','#56b4e9','#009e73']
sns.set_palette(cb_palette)
import patchworklib as pw
import pymc as pm
import arviz as az
def draw_prior_sample(model, return_idata=False):
  with model:
    prior_idata = pm.sample_prior_predictive(draws=10_000, random_seed=5650)
  prior_df = prior_idata.prior.to_dataframe().reset_index().drop(columns='chain')
  if return_idata:
    return prior_idata, prior_df
  return prior_df
# Plotting function
def gen_dist_plot(dist_df, plot_title, size='third', custom_ylim=None):
  plot_width = 2
  if size == 'quarter':
    plot_width = 1.5
  plot_height = 0.625 * plot_width
  plot_figsize = (plot_width, plot_height)
  ax = pw.Brick(figsize=plot_figsize)
  sns.histplot(
    x="p_heads", data=dist_df, ax=ax,
    bins=25, alpha=0.8, stat='probability'
  );
  ax.set_xlim(0, 1);
  if custom_ylim is not None:
    ax.set_ylim(custom_ylim)
  ax.set_title(plot_title)
  return ax
```

## The Beta Distribution $\text{Beta}(\alpha, \beta)$: The "Workhorse" Prior! {.smaller .title-09 .crunch-title .crunch-quarto-figure .crunch-ul .crunch-img .text-60}

<center style='margin: 5px;'>

[**Biased coin** framing: $\alpha$ is "pseudo-count" of **heads**, $\beta$ = "pseudo-count" of **tails**]{style="border: 1px solid black; padding: 5px;"}

</center>

:::: {.columns}
::: {.column width="33%"}

<center>

<i class='bi bi-1-circle'></i> $p \sim$ "$\text{Beta}(0, 0)$"

</center>

```{python}
#| label: beta00-plot
#| fig-align: center
# b00_df = pd.DataFrame({'p_heads': np.arange(0, 1+1/25, 1/25)})
# b00_df['Probability'] = 0
ax = pw.Brick(figsize=(2, 1.25))
sns.histplot(
  x=[], ax=ax, # data=b00_df
);
ax.set_xlabel('p_heads');
ax.set_ylabel('Probability')
ax.set_xticks([0, 0.25, 0.5, 0.75, 1]);
ax.set_ylim(0, 1);
ax.savefig()
```

:::
::: {.column width="33%"}

<center>

<i class='bi bi-2-circle'></i> $p \sim$ "$\text{Beta}(1, 0)$"

</center>

```{python}
#| label: beta10-plot
#| fig-align: center
b10_df = pd.DataFrame({
  'draw': [0],
  'p_heads': [0],
})
ax = pw.Brick(figsize=(2, 1.25))
sns.histplot(
  x='p_heads', stat='probability', ax=ax,
  bins=25,
  data=b10_df
);
ax.set_xlim(0, 1);
ax.set_ylim(0, 1);
ax.savefig()
```

:::
::: {.column width="33%"}

<center>

<i class='bi bi-3-circle'></i> $p \sim$ "$\text{Beta}(0, 1)$"

</center>

```{python}
#| label: beta01-plot
#| fig-align: center
b01_df = pd.DataFrame({
  'draw': [0],
  'p_heads': [1],
})
ax = pw.Brick(figsize=(2, 1.25))
sns.histplot(
  x='p_heads', stat='probability', ax=ax,
  bins=25,
  data=b01_df
);
ax.set_xlim(0, 1);
ax.set_ylim(0, 1);
ax.savefig()
```

:::
::::

:::: {.columns}
::: {.column width="33%"}

<center>
<i class='bi bi-4-circle'></i> **Informative** Prior
</center>

```{python}
#| label: fig-informative-prior
#| crop: false
#| fig-align: center
#| fig-cap: "*\"I'll start by assuming a **fair** coin\"*"
# Informative Prior
with pm.Model() as informative_model:
  p_heads = pm.Beta("p_heads", alpha=2, beta=2)
  result = pm.Bernoulli("result", p=p_heads)
informative_df = draw_prior_sample(informative_model)
informative_plot = gen_dist_plot(informative_df, "Beta(2, 2) Prior on p");
informative_plot.savefig()
```

:::
::: {.column width="33%"}

<center>
<i class='bi bi-5-circle'></i> **Weak** Prior
</center>

```{python}
#| label: fig-weak-prior
#| fig-align: center
#| fig-cap: "*\"I have no idea, I'll assume all biases equally likely\"*"
# Weakly Informative
with pm.Model() as weak_model:
  p_heads = pm.Beta("p_heads", alpha=1, beta=1)
  result = pm.Bernoulli("result", p=p_heads)
weak_df = draw_prior_sample(weak_model)
weak_plot = gen_dist_plot(weak_df, "Beta(1, 1) Prior on p");
weak_plot.savefig()
```

:::
::: {.column width="33%"}

<center>
<i class='bi bi-6-circle'></i> **Skeptical** (Jeffreys) Prior
</center>

```{python}
#| label: fig-skeptical-prior
#| fig-align: center
#| fig-cap: "*\"I don't trust this coin, it'll take lots of flips with H/T balance to convince me it's fair!\"*"
# Skeptical / Jeffreys
with pm.Model() as skeptical_model:
  p_heads = pm.Beta("p_heads", alpha=0.5, beta=0.5)
  result = pm.Bernoulli("result", p=p_heads)
skeptical_df = draw_prior_sample(skeptical_model)
skeptical_plot = gen_dist_plot(skeptical_df, "Beta(0.5, 0.5) Prior on p");
# And combine
skeptical_plot.savefig()
```

:::
::::

## Modeling *Domain Knowledge* with Priors {.smaller .crunch-title .title-12}

**Population proportion** framing: $\frac{\alpha}{\alpha + \beta}$ = mean, $\alpha + \beta$ = "precision"

:::: {.columns}
::: {.column width="50%"}

```{python}
#| label: heads-more-likely
#| fig-align: center
with pm.Model() as heads_model:
  p_heads = pm.Beta("p_heads", alpha=2, beta=1)
  result = pm.Bernoulli("result", p=p_heads)
heads_df = draw_prior_sample(heads_model)
heads_plot = gen_dist_plot(heads_df, "Beta(2, 1) Prior on p");
heads_plot.savefig()
```

:::
::: {.column width="50%"}

```{python}
#| label: tails-more-likely
#| fig-align: center
with pm.Model() as tails_model:
  p_heads = pm.Beta("p_heads", alpha=1, beta=2)
  result = pm.Bernoulli("result", p=p_heads)
tails_df = draw_prior_sample(tails_model)
tails_plot = gen_dist_plot(tails_df, "Beta(1, 2) Prior on p");
tails_plot.savefig()
```

:::
::::

:::: {.columns}
::: {.column width="50%"}

```{python}
#| label: tails-very-likely
#| fig-align: center
with pm.Model() as beta23_model:
  p_heads = pm.Beta("p_heads", alpha=2, beta=3)
  result = pm.Bernoulli("result", p=p_heads)
beta23_df = draw_prior_sample(beta23_model)
beta23_plot = gen_dist_plot(beta23_df, "Beta(2, 3) Prior on p");
beta23_plot.savefig()
```

:::
::: {.column width="50%"}

```{python}
#| label: beta2030-model
#| fig-align: center
with pm.Model() as beta100_model:
  p_heads = pm.Beta("p_heads", alpha=40, beta=60)
  result = pm.Bernoulli("result", p=p_heads)
beta100_df = draw_prior_sample(beta100_model)
beta100_plot = gen_dist_plot(beta100_df, "Beta(40, 60) Prior on p");
beta100_plot.savefig()
```

:::
::::

# Sensitivity Analysis 2: How Sensitive Are My Results to *Omitted/Included Variable Bias*? {.smaller .title-10 .crunch-title data-stack-name="Included Variable Bias"}

* You probably know about **omitted variable bias** from previous classes / intuition... but, **included variable bias?**
* That's right folks... **colliders** can be agents of chaos, laying waste to our best, most meticulously-planned-out models

![](images/mood_crop.jpg){fig-align="center"}

## Does Aging Cause Sadness? {.smaller .crunch-title .crunch-ul .crunch-quarto-figure .table-80 .crunch-img .crunch-quarto-layout-panel .crunch-quarto-layout-cell .crunch-figure .crunch-figcaption .inline-70}

```{=html}
<table>
<thead>
</thead>
<tbody>
<tr>
  <td width="10%"><span data-qmd="<i class='bi bi-1-circle'></i>"></span></td>
  <td width="90%">Each year, 20 people are born with uniformly-distributed happiness values</td>
</tr>
<tr>
  <td><span data-qmd="<i class='bi bi-2-circle'></i>"></span></td>
  <td>Each year, each person ages one year; Happiness does not change</td>
</tr>
<tr>
  <td><span data-qmd="<i class='bi bi-3-circle'></i>"></span></td>
  <td>At age 18, individuals can become married; Odds of marriage each year proportional to individual's happiness; Once married, they remain married</td>
</tr>
<tr>
  <td><span data-qmd="<i class='bi bi-4-circle'></i>"></span></td>
  <td>At age 70 individuals leave the sample (They move to Boca Raton, Florida)</td>
</tr>
</tbody>
</table>
```

:::: {layout="[65,35]" layout-valign="center"}
::: {#collider-plot}

```{python}
#| label: fig-happiness-plot
#| fig-align: center
#| fig-cap: "Happiness by Age"
import numpy as np
rng = np.random.default_rng(seed=5650)
from scipy.special import expit

# The original R code:
# sim_happiness <- function( seed=1977 , N_years=1000 , max_age=65 , N_births=20 , aom=18 ) {
#     set.seed(seed)
#     H <- M <- A <- c()
#     for ( t in 1:N_years ) {
#         A <- A + 1 # age existing individuals
#         A <- c( A , rep(1,N_births) ) # newborns
#         H <- c( H , seq(from=-2,to=2,length.out=N_births) ) # sim happiness trait - never changes
#         M <- c( M , rep(0,N_births) ) # not yet married
#         # for each person over 17, chance get married
#         for ( i in 1:length(A) ) {
#             if ( A[i] >= aom & M[i]==0 ) {
#                 M[i] <- rbern(1,inv_logit(H[i]-4))
#             }
#         }
#         # mortality
#         deaths <- which( A > max_age )
#         if ( length(deaths)>0 ) {
#             A <- A[ -deaths ]
#             H <- H[ -deaths ]
#             M <- M[ -deaths ]
#        }
#     }
#     d <- data.frame(age=A,married=M,happiness=H)
#     return(d)

# DGP: happiness -> marriage <- age
years = 70
num_births = 41
colnames = ['age','a','h','m']
sim_dfs = []
A = np.zeros(shape=(num_births,1))
H = np.linspace(-2, 2, num=num_births)
M = np.zeros(shape=(num_births,1))
def update_m(row):
  if row['m'] == 0:
    return int(rng.binomial(
      n=1,
      p=expit(row['h'] - 3.875),
      size=1,
    ))
  return 1
def sim_cohort_to(max_age):
  sim_df = pd.DataFrame({
      'age': [1 for _ in range(num_births)],
      'h': np.linspace(-2, 2, num=num_births),
      'm': [0 for _ in range(num_births)],
    }
  )
  for t in range(2, max_age + 1):
    sim_df['age'] = sim_df['age'] + 1
    if t >= 18:
      sim_df['m'] = sim_df.apply(update_m, axis=1)
  return sim_df
all_sim_dfs = []
for cur_max_age in range(1, 71):
  cur_sim_df = sim_cohort_to(cur_max_age)
  all_sim_dfs.append(cur_sim_df)
full_sim_df = pd.concat(all_sim_dfs)

# full_sim_df.head()
cbg_palette = ['#c6c6c666'] + cb_palette
full_sim_df['m_label'] = full_sim_df['m'].apply(lambda x: "Unmarried" if x == 0 else "Married")
full_sim_df = full_sim_df.rename(columns={'age': 'Age', 'h': 'Happiness'})
ax = pw.Brick(figsize=(5.25,2.75));
sns.scatterplot(
  x='Age', y='Happiness', hue='m_label',
  data=full_sim_df,
  ax=ax,
  palette=cbg_palette,
  sizes=2,
  legend=True,
);
ax.legend_.set_title("")
ax.move_legend("upper center", bbox_to_anchor=(0.5, 1.15), ncol=2)
ax.savefig()
```

:::
::: {#collider-text}

```{python}
#| label: fig-mean-happiness-plot
#| fig-align: center
#| fig-cap: "Mean happiness by age x marriage status"
mean_hap_df = full_sim_df.groupby(['Age','m_label'])['Happiness'].mean().reset_index()
ax = pw.Brick(figsize=(3,2.5));
sns.lineplot(
  x='Age', y='Happiness', hue='m_label', data=mean_hap_df,
  ax=ax
);
ax.savefig()
```

<center>

[...What's happening here?]{.text-70}<br>
$\textsf{Happy} \rightarrow {}^{🤔}\textsf{Marriage}^{🤔} \leftarrow \textsf{Age}$

</center>

:::
::::

## References

::: {#refs}
:::
