---
# title: "DSAN 5650"
title: "Syllabus"
subtitle: "*DSAN 5650: Causal Inference for Computational Social Science*"
times: "Wednesdays 6:30-9pm, Online"
author: "Prof. Jeff Jacobs"
email: "jj1088@georgetown.edu"
term: "Summer 2025, Georgetown University"
# latex-auto-install: false
bibliography: "_DSAN5650.bib"
csl: chicago-17-no-url.csl
format:
  html:
    link-external-icon: true
    link-external-newwindow: true
#   pdf:
#     output-file: "DSAN5650_Summer2025_Syllabus.pdf"
#     template-partials:
#       - title.tex
#     title-meta: "DSAN 5650 Syllabus - Summer 2025"
#     documentclass: "scrartcl"
---

**Welcome to *DSAN 5650: Causal Inference for Computational Social Science* at Georgetown University!**

The course meets on **Wednesdays from 6:30-9pm online via [Zoom](https://maps.app.goo.gl/fVVfDFpp4MEuukXa9)**

## Course Staff

* Prof. Jeff Jacobs, [`jj1088@georgetown.edu`](mailto:jj1088@georgetown.edu)
  * Office hours (Click to schedule): [Tuesdays, 3:30-6pm](https://jjacobs.me/meet)
* TA Courtney Green, [`crg123@georgetown.edu`](mailto:crg123@georgetown.edu)
  * Office hours by appointment
* TA Wendy Hu, [`lh1078@georgetown.edu`](mailto:lh1078@georgetown.edu)
  * Office hours by appointment

## Course Description

This course provides students with the opportunity to take the analytical skills, machine learning algorithms, and statistical methods learned throughout their first year in the program and explore how they can be employed to carry out rigorous, original research in the behavioral and social sciences. With a particular emphasis on tackling the additional challenges which arise when moving from associational to causal inference, particularly when only observational (as opposed to experimental) data is available, students will become proficient in cutting-edge causal Machine Learning techniques such as propensity score matching, synthetic controls, causal program evaluation, inverse social welfare function estimation from panel data, and Double-Debiased Machine Learning.

In-class examples will cover continuous, discrete-choice, and textual data from a wide swath of social and behavioral sciences: economics, political science, sociology, anthropology, quantitative history, and digital humanities. After gaining experience through in-class labs and homework assignments focused on reproducing key findings from recent journal articles in each of these disciplines, students will spend the final weeks of the course on a final project demonstrating their ability to develop, evaluate, and test the robustness of a causal hypothesis.

*Prerequisites: DSAN 5000, DSAN 5100 (DSAN 5300 recommended but not required)*

## Course Overview

The fundamental building block for the course is the idea of a **Data-Generating Process (DGP)**. You may have encountered this concept in passing during other DSAN courses (for example, in DSAN 5100, a phrase like "Assume $X$ is drawn i.i.d. from a Normal distribution with mean $\mu$ and variance $\sigma^2$" is a statement characterizing the DGP of a Random Variable $X$), but in this course we will "zoom in" on this concept rather than treating it like a black box or a footnote to e.g. a theorem like the Law of Large Numbers.

This deep dive into DGPs is necessary for us here, since our goal in the course is **to move from *associational* statements** like "an increase of $X$ by one unit is associated with an increase of $Y$ by $\beta$ units" to ***causal* statements** like "increasing $X$ by one unit *causes* $Y$ to increase by $\beta$ units". As you'll see in Week 1, the tools from probability theory and statistics that you learned in DSAN 5100---Random Variables, Cumulative Distribution Functions, Conditional Probability, and so on---are necessary but **not *sufficient*** to analyze data from a causal perspective.

For example, if we use our tools from DSAN 5000 and DSAN 5100 on some dataset to discover that:

* The probability that some event $E_1$ occurs is $\Pr(E_1) = 0.5$, and
* The probability that $E_1$ occurs **conditional on** another event $E_0$ occurring is $\Pr(E_1 \mid E_0) = 0.75$,

we unfortunately cannot infer from these two pieces of information that the occurrence of $E_0$ **causes** an increase in the likelihood of $E_1$ occurring.

This issue (that **conditional probabilities** could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships... In recent decades, however, researchers have built up what amounts to an additional "layer" of modeling tools which augment the existing machinery of probability theory to address causality head-on!^[@pearl_causality_2000 represents a key work in this field of research, as it essentially brought together different pieces of causal models into one unified, rigorous framework.]

For instance, a modeling approach called **"$\textsf{do}$-Calculus"**, that we will learn in this class, *extends* the core operations and definitions of probability theory to allow such a move towards inferring causality! It does this by introducing a $\textsf{do}(\cdot)$ operator that can be applied to Random Variables like $X$, with e.g. $\textsf{do}(X = 5)$ representing the event wherein someone has **intervened** in a **Data-Generating Process** to **force** the value of $X$ to be 5.

With this operator in hand (that is, used alongside an explicit model of a DGP satisfying a set of underlying axioms which are slightly more strict than the axioms of probability theory), it turns out that we *can* make causal inferences using a very similar pair of facts! If we know that:

* The probability that some event $E_1$ occurs is $\Pr(E_1) = 0.5$, and
* The probability that $E_1$ occurs **conditional on** the event $\textsf{do}(E_0)$ occurring is $\Pr(E_1 \mid \textsf{do}(E_0)) = 0.75$,

**now** we can actually draw the inference that the occurrence of $E_0$ **caused** an increase in the likelihood of $E_1$ occurring!

This stylized comparison (between what's possible using "core" probability theory and what's possible when we augment it with additional causal modeling tools) serves as our basic motivation for the course, so that from Week 2 onwards we build upon this foundation to reach the three learning goals described in the next section!

## Main Textbooks / Resources

Unlike the case for topics like calculus or [statistical learning](https://www.statlearning.com/), this field is too new (and exciting! with new methods being developed month-to-month) to have a single set of "established" textbooks. Thus, the main collection of resources (books, papers, and explanatory videos) we'll draw on for this class are available on the [resources page](./resources.qmd). However, there are three "core" textbooks you can draw on which best align with the topics in this course:

* Morgan and Winship, *Counterfactuals and Causal Inference: Methods and Principles for Social Research* [@morgan_counterfactuals_2015] \[[PDF](https://www.dropbox.com/scl/fi/rmuw9yw0gflg9wprufzu4/Counterfactuals-and-Causal-Inference.pdf?rlkey=gpzpuqgmzgsdwo18j22pl6yiw&dl=1)\]
    * The book which comes closest to being an all-encompassing, single textbook for the class. It brings together different "strands" of causal modeling research (since each field---economics, bioinformatics, sociology, etc.---tends to use its own notation and vocabulary), unifying them into a single approach. The only reason we can't use it as *the* main textbook is because it hasn't been updated since 2015, and most of the assignments in this class use computational tools from 2018 onwards!
* Angrist and Pischke, *Mastering 'Metrics: The Path from Cause to Effect* [@angrist_mastering_2014] \[[PDF](https://www.dropbox.com/scl/fi/3xa9tfswmv1w8ez54prtk/Joshua-D.-Angrist-J-rn-Steffen-Pischke-Mastering-Metrics.pdf?rlkey=2w0ipphrgxrejd7ddkv1wd635&dl=1)\]
    * This book is included as the second of the three "core" texts mainly because, it uses the language of causality specific to Econometrics, the language that is most familiar to me from my PhD training in Political Economy. However, if you tend to learn better by example, it also does a good job of foregrounding specific examples (like evaluating charter schools and community policing policies), so that the methods emerging naturally from attempts to solve these puzzles when association methods like linear regression fail to capture their causal linkages.
* Pearl and Mackenzie, *The Book of Why: The New Science of Cause and Effect* [@pearl_book_2018] \[[EPUB](https://www.dropbox.com/scl/fi/0r7kg7hwibztvfnby8vdg/The-Book-of-Why.epub?rlkey=czdiw5mku5uamjdiwyyzlpmdr&dl=1)\]
    * This book contrasts with the Angrist and Pischke book in using the language of causality formed within *Computer Science* rather than Economics. It can be a good starting point especially if you're unfamiliar with the heavy use of *diagrams* for scientific modeling---basically, whereas Angrist and Pischke's first instinct is to use (sometimes informal) *equations* like $y = mx + b$ to explain steps in the procedures, Pearl and Mackenzie's instinct would be to instead use something like $\require{enclose}\enclose{circle}{\kern .01em ~x~\kern .01em} \overset{\small m, b}{\longrightarrow} \enclose{circle}{\kern.01em y~\kern .01em}$ to represent the same concept (in this case, a line with slope $m$ and intercept $b$!).

## Schedule

The following is a rough map of what we will work through together throughout the semester; given that **everyone learns at a different pace**, my aim is to leave us with a good amount of **flexibility** in terms of how much time we spend on each topic: if I find that it takes me longer than a week to convey a certain topic in sufficient depth, for example, then I view it as a strength rather than a weakness of the course that we can then rearrange the calendar below by adding an extra week on that particular topic! Similarly, if it seems like I am spending too much time on a topic, to the point that students seem bored or impatient to move onto the next topic, we can move a topic intended for the next week to the current week!

```{=html}
<table class='sticky-table'>
<thead>
<tr class='sticky-head'>
    <th>Unit</th>
    <th>Week</th>
    <th>Date</th>
    <th>Topic</th>
</tr>
</thead>
<tbody>
<tr>
    <td><span data-qmd="**Unit 1**: The Language of Causal Modeling"></span></td>
    <td>1</td>
    <td><span data-qmd="{{< var w01.date-md >}}"></span></td>
    <td><span data-qmd='[{{< var w01.title >}}](./w01/)<br>**Relevant Supplementary Material:**<br>Santa Fe Institute online course: [*Introduction to Renormalization*](https://www.youtube.com/playlist?list=PLF0b3ThojznTzAA7bfLWh4RKzRrwNF4L0), up through Video 5, "Coarse Graining II: Entropy"^[Challenge yourself to keep watching to the end of the 5th video here, if you can! Even if you feel frustrated/scared! Because, the *examples* (e.g., macro vs. microeconomics) are what we mainly care about here, more so than e.g. the mathematical definition of entropy (which we will go towards at a slower pace!)]'></span></td>
</tr>
<tr>
    <td></td>
    <td>2</td>
    <td><span data-qmd="{{< var w02.date-md >}}"></span></td>
    <td><span data-qmd='[{{< var w02.title >}}](./w02/)<br>**Relevant Reading:**<br>@koller_probabilistic_2009, Chapter 2: "Foundations". *Especially pp. 15-34, as a DSAN 5100 refresher!*'></span></td>
</tr>
<tr class='new-unit'>
    <td><span data-qmd="**Unit 2**: Doin Thangs"></span></td>
    <td>3</td>
    <td><span data-qmd="{{< var w03.date-md >}}"></span></td>
    <td><span data-qmd="[{{< var w03.title >}}](./w03/)<br>*[Deliverable, 11:59pm EDT] HW1: {{< var hw1.title >}}*"></span></td>
</tr>
<tr>
    <td></td>
    <td>4</td>
    <td><span data-qmd="{{< var w04.date-md >}}"></span></td>
    <td><span data-qmd="[{{< var w04.title >}}](./w04/)"></span></td>
</tr>
<tr class='new-unit'>
    <td><span data-qmd="**Unit 3**: Matching Apples to Apples"></span></td>
    <td>5</td>
    <td><span data-qmd="{{< var w05.date-md >}}"></span></td>
    <td><span data-qmd="[{{< var w05.title >}}](./w05/)"></span></td>
</tr>
<tr>
    <td></td>
    <td></td>
    <td><span data-qmd="*Jun 22 (Sunday)*"></span></td>
    <td><span data-qmd="*HW2 Released on JupyterHub*"></span></td>
</tr>
<tr>
    <td></td>
    <td>6</td>
    <td><span data-qmd="{{< var w06.date-md >}}"></span></td>
    <td><span data-qmd="[{{<var w06.title >}}](./w06/)"></span></td>
</tr>
<!-- <tr>
    <td></td>
    <td></td>
    <td><span data-qmd="*Feb 21 (Friday), 5:59pm EST*"></span></td>
    <td><span data-qmd="*[Deliverable] <a href='{{< var hw2.url >}}' target='_blank'>HW2: {{< var hw2.title >}}</a>*"></span></td>
</tr> -->
<tr class='new-unit'>
    <td><span data-qmd="**Midterm Week**"></span></td>
    <td>7</td>
    <td><span data-qmd="*Jul 1 (Tuesday)*"></span></td>
    <td><span data-qmd="*[Deliverable, 11:59pm EDT] HW2: {{< var hw2.title >}}*"></span></td>
</tr>
<tr>
    <td></td>
    <td></td>
    <td><span data-qmd="{{< var w07.date-md >}}"></span></td>
    <td><span data-qmd="[{{< var w07.title >}}](./w07/)<br>*27-Hour Take-Home Midterm released, 9:00pm EDT*"></span></td>
</tr>
<tr>
    <td></td>
    <td></td>
    <td><span data-qmd="*Jul 3 (Thursday)*"></span></td>
    <td><span data-qmd="*[Deliverable, 11:59pm EDT] 27-Hour Take-Home Midterm*"></span></td>
</tr>
<tr class='new-unit'>
    <td><span data-qmd="**Unit 4:** Machine Learning for Causal Inference"></span></td>
    <td>8</td>
    <td><span data-qmd="{{< var w08.date-md >}}"></span></td>
    <td><span data-qmd="[{{< var w08.title >}}](./w08/)"></span></td>
</tr>
<tr>
    <td></td>
    <td>9</td>
    <td><span data-qmd="{{< var w09.date-md >}}"></span></td>
    <td><span data-qmd="[{{< var w09.title >}}](./w09/)"></span></td>
</tr>
<tr>
    <td></td>
    <td>10</td>
    <td><span data-qmd="{{< var w10.date-md >}}"></span></td>
    <td><span data-qmd="[{{< var w10.title >}}](./w10/)"></span></td>
</tr>
<tr>
    <td></td>
    <td>11</td>
    <td><span data-qmd="{{< var w11.date-md >}}"></span></td>
    <td><span data-qmd="[{{< var w11.title >}}](./w11/)"></span></td>
</tr>
<tr class='new-unit'>
    <td><span data-qmd="**Final Project Zone**"></span></td>
    <td>12</td>
    <td><span data-qmd="{{< var w12.date-md >}}"></span></td>
    <td><span data-qmd="{{< var w12.title >}}"></span></td>
</tr>
<tr>
    <td></td>
    <td></td>
    <td><span data-qmd="{{< var final.date-md >}}"></span></td>
    <td><span data-qmd="*[Deliverable] [Final Project](./final.qmd)*"></span></td>
</tr>
</tbody>
</table>
```

## Assignments and Grading

The main assignment in the course will be your **final project**, submitted at the end of the semester. However, there will also be a (virtual) in-class midterm exam and a series of assignments which exist to let you explore each of the modules of the course, in turn.

| Assignment | Due Date | % of Grade |
| - | - | - |
| [HW1: {{< var hw1.title >}}]({{< var hw1.url >}}) | {{< var hw1.date-full >}} | 9% |
| [HW2: {{< var hw2.title >}}]({{< var hw2.url >}}) | {{< var hw2.date-full >}} | 9% |
| [HW3: {{< var hw3.title >}}]({{< var hw3.url >}}) | {{< var hw3.date-full >}} | 9% |
| {{< var midterm.title >}} | {{< var midterm.date-full >}} | 25% |
| [HW4: {{< var hw4.title >}}]({{< var hw4.url >}}) | {{< var hw4.date-full >}} | 9% |
| [HW5: {{< var hw5.title >}}]({{< var hw5.url >}}) | {{< var hw5.date-full >}} | 9% |
| [Final Project](./final.qmd) | {{< var final.date-full >}} | 30% |

: {tbl-colwidths="[55,25,20]"}

### Homework Lateness Policy

After the due date, for each assignment besides the midterm, you will have a grace period of 24 hours to submit the assignment without a lateness penalty. After this 24-hour grace period, late penalties will be applied based on the following scale (unless you obtain an excused lateness from one of the instructional staff!):

* 0 to 24 hours late: no penalty
* 24 to 30 hours late: 2.5% penalty
* 30 to 42 hours late: 5% penalty
* 42 to 54 hours late: 10% penalty
* 54 to 66 hours late: 20% penalty
* More than 66 hours late: Assignment submissions no longer accepted (without instructor approval)
