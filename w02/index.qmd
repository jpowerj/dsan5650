---
title: "Week 2: Probabilistic Graphical Models (PGMs), the Language of DGPs"
subtitle: "*DSAN 5650: Causal Inference for Computational Social Science*<br><span class='subsubtitle'>Summer 2025, Georgetown University</span>"
author: "Jeff Jacobs"
institute: "[`jj1088@georgetown.edu`](mailto:jj1088@georgetown.edu)"
bibliography: "../_DSAN5650.bib"
date: 2025-05-28
date-format: full
lecnum: 2
categories:
  - "Class Sessions"
format:
  revealjs:
    df-print: kable
    footer: "DSAN 5650 Week 2: {{< var w02.footer >}}"
    output-file: "slides.html"
    html-math-method: mathjax
    scrollable: true
    link-external-icon: true
    link-external-newwindow: true
    theme: [default, "../dsan-globals/jjquarto.scss"]
    include-in-header:
      text: "<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css'><link rel='stylesheet' type='text/css' href='https://cdn.jsdelivr.net/gh/dreampulse/computer-modern-web-font@master/fonts.css'>"
    slide-number: true
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar'><span style='position: absolute; left: 8; padding-left: 8px;'><a href='./index.html'>&larr; Return to Notes</a></span><ul class='menu'></ul></div>"
      scale: 0.5
    revealjs-plugins:
      - simplemenu
  html:
    df-print: kable
    output-file: "index.html"
    html-math-method: mathjax
    link-external-icon: true
    link-external-newwindow: true
---

::: {.content-visible unless-format="revealjs"}

<center>
<a class="h2" href="./slides.html" target="_blank">Open slides in new window &rarr;</a>
</center>

:::

# Schedule {.smaller .crunch-title .crunch-callout .code-90 data-name="Schedule"}

Today's Planned Schedule:

| | Start | End | Topic |
|:- |:- |:- |:- |
| **Lecture** | 6:30pm | 6:45pm | [TA Intros &rarr;](#motivation-ii-causal-inference) |
| | 6:45pm | 7:00pm | [HW1 Questions and Concerns &rarr;]()
| | 7:00pm | 7:30pm | [Motivating Examples: Causal Inference &rarr;](#motivation-ii-causal-inference) |
| | 7:30pm | 7:45pm | [Your First Probabilistic Graphical Model! &rarr;](#your-first-probabilistic-graphical-model)
| **Break!** | 7:45pm | 8:00pm | |
| | 8:00pm | 8:30pm | [PGM Nuts and Bolts &rarr;](#pgm-nuts-and-bolts)
| | 8:30pm | 9:00pm | [Course Logistics &rarr;](#course-logistics) |

: {tbl-colwidths="[12,12,12,64]"}

{{< include ../dsan-globals/_globals-py.qmd >}}

{{< include ../dsan-globals/_globals-tex.qmd >}}

# TA Intros {data-stack-name="TA Intros"}

## Courtney Green

## Wendy Hu

# HW1 Questions and/or Concerns {data-stack-name="HW1"}

## Technical Issues (JupyterHub)

## Content Issues

# Motivating Examples: Causal Inference {data-stack-name="Causal Inference"}

* The *methodology* we'll use to *draw inferences* about social phenomena from data

## Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality {.smaller .crunch-title .title-10 .crunch-p .crunch-img}

:::: {.columns}
::: {.column width="50%"}

<i class='bi bi-1-circle'></i> You'll no longer be able to read "scientific" writing without striking this expression (involuntarily):

:::
::: {.column width="50%"}

<i class='bi bi-2-circle'></i> "Scientific" talks will begin to sound like the following:

:::
::::

:::: {layout="[5,5]" layout-valign="default"}
::: {#expression}

![](images/hrngg.png){fig-align="center" width="300"}

:::
::: {#looked-at-the-data}

{{< video https://jpj.georgetown.domains/dsan5650-scratch/looked_at_the_data.mp4 >}}

:::
::::

## Blasting Off Into Causality! {.title-10 .crunch-title .not-title-slide}

![](images/posadism_causal.jpg){fig-align="center"}

## Data-Generating Processes (DGPs) {.title-09 .text-85 .crunch-title .crunch-ul .crunch-quarto-figure .crunch-quarto-layout-panel .crunch-li-5}

:::: {layout="[55,45]" layout-align="center" layout-valign="center"}
::: {#dgps-5100}

* You saw this in DSAN 5100!
* Â«$X_1, \ldots, X_n$ drawn i.i.d. Normal, mean $\mu$ variance $\sigma^2$Â» characterizes **DGP of $(X_1, \ldots, X_n)$**

:::

![](images/hmm_clusters_single_dist.svg){fig-align="center" width="280"}

::::

* 5650: **Dive into DGPs**, rather than treating as black box/footnote to Law of Large Numbers, so we can move [*asymptotically!*]...
* **From *associational* statements**:
  * Â«$\underbrace{\text{An increase}}_{\small\text{noun}}$ in $X$ by 1 is associated with increase in $Y$ by $\beta$Â»
* **To *causal* ones**: Â«$\underbrace{\text{Increasing}}_{\small\text{verb}}$ $X$ by 1 *causes* $Y$ to increase by $\beta$Â»

## DGPs and the Emergence of Order {.crunch-title .crunch-quarto-layout-panel .title-09 .crunch-quarto-figure}

::: {layout="[1,1]"}

* Who can guess the state of this process after 10 steps, with 1 person?
* 10 people? 50? 100? (If they find themselves on the same spot, they stand on each other's heads)
* 100 steps? 1000?

![](images/random_walk.svg){fig-align="center" width="430"}

:::

## The Result: 16 Steps

![](images/random-walk-16-1.png){fig-align="center"}

## The Result: 64 Steps

![](images/random-walk-64-1.png){fig-align="center"}

## "Mathematical/Scientific Modeling" {.smaller .crunch-title .crunch-ul}

* Thing we observe (poking out of water): **data**
* Hidden but possibly discoverable via deeper dive (ecosystem under surface): **DGP**

<!-- My sincere belief and definitely not an image forwarded to me unironically by a family member when I was a tween -->

![](images/fwds_from_grandma.jpg){fig-align="center"}

## So What's the Problem? {.crunch-title .crunch-ul .crunch-li-8 .inline-90}

* Non-probabilistic models: High potential for being garbage
  * *tldr: even if SUPER certain, using $\Pr(\mathcal{H}) = 1-\varepsilon$ with tiny $\varepsilon$ has literal life-saving advantages* [@finetti_probability_1972]
* Probabilistic models: Getting there, still looking at "surface"
  * Of the $N = 100$ times we observed event $X$ occurring, event $Y$ also occurred $90$ of those times
  * $\implies \Pr(Y \mid X) = \frac{\#[X, Y]}{\#[X]} = \frac{90}{100} = 0.9$
* Causal models: Does $Y$ happen **because of** $X$ happening? For that, need to start modeling **what's happening under the surface** making $X$ and $Y$ **"pop up" together** so often

## The *Shallow* Problem of Causal Inference {.title-08}

:::: {.columns}
::: {.column width="65%"}

![](images/ski-revenue-lawyers-1.png){fig-align="center"}

:::
::: {.column width="35%"}

``` {.r style="font-size: 72%;"}
cor(ski_df$value, law_df$value)
```

```
[1] 0.9921178
```

::: {style="font-size: 50% !important;"}
(Data from Vigen, <a href='http://web.archive.org/web/20191006000802/http://tylervigen.com/view_correlation?id=29272' target='_blank'>Spurious Correlations</a>)
:::

This, however, is only a *mini-boss*. Beyond it lies the truly invincible **FINAL BOSS**... ðŸ™€

:::
::::

## The *Fundamental* Problem of Causal Inference {.crunch-title .crunch-ul .crunch-callout .title-07}

The only workable definition of Â«$X$ causes $Y$Â»:

::: {.callout-note icon="false" title="<i class='bi bi-info-circle pe-1'></i> Defining Causality [@hume_treatise_1739, ruining everything as usual ðŸ˜¤]"}

$X$ causes $Y$ if and only if:

1. $X$ *temporally precedes* $Y$ and
2. 
    * In **two worlds** $W_0$ and $W_1$ where
    * everything is exactly the same **except that** $X = 0$ in $W_0$ and $X = 1$ in $W_1$,
    * $Y = 0$ in $W_0$ and $Y = 1$ in $W_1$

:::

* The problem? We live in **one** world, not two identical worlds simultaneously ðŸ˜­

## What Is To Be Done?

![](images/face_everything_and_rise.jpg){fig-align="center"}

## Probability++ {.crunch-title}
  
* Tools from prob/stats (RVs, CDFs, Conditional Probability) **necessary but not sufficient** for causal inference!
* Example: Say we use DSAN 5100 tools to discover:
  * Probability that event $E_1$ occurs is $\Pr(E_1) = 0.5$
  * Probability that $E_1$ occurs **conditional on** another event $E_0$ occurring is $\Pr(E_1 \mid E_0) = 0.75$
* Unfortunately, we still **cannot** infer that the occurrence of $E_0$ **causes** an increase in the likelihood of $E_1$ occurring.

## Beyond Conditional Probability {.crunch-title .text-90}

* This issue (that **conditional probabilities** could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships...
* Recent decades: researchers have built up an additional "layer" of modeling tools, augmenting existing machinery of probability to address causality head-on!
* @pearl_causality_2000: Formal proofs that ($\Pr$ axioms) $\cup$ ($\textsf{do}$ axioms) $\Rightarrow$ causal inference procedures successfully recover causal effects

## Preview: do-Calculus {.crunch-title .crunch-math .text-85 .table-85 .crunch-ul .inline-90 .math-90 .crunch-li-6}

* *Extends* core of probability to incorporate causality, via $\textsf{do}$ operator
* $\textsf{do}(X = 5)$ is a "special" **event**, representing **intervention in DGP** to **force** $X \leftarrow 5$... $\textsf{do}(X = 5)$ **not the same event** as $X = 5$!

| $X = 5$ | $\neq$ | $\textsf{do}(X = 5)$ |
|:-:|:-:|:-:|
| *Observing* that $X$ took on value 5 (for some possibly-unknown reason) | $\neq$ | *Intervening* to force $X \leftarrow 5$, all else in DGP remaining the same (intervention then "flows" through rest of DGP) |

: {tbl-colwidths="[46,2,52]"}

* Probably the most difficult thing in 5650 to wrap head around
* "Special": $\Pr(\textsf{do}(X = 5))$ *not* well-defined, only $\Pr(Y = 6 \mid \textsf{do}(X = 5))$
* To emphasize special-ness, we may use notation like:

  $$
  \Pr(Y = 6 \mid \textsf{do}(X = 5)) \equiv \textstyle \Pr_{\textsf{do}(X = 5)}(Y = 6)
  $$

  to avoid confusion with "normal" events

## Causal World Unlocked ðŸ˜Ž (With Great Power Comes Great Responsibility...) {.title-08 .crunch-title .crunch-ul}

* With $\textsf{do}(\cdot)$ in hand... (Alongside DGP satisfying axioms slightly more strict than core probability axioms)
* We *can* make causal inferences from similar pair of facts! If:
  * Probability that event $E_1$ occurs is $\Pr(E_1) = 0.5$,
  * The probability that $E_1$ occurs **conditional on** the event $\textsf{do}(E_0)$ occurring is $\Pr(E_1 \mid \textsf{do}(E_0)) = 0.75$,
* **Now** we can actually infer that the occurrence of $E_0$ **caused** an increase in the likelihood of $E_1$ occurring!

## Ulysses and the [Computational] Sirens {.smaller .crunch-title}

![](images/sirens.jpg){fig-align="center"}

# Your First Probabilistic Graphical Model! {data-stack-name="PGMs"}

## Bayesian Inference but with Pictures

# PGM Nuts and Bolts

## Nodes are *Random Variables*

## Edges Indicate *Conditional Probability Tables* (CPTs)

## References

::: {#refs}
:::
