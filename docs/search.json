[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "PDF Links for Jeff‚Äôs Off-Hand References\n\n\n\n\n\n\n\n\n\n\nWhat Jeff calls it\nCitation (hover and/or click for PDF link)\n\n\n\n\n‚ÄúThe Multilevel book‚Äù\nGelman and Hill (2007)\n\n\n‚ÄúThe Koller PGM book‚Äù\nKoller and Friedman (2009)\n\n\n‚ÄúThe non-technical Pearl book‚Äù\nPearl and Mackenzie (2018)\n\n\n‚ÄúThe technical Pearl book‚Äù (containing, e.g., axioms and proofs)\nPearl (2000)\n\n\n‚ÄúThe French Revolution paper‚Äù\nBarron et al. (2018)",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#core-textbooks",
    "href": "resources.html#core-textbooks",
    "title": "Resources",
    "section": "Core Textbooks",
    "text": "Core Textbooks\nAs mentioned on the syllabus, the field of causal inference in general (and causal inference for computational social science in particular) moves excitingly-fast, such that the material has yet to ‚Äúcongeal‚Äù into a single, all-encompassing textbook. Nonetheless, the following three books cover a substantial amount of ground (described in more detail below each citation), so that together they form a coherent ‚Äúthree-volume textbook‚Äù for this class! If you can only read three books this summer, read these :)\n\n\n\n\n\n\n Morgan and Winship (2015)\n\n\n\nMorgan and Winship, Counterfactuals and Causal Inference: Methods and Principles for Social Research [PDF]\n\n\n\n\n\n\n\n\n\nThis is the book which comes closest to being an all-encompassing, single textbook for the class! It brings together different ‚Äústrands‚Äù of causal modeling research (since each field‚Äîeconomics, bioinformatics, sociology, etc.‚Äîtends to use its own notation and vocabulary), unifying them into a single approach. The only reason we can‚Äôt use it as the main textbook is because it hasn‚Äôt been updated since 2015, and most of the assignments in this class use computational tools from 2018 onwards!\n\n\n\n\n\n\n\n\n\n\n\n Angrist and Pischke (2014)\n\n\n\nAngrist and Pischke, Mastering ‚ÄôMetrics: The Path from Cause to Effect (Angrist and Pischke 2014) [PDF]\n\n\n\n\n\n\n\n\n\nThis book is included as the second of the three ‚Äúcore‚Äù texts mainly because, it uses the language of causality specific to Econometrics, the language that is most familiar to me from my PhD training in Political Economy. However, if you tend to learn better by example, it also does a good job of foregrounding specific examples (like evaluating charter schools and community policing policies), so that the methods emerging naturally from attempts to solve these puzzles when association methods like linear regression fail to capture their causal linkages.\n\n\n\n\n\n\n\n\n\n\n\n Pearl and Mackenzie (2018)\n\n\n\nPearl and Mackenzie, The Book of Why: The New Science of Cause and Effect [EPUB]\n\n\n\n\n\n\n\n\n\nThis book contrasts with the Angrist and Pischke book in using the language of causality formed within Computer Science rather than Economics. It can be a good starting point especially if you‚Äôre unfamiliar with the heavy use of diagrams for scientific modeling‚Äîbasically, whereas Angrist and Pischke‚Äôs first instinct is to use (sometimes informal) equations like \\(y = mx + b\\) to explain steps in the procedures, Pearl and Mackenzie‚Äôs instinct would be to instead use something like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~x~\\kern .01em} \\overset{\\small m, b}{\\longrightarrow} \\enclose{circle}{\\kern.01em y~\\kern .01em}\\) to represent the same concept (in this case, a line with slope \\(m\\) and intercept \\(b\\)!).",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#topic-specific-textbooks",
    "href": "resources.html#topic-specific-textbooks",
    "title": "Resources",
    "section": "Topic-Specific Textbooks",
    "text": "Topic-Specific Textbooks\n\nProbabilistic Graphical Models\n Koller and Friedman (2009), Probabilistic Graphical Models: Principles and Techniques\nChapter 2, with an overview of probability theory and graph theory, is included in the Syllabus as the recommended reading for Week 2! Then, Chapter 3 will give you the core concepts of Bayes Nets specifically, while Chapter 6 will add in a few of the useful additional tools we use, like plate notation.\n\n\nMultilevel Modeling\n Gelman and Hill (2007), Data Analysis Using Regression and Multilevel/Hierarchical Models",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#reference-texts",
    "href": "resources.html#reference-texts",
    "title": "Resources",
    "section": "Reference Texts",
    "text": "Reference Texts\nIn contrast to the books in the ‚ÄúCore Textbooks‚Äù section, these books are not ‚Äúthe‚Äù textbooks for the class! These are here instead as reference books, to keep on hand (a) for when the lectures or the above books are unclear on some topic, and/or (b) for deeper dives into certain topics (where the latter may become a more relevant mission for you as we move towards the final project üòâ)\n\nCausality\n Pearl (2000), Causality: Models, Reasoning, and Inference\nAs mentioned during Week 2, this is the book containing the fully-developed ‚Äúunified theory‚Äù of causality, starting from a set of axioms and deriving the possibilities of causal inference as formal theorems.\nWithin ‚Äúpure‚Äù mathematics, if you kept digging into the foundations of things like calculus or algebra, you would eventually arrive at Alfred North Whitehead and Bertrand Russell‚Äôs Principia Mathematica‚ÄîPearl (2000) is that but for causality: the foundational axioms and core theorems are all in here.\n Hume (1748), An Enquiry Concerning Human Understanding\nThis book serves as the philosophical ‚Äújumping off point‚Äù for causality: you can think of it like, there‚Äôs a nice progress-narrative of the human study of causality, that starts with the uncomfortable questions raised in Hume (1748) about the possibility (or impossibility!) of inferring knowledge of causality via inductive reasoning, and culminates in Pearl (2000).\n\n\nComputational Social Science\n**Gelman et al. (2013), *Bayesian Data Analysis (Third Edition)**\nContains info on Bayesian Workflow throughout, plus important chapters on Multilevel Modeling (Ch 5) and Bayesian Decision Analysis (Ch 9) ‚Äì the latter was referenced in the writeup on Bayesian Workflow\n Sperber (1996), Explaining Culture: A Naturalistic Approach\nMentioned in a footnote in Week 1, this is a surprisingly-old book that made waves in certain communities (like, e.g., among people like me who geek about studying culture quantitatively), by essentially proposing a ‚Äúresearch program‚Äù for the rigorous quantitative/empirical study of culture. In Jeff‚Äôs perfect world, this book would spark a progress-narrative in the same way that Hume (1748)",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#applied-examples",
    "href": "resources.html#applied-examples",
    "title": "Resources",
    "section": "Applied Examples",
    "text": "Applied Examples\n\nThe Holy Grail (But, Field = Comparative Politics)\n Kalyvas (2006), The Logic of Violence in Civil War\nThis book is essentially‚Ä¶ like, when you read stories about people spending decades hand-carving pathways through a mountain using only a hammer and chisel, this is the social science equivalent of that. A painstaking labor-of-love book that checks every single box I can think of in terms of causally-focused computational social science. It‚Äôs my model for any research I try to carry out.\n\n\nHistory\n Barron et al. (2018), ‚ÄúIndividuals, Institutions, and Innovation in the Debates of the French Revolution‚Äù\n Blaydes, Grimmer, and McQueen (2018), ‚ÄúMirrors for Princes and Sultans: Advice on the Art of Governance in the Medieval Christian and Islamic Worlds‚Äù\n\n\nEconometric Policy Evaluation\n Bj√∂rkegren, Blumenstock, and Knight (2025), ‚ÄúWhat Do Policies Value?‚Äù, Review of Economic Studies [PDF]\nAnother ‚Äúholy grail‚Äù paper, but in this case more for the Data Ethics and Policy course than this course. However, it does touch substantially on the issue of associational vs.¬†causal inference, especially in terms of how going towards causality is a ‚Äúhigh stakes‚Äù endeavor here, since we‚Äôre inferring normative ethical values from data (as opposed to descriptive statistics like the magnitude of the causal effect \\(X \\rightarrow Y\\))",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#video-resources",
    "href": "resources.html#video-resources",
    "title": "Resources",
    "section": "Video Resources",
    "text": "Video Resources\nTo me (as in, given how my brain works), there are certain topics which I‚Äôve spent hours trying to understand via reading, only to realize that there‚Äôs some simple diagram or animation out there which ‚Äúclicks‚Äù it in my mind 10000 times more effectively than the reading ever would.\nSo, to that end, these video resources are just as important as (for some topics far more important than) the resources above!1\n Ahrens (2024), ‚ÄúRobust Causal Inference using Double/Debiased Machine Learning: A Guide for Empirical Research‚Äù, MZES Methods Bites Seminar Link\nHere I literally added it to the resources folder before I realized that it has a discussion of a paper I did during the PhD. So, perhaps leaving it in here is some sort of humble brag, but finding it originally was not! (It‚Äôs what came up for me when I searched ‚ÄúDouble/Debiased Machine Learning‚Äù on YouTube in May 2025 üòú)",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#miscellaneous-commonly-used-books-that-were-not-really-using",
    "href": "resources.html#miscellaneous-commonly-used-books-that-were-not-really-using",
    "title": "Resources",
    "section": "Miscellaneous / Commonly-Used Books That We‚Äôre Not Really Using",
    "text": "Miscellaneous / Commonly-Used Books That We‚Äôre Not Really Using\n King, Keohane, and Verba (1994), Designing Social Inquiry\nI don‚Äôt really have a substantive critique of this book, or a deep reason for not using it, other than that I find myself getting really bored whenever I try to read it üôà",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#footnotes",
    "href": "resources.html#footnotes",
    "title": "Resources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor similar reasons, audiobooks may provide more effective ways to digest some topics in the course!‚Ü©Ô∏é",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "midterm.html",
    "href": "midterm.html",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "",
    "text": "The DSAN 5650 in-class midterm will start at 6:30pm EDT on Wednesday, July 2nd. It is intended to take only 1.5 hours, but you will have 3 hours to take it just in case, so that it will be due by 9:30pm EDT.\nWe‚Äôve covered a lot of‚Ä¶ sometimes-disjointed topics thus far, so our goal for the midterm is to emphasize the most important takeaways from across the first six weeks of the class, by putting them in your brain‚Äôs short-term memory one more time before we dive into the second, computationally-focused half of the course when you come back from July 4th weekend! As a reminder, there is a very good reason to continually try and re-remember the same course topics, coming from studies of long-term memory retention:"
  },
  {
    "objectID": "midterm.html#overview",
    "href": "midterm.html#overview",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "",
    "text": "The DSAN 5650 in-class midterm will start at 6:30pm EDT on Wednesday, July 2nd. It is intended to take only 1.5 hours, but you will have 3 hours to take it just in case, so that it will be due by 9:30pm EDT.\nWe‚Äôve covered a lot of‚Ä¶ sometimes-disjointed topics thus far, so our goal for the midterm is to emphasize the most important takeaways from across the first six weeks of the class, by putting them in your brain‚Äôs short-term memory one more time before we dive into the second, computationally-focused half of the course when you come back from July 4th weekend! As a reminder, there is a very good reason to continually try and re-remember the same course topics, coming from studies of long-term memory retention:"
  },
  {
    "objectID": "midterm.html#part-1-pgms-the-language-of-dgps",
    "href": "midterm.html#part-1-pgms-the-language-of-dgps",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "Part 1: PGMs, the Language of DGPs",
    "text": "Part 1: PGMs, the Language of DGPs"
  },
  {
    "objectID": "midterm.html#part-2-statistical-matching",
    "href": "midterm.html#part-2-statistical-matching",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "Part 2: Statistical Matching",
    "text": "Part 2: Statistical Matching"
  },
  {
    "objectID": "midterm.html#references",
    "href": "midterm.html#references",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "w01/slides.html#prof.-jeff-introduction",
    "href": "w01/slides.html#prof.-jeff-introduction",
    "title": "Week 1: Science from Particles to People",
    "section": "Prof.¬†Jeff Introduction!",
    "text": "Prof.¬†Jeff Introduction!\n\nBorn in NW DC ‚Üí high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Econ"
  },
  {
    "objectID": "w01/slides.html#the-world-outside-of-dc",
    "href": "w01/slides.html#the-world-outside-of-dc",
    "title": "Week 1: Science from Particles to People",
    "section": "The World Outside of DC",
    "text": "The World Outside of DC\n Studied abroad in Beijing (Peking University/ÂåóÂ§ß) ‚Üí internship with Huawei in Hong Kong (HKUST)\n\n\n\n Stanford, MS in Computer Science\n Research Economist, UC Berkeley\n Columbia, PhD in Political Economy"
  },
  {
    "objectID": "w01/slides.html#why-is-georgetown-having-me-teach-this",
    "href": "w01/slides.html#why-is-georgetown-having-me-teach-this",
    "title": "Week 1: Science from Particles to People",
    "section": "Why Is Georgetown Having Me Teach This?",
    "text": "Why Is Georgetown Having Me Teach This?\n\n\n\n\n\n\n\nQuanty things \\(\\leadsto\\) PhD in Political Economy\nPhD exam major: Political Philosophy\nPhD exam minor: International Relations\nPhD exam paper: ‚ÄúHow to Do Things with Translations‚Äù\nGame-changing Research Fellowships at‚Ä¶ (Nothing was the same -drake):\nSanta Fe Institute: dedicated to the multidisciplinary study of complex systems: physical, computational, biological, social\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\n\nFigure¬†1: Years spent questing in dungeons of academia\n\n\n\n\n\n\n\nCentre for the Study of the History of Political Thought, Queen Mary University of London (QMUL): New approaches to the history of political thought [Quentin Skinner, ‚ÄúCambridge School‚Äù] have changed how we study ideas from the past and their relevance to contemporary politics. The focus of the Centre is to explore [ts]."
  },
  {
    "objectID": "w01/slides.html#dissertation-nlp-x-history",
    "href": "w01/slides.html#dissertation-nlp-x-history",
    "title": "Week 1: Science from Particles to People",
    "section": "Dissertation (NLP x History)",
    "text": "Dissertation (NLP x History)\n‚ÄúOur Word is Our Weapon‚Äù: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada"
  },
  {
    "objectID": "w01/slides.html#ir-part-wars-of-ideas-i-cold-war",
    "href": "w01/slides.html#ir-part-wars-of-ideas-i-cold-war",
    "title": "Week 1: Science from Particles to People",
    "section": "(IR Part) Wars of Ideas I: Cold War",
    "text": "(IR Part) Wars of Ideas I: Cold War\n\nCold War arms shipments (SIPRI) vs.¬†propaganda (–ü–µ—á–∞—Ç—å –°–°–°–†): here, to üá™üáπ"
  },
  {
    "objectID": "w01/slides.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "href": "w01/slides.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "title": "Week 1: Science from Particles to People",
    "section": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993",
    "text": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993"
  },
  {
    "objectID": "w01/slides.html#research-nowadays",
    "href": "w01/slides.html#research-nowadays",
    "title": "Week 1: Science from Particles to People",
    "section": "Research Nowadays",
    "text": "Research Nowadays\n\nMost cited paper: ‚ÄúMonopsony in Online Labor Markets‚Äù (Uses Double-Debiased ML for Causal Inference!)\nMost recent paper: ‚ÄúOperationalizing Freedom as Non-Domination in the Labor Market‚Äù (Cambridge U Press)\nRarely cited paper but often-thought-about obsession: ‚ÄúHow To Do Things With Translations‚Äù\nRelated thing you can buy in a bookstore: [Editorial board for new translation of] Capital, Vol. 1 by Karl Marx (Princeton U Press)"
  },
  {
    "objectID": "w01/slides.html#but-now-teaching",
    "href": "w01/slides.html#but-now-teaching",
    "title": "Week 1: Science from Particles to People",
    "section": "But Now‚Ä¶ Teaching!",
    "text": "But Now‚Ä¶ Teaching!\n\nGrowth mindset: ‚ÄúI can‚Äôt do this‚Äù \\(\\leadsto\\) ‚ÄúI can‚Äôt do this yet!‚Äù\nThink of anything you‚Äôre able to do‚Ä¶ There was a point in life when you didn‚Äôt know how to do it! What happened? Your brain established and/or rearranged neural pathways as you struggled with it!\nHow does this neural re-arrangement work? One ‚Äúcheatcode‚Äù is spaced repetition:\n\n\nImage source"
  },
  {
    "objectID": "w01/slides.html#lil-wayne-on-spaced-repetition",
    "href": "w01/slides.html#lil-wayne-on-spaced-repetition",
    "title": "Week 1: Science from Particles to People",
    "section": "Lil Wayne on Spaced Repetition",
    "text": "Lil Wayne on Spaced Repetition\n\n\n\n\n\n\nFigure¬†2: From The Carter (Documentary)"
  },
  {
    "objectID": "w01/slides.html#maria-montessori-on-your-final-project",
    "href": "w01/slides.html#maria-montessori-on-your-final-project",
    "title": "Week 1: Science from Particles to People",
    "section": "Maria Montessori on Your Final Project",
    "text": "Maria Montessori on Your Final Project\n\nOur teaching should be governed, not by a desire to make students learn things, but by the endeavor to keep burning within them that light which is called curiosity. (Montessori 1916)1\n\n\nTo this end: your final project is to explore some potential causal linkage from \\(X\\) to \\(Y\\) (more later!)\nThe sole requirement is: sufficient curiosity to serve as fuel for your journey from associational world to causal world\n\n‚ÄúCouldn‚Äôt do classes, them lessons used to bore me, I was tryna school heads like Waldorf and Montessori‚Äù -Bar dropped by local rapper one day in freestyle that I never forgot"
  },
  {
    "objectID": "w01/slides.html#coarse-graining-units-of-observation",
    "href": "w01/slides.html#coarse-graining-units-of-observation",
    "title": "Week 1: Science from Particles to People",
    "section": "Coarse-Graining Units of Observation",
    "text": "Coarse-Graining Units of Observation\n\n\n\nField\nExample Unit of Observation\n\n\n\n\nPhysics\nParticle\n[Particle = Fine-Graining of Molecules]\n\n\nChemistry\nMolecule = \\(\\cup\\)(Particles)\n[Molecule = Coarse-Graining of Particles]\n\n\nBiology\nCell = \\(\\cup\\)(Molecules)\n\n\nNeuro/Cog Sci\nBrain = \\(\\cup\\)(Cells)\n\n\nHuman Physiology\nBody = Brain \\(\\cup\\) Other Organs\n\n\n‚Üë Science ¬†¬† üßê something happens here‚Ä¶ ü§î ¬†¬† ‚Üì Social Science\n\n\nAnthropology\nHuman-Relational System (e.g., Kinship) = \\(\\cup\\)(Brains) \\(\\times\\) Natural Environment\n\n\nEconomics\nEconomy = Specific relational system of exchange w.r.t. scarce resources\n\n\nPolitical Economy\nEconomy-Context = Economic Exchange \\(\\cap\\) Relational Power\n\n\nSociology\nSociety = \\(\\cup\\)(Relational Systems: Kinship, Friendship, Authority, Power, Violence)\n\n\nHistory\nLongue-Dur√©e = Evolution of Societies over Time and Space"
  },
  {
    "objectID": "w01/slides.html#homo-sapienshomo-arbitratushomo-mischievous",
    "href": "w01/slides.html#homo-sapienshomo-arbitratushomo-mischievous",
    "title": "Week 1: Science from Particles to People",
    "section": "Homo sapiens/Homo arbitratus/Homo mischievous",
    "text": "Homo sapiens/Homo arbitratus/Homo mischievous\n\nLatin sapiens denotes being ‚Äúdiscerning‚Äù or ‚Äúwise‚Äù\n\n\n\n\n\n\n\n\nBut‚Ä¶ technically nothing stops us from choosing to be ‚Äúunwise‚Äù whenever we‚Äôd like‚Ä¶ bc free will\n\\(\\Rightarrow\\) For this class, humans are Homo arbitratus: arbitratus denotes choosing what to do, after we‚Äôve sapiently ‚Äúdiscerned‚Äù/thought about it"
  },
  {
    "objectID": "w01/slides.html#laws-of-physics-vs.-laws-of-social-science",
    "href": "w01/slides.html#laws-of-physics-vs.-laws-of-social-science",
    "title": "Week 1: Science from Particles to People",
    "section": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science",
    "text": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science\n\nIf we tell an inanimate object that we‚Äôve discovered a law saying that it will accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n‚Ä¶It will likely1 still accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n\n\n\n\n\n\n\n\nIf we tell a human we‚Äôve discovered a law saying they will quack like a duck at 7:30pm EDT every Wednesday\n\n‚Ä¶They can utilize their free will to violate this ‚Äúlaw‚Äù\n\n\n\n\n\n\n\n\n\n\n\n\nObligatory quantum mechanics footnote: human agency [maybe] plays a role, in a quirky way, in physics at tiny subatomic scales‚Ä¶ but once we coarse grain to atoms (and avoid speed of light), Newton‚Äôs Laws accurate to many decimals ü§Ø"
  },
  {
    "objectID": "w01/slides.html#strangely-relevant-cs-topic-the-halting-problem",
    "href": "w01/slides.html#strangely-relevant-cs-topic-the-halting-problem",
    "title": "Week 1: Science from Particles to People",
    "section": "Strangely-Relevant CS Topic: The Halting Problem",
    "text": "Strangely-Relevant CS Topic: The Halting Problem\n\nKurt G√∂del \\(\\leftrightarrow\\) Alan Turing: Entscheidungsproblem\nTheorem: It is not possible to write a computer program \\(P(x)\\) that detects whether or not a computer program \\(x\\) will eventually halt (as opposed to, e.g., looping forever)\nProof: Assume \\(P(x)\\) is possible to write. Run it on mischievous_program.py. Infinite contradiction loop.\n\n\n\n\n\nhalting_problem_solver.py\n\ndef will_it_halt(program, input):\n  # Put code you think will work here\n  # Return True if program will halt, False otherwise\n\n\n\nmischievous_program.py\n\ndef my_mischievous_function(input):\n  if will_it_halt(my_mischevious_function, input):\n    while True: pass # Infinite loop\n  else:\n    return # Halt\n\n\n\n\n\n\n\n\nInput‚Üí‚ÜìProgram\n0\n1\n2\n\\(\\cdots\\)\n\n\n\n\n0\nHalt\nLoop\nLoop\n\\(\\cdots\\)\n\n\n1\nLoop\nLoop\nHalt\n\\(\\cdots\\)\n\n\n2\nLoop\nLoop\nLoop\n\\(\\cdots\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\mathbf{\\ddots}\\)\n\n\n\nLoop\nHalt\nHalt\n\\(\\mathbf{\\cdots}\\)"
  },
  {
    "objectID": "w01/slides.html#the-takeaway-bayesian-humility",
    "href": "w01/slides.html#the-takeaway-bayesian-humility",
    "title": "Week 1: Science from Particles to People",
    "section": "The Takeaway: [Bayesian] Humility!",
    "text": "The Takeaway: [Bayesian] Humility!\n\nSocial science, with ‚Äúscience‚Äù used in the same sense as for physics, may be a quixotic endeavor1\nInstead, we‚Äôll do social science, where we use data to‚Ä¶\n Infer tendencies: \\(\\mathcal{H}\\) = ¬´\\(X\\) tends to cause \\(Y\\)¬ª\n With some degree of veracity: \\(\\Pr(\\mathcal{H}) \\approx 0.7\\)\n Construct models that we can update with new evidence: Bayes‚Äô rule! \\(\\Pr(\\mathcal{H} \\mid E) = \\frac{\\Pr(E \\mid \\mathcal{H} ) \\Pr(\\mathcal{H})}{\\Pr(E)} \\approx 0.8\\)\nPls notice ‚Äúslippage‚Äù between aleatory probability within \\(\\mathcal{H}\\) (‚Äútends to‚Äù) vs.¬†epistemic probability ‚Äúoutside of‚Äù, talking about \\(\\mathcal{H}\\) (‚ÄúI‚Äôm 70% confident about \\(\\mathcal{H}\\)‚Äù)\n\nAt least, for the time being‚Ä¶ BUT see Sperber (1996), which will come up later"
  },
  {
    "objectID": "w01/slides.html#the-logic-of-violence-in-civil-war",
    "href": "w01/slides.html#the-logic-of-violence-in-civil-war",
    "title": "Week 1: Science from Particles to People",
    "section": "The Logic of Violence in Civil War",
    "text": "The Logic of Violence in Civil War\n\nKalyvas (2006)"
  },
  {
    "objectID": "w01/slides.html#matching-estimators",
    "href": "w01/slides.html#matching-estimators",
    "title": "Week 1: Science from Particles to People",
    "section": "Matching Estimators",
    "text": "Matching Estimators\n\nImage Source"
  },
  {
    "objectID": "w01/slides.html#case-study-military-inequality-leadsto-military-success",
    "href": "w01/slides.html#case-study-military-inequality-leadsto-military-success",
    "title": "Week 1: Science from Particles to People",
    "section": "Case Study: Military Inequality \\(\\leadsto\\) Military Success",
    "text": "Case Study: Military Inequality \\(\\leadsto\\) Military Success\n\nLyall (2020): ‚ÄúTreating certain ethnic groups as second-class citizens [‚Ä¶] leads victimized soldiers to subvert military authorities once war begins. The higher an army‚Äôs inequality, the greater its rates of desertion, side-switching, and casualties‚Äù\n\n\nMatching constructs pairs of belligerents that are similar across a wide range of traits thought to dictate battlefield performance but that vary in levels of prewar inequality. The more similar the belligerents, the better our estimate of inequality‚Äôs effects, as all other traits are shared and thus cannot explain observed differences in performance, helping assess how battlefield performance would have improved (declined) if the belligerent had a lower (higher) level of prewar inequality.\nSince [non-matched] cases are dropped [‚Ä¶] selected cases are more representative of average belligerents/wars than outliers with few or no matches, [providing] surer ground for testing generalizability of the book‚Äôs claims than focusing solely on canonical but unrepresentative usual suspects (Germany, the United States, Israel)"
  },
  {
    "objectID": "w01/slides.html#does-inequality-cause-poor-military-performance",
    "href": "w01/slides.html#does-inequality-cause-poor-military-performance",
    "title": "Week 1: Science from Particles to People",
    "section": "Does Inequality Cause Poor Military Performance?",
    "text": "Does Inequality Cause Poor Military Performance?\n\n\n\n\n\n\n\n\nCovariates\nSultanate of Morocco Spanish-Moroccan War, 1859-60\nKhanate of Kokand War with Russia, 1864-65\n\n\n\n\n\\(X\\): Military Inequality\nLow (0.01)\nExtreme (0.70)\n\n\n\\(\\mathbf{Z}\\): Matched Covariates:\n\n\n\n\nInitial relative power\n66%\n66%\n\n\nTotal fielded force\n55,000\n50,000\n\n\nRegime type\nAbsolutist Monarchy (‚àí6)\nAbsolute Monarchy (‚àí7)\n\n\nDistance from capital\n208km\n265km\n\n\nStanding army\nYes\nYes\n\n\nComposite military\nYes\nYes\n\n\nInitiator\nNo\nNo\n\n\nJoiner\nNo\nNo\n\n\nDemocratic opponent\nNo\nNo\n\n\nGreat Power\nNo\nNo\n\n\nCivil war\nNo\nNo\n\n\nCombined arms\nYes\nYes\n\n\nDoctrine\nOffensive\nOffensive\n\n\nSuperior weapons\nNo\nNo\n\n\nFortifications\nYes\nYes\n\n\nForeign advisors\nYes\nYes\n\n\nTerrain\nSemiarid coastal plain\nSemiarid grassland plain\n\n\nTopography\nRugged\nRugged\n\n\nWar duration\n126 days\n378 days\n\n\nRecent war history w/opp\nYes\nYes\n\n\nFacing colonizer\nYes\nYes\n\n\nIdentity dimension\nSunni Islam/Christian\nSunni Islam/Christian\n\n\nNew leader\nYes\nYes\n\n\nPopulation\n8‚Äì8.5 million\n5‚Äì6 million\n\n\nEthnoling fractionalization (ELF)\nHigh\nHigh\n\n\nCiv-mil relations\nRuler as commander\nRuler as commander\n\n\n\\(Y\\): Battlefield Performance:\n\n\n\n\nLoss-exchange ratio\n0.43\n0.02\n\n\nMass desertion\nNo\nYes\n\n\nMass defection\nNo\nNo\n\n\nFratricidal violence\nNo\nYes"
  },
  {
    "objectID": "w01/slides.html#particularly-fun-non-standard-examples",
    "href": "w01/slides.html#particularly-fun-non-standard-examples",
    "title": "Week 1: Science from Particles to People",
    "section": "Particularly Fun Non-‚ÄúStandard‚Äù Examples",
    "text": "Particularly Fun Non-‚ÄúStandard‚Äù Examples\n\nDeDeo, French Revolution\nGrimmer, Mirrors for Sultans and Princes"
  },
  {
    "objectID": "w01/slides.html#jupyterhub",
    "href": "w01/slides.html#jupyterhub",
    "title": "Week 1: Science from Particles to People",
    "section": "JupyterHub",
    "text": "JupyterHub\n\nhttps://guhub.io\n\n\nImage source"
  },
  {
    "objectID": "w01/slides.html#references",
    "href": "w01/slides.html#references",
    "title": "Week 1: Science from Particles to People",
    "section": "References",
    "text": "References\n\n\nKalyvas, Stathis N. 2006. The Logic of Violence in Civil War. Cambridge University Press. https://www.dropbox.com/scl/fi/4es1r44ldnqtcroonr5g7/The-Logic-of-Violence-in-Civil-War.pdf?rlkey=uonfmw5zkx1ja5l8emv2bzfmg&dl=1.\n\n\nLyall, Jason. 2020. Divided Armies: Inequality and Battlefield Performance in Modern War. Princeton University Press.\n\n\nMontessori, Maria. 1916. Spontaneous Activity in Education: A Basic Guide to the Montessori Methods of Learning in the Classroom. Lulu Press.\n\n\nSperber, Dan. 1996. Explaining Culture: A Naturalistic Approach. Cambridge: Blackwell. https://www.dropbox.com/scl/fi/he6idaajr6vopybv13qhd/Explaining-Culture-A-Naturalistic-Approach.pdf?rlkey=8oyhnsu5ycet4osstft252jaw&dl=1."
  },
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Science from Particles to People",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#prof.-jeff-introduction",
    "href": "w01/index.html#prof.-jeff-introduction",
    "title": "Week 1: Science from Particles to People",
    "section": "Prof.¬†Jeff Introduction!",
    "text": "Prof.¬†Jeff Introduction!\n\nBorn in NW DC ‚Üí high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Econ",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#the-world-outside-of-dc",
    "href": "w01/index.html#the-world-outside-of-dc",
    "title": "Week 1: Science from Particles to People",
    "section": "The World Outside of DC",
    "text": "The World Outside of DC\n Studied abroad in Beijing (Peking University/ÂåóÂ§ß) ‚Üí internship with Huawei in Hong Kong (HKUST)\n\n\n\n Stanford, MS in Computer Science\n Research Economist, UC Berkeley\n Columbia, PhD in Political Economy",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#why-is-georgetown-having-me-teach-this",
    "href": "w01/index.html#why-is-georgetown-having-me-teach-this",
    "title": "Week 1: Science from Particles to People",
    "section": "Why Is Georgetown Having Me Teach This?",
    "text": "Why Is Georgetown Having Me Teach This?\n\n\n\n\n\n\n\nQuanty things \\(\\leadsto\\) PhD in Political Economy\nPhD exam major: Political Philosophy\nPhD exam minor: International Relations\nPhD exam paper: ‚ÄúHow to Do Things with Translations‚Äù\nGame-changing Research Fellowships at‚Ä¶ (Nothing was the same -drake):\nSanta Fe Institute: dedicated to the multidisciplinary study of complex systems: physical, computational, biological, social\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\n\nFigure¬†1: Years spent questing in dungeons of academia\n\n\n\n\n\n\n\nCentre for the Study of the History of Political Thought, Queen Mary University of London (QMUL): New approaches to the history of political thought [Quentin Skinner, ‚ÄúCambridge School‚Äù] have changed how we study ideas from the past and their relevance to contemporary politics. The focus of the Centre is to explore [ts].",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#dissertation-nlp-x-history",
    "href": "w01/index.html#dissertation-nlp-x-history",
    "title": "Week 1: Science from Particles to People",
    "section": "Dissertation (NLP x History)",
    "text": "Dissertation (NLP x History)\n‚ÄúOur Word is Our Weapon‚Äù: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#ir-part-wars-of-ideas-i-cold-war",
    "href": "w01/index.html#ir-part-wars-of-ideas-i-cold-war",
    "title": "Week 1: Science from Particles to People",
    "section": "(IR Part) Wars of Ideas I: Cold War",
    "text": "(IR Part) Wars of Ideas I: Cold War\n\nCold War arms shipments (SIPRI) vs.¬†propaganda (–ü–µ—á–∞—Ç—å –°–°–°–†): here, to üá™üáπ",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "href": "w01/index.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "title": "Week 1: Science from Particles to People",
    "section": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993",
    "text": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#research-nowadays",
    "href": "w01/index.html#research-nowadays",
    "title": "Week 1: Science from Particles to People",
    "section": "Research Nowadays",
    "text": "Research Nowadays\n\nMost cited paper: ‚ÄúMonopsony in Online Labor Markets‚Äù (Uses Double-Debiased ML for Causal Inference!)\nMost recent paper: ‚ÄúOperationalizing Freedom as Non-Domination in the Labor Market‚Äù (Cambridge U Press)\nRarely cited paper but often-thought-about obsession: ‚ÄúHow To Do Things With Translations‚Äù\nRelated thing you can buy in a bookstore: [Editorial board for new translation of] Capital, Vol. 1 by Karl Marx (Princeton U Press)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#but-now-teaching",
    "href": "w01/index.html#but-now-teaching",
    "title": "Week 1: Science from Particles to People",
    "section": "But Now‚Ä¶ Teaching!",
    "text": "But Now‚Ä¶ Teaching!\n\nGrowth mindset: ‚ÄúI can‚Äôt do this‚Äù \\(\\leadsto\\) ‚ÄúI can‚Äôt do this yet!‚Äù\nThink of anything you‚Äôre able to do‚Ä¶ There was a point in life when you didn‚Äôt know how to do it! What happened? Your brain established and/or rearranged neural pathways as you struggled with it!\nHow does this neural re-arrangement work? One ‚Äúcheatcode‚Äù is spaced repetition:\n\n\n\n\nImage source",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#lil-wayne-on-spaced-repetition",
    "href": "w01/index.html#lil-wayne-on-spaced-repetition",
    "title": "Week 1: Science from Particles to People",
    "section": "Lil Wayne on Spaced Repetition",
    "text": "Lil Wayne on Spaced Repetition\n\n\n\n\n\n\nFigure¬†2: From The Carter (Documentary)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#maria-montessori-on-your-final-project",
    "href": "w01/index.html#maria-montessori-on-your-final-project",
    "title": "Week 1: Science from Particles to People",
    "section": "Maria Montessori on Your Final Project",
    "text": "Maria Montessori on Your Final Project\n\nOur teaching should be governed, not by a desire to make students learn things, but by the endeavor to keep burning within them that light which is called curiosity. (Montessori 1916)1\n\n\nTo this end: your final project is to explore some potential causal linkage from \\(X\\) to \\(Y\\) (more later!)\nThe sole requirement is: sufficient curiosity to serve as fuel for your journey from associational world to causal world",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#coarse-graining-units-of-observation",
    "href": "w01/index.html#coarse-graining-units-of-observation",
    "title": "Week 1: Science from Particles to People",
    "section": "Coarse-Graining Units of Observation",
    "text": "Coarse-Graining Units of Observation\n\n\n\nField\nExample Unit of Observation\n\n\n\n\nPhysics\nParticle\n[Particle = Fine-Graining of Molecules]\n\n\nChemistry\nMolecule = \\(\\cup\\)(Particles)\n[Molecule = Coarse-Graining of Particles]\n\n\nBiology\nCell = \\(\\cup\\)(Molecules)\n\n\nNeuro/Cog Sci\nBrain = \\(\\cup\\)(Cells)\n\n\nHuman Physiology\nBody = Brain \\(\\cup\\) Other Organs\n\n\n‚Üë Science ¬†¬† üßê something happens here‚Ä¶ ü§î ¬†¬† ‚Üì Social Science\n\n\nAnthropology\nHuman-Relational System (e.g., Kinship) = \\(\\cup\\)(Brains) \\(\\times\\) Natural Environment\n\n\nEconomics\nEconomy = Specific relational system of exchange w.r.t. scarce resources\n\n\nPolitical Economy\nEconomy-Context = Economic Exchange \\(\\cap\\) Relational Power\n\n\nSociology\nSociety = \\(\\cup\\)(Relational Systems: Kinship, Friendship, Authority, Power, Violence)\n\n\nHistory\nLongue-Dur√©e = Evolution of Societies over Time and Space",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#homo-sapienshomo-arbitratushomo-mischievous",
    "href": "w01/index.html#homo-sapienshomo-arbitratushomo-mischievous",
    "title": "Week 1: Science from Particles to People",
    "section": "Homo sapiens/Homo arbitratus/Homo mischievous",
    "text": "Homo sapiens/Homo arbitratus/Homo mischievous\n\nLatin sapiens denotes being ‚Äúdiscerning‚Äù or ‚Äúwise‚Äù\n\n\n\n\n\n\n\n\nBut‚Ä¶ technically nothing stops us from choosing to be ‚Äúunwise‚Äù whenever we‚Äôd like‚Ä¶ bc free will\n\\(\\Rightarrow\\) For this class, humans are Homo arbitratus: arbitratus denotes choosing what to do, after we‚Äôve sapiently ‚Äúdiscerned‚Äù/thought about it",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#laws-of-physics-vs.-laws-of-social-science",
    "href": "w01/index.html#laws-of-physics-vs.-laws-of-social-science",
    "title": "Week 1: Science from Particles to People",
    "section": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science",
    "text": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science\n\nIf we tell an inanimate object that we‚Äôve discovered a law saying that it will accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n‚Ä¶It will likely2 still accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n\n\n\n\n\n\n\n\nIf we tell a human we‚Äôve discovered a law saying they will quack like a duck at 7:30pm EDT every Wednesday\n\n‚Ä¶They can utilize their free will to violate this ‚Äúlaw‚Äù",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#strangely-relevant-cs-topic-the-halting-problem",
    "href": "w01/index.html#strangely-relevant-cs-topic-the-halting-problem",
    "title": "Week 1: Science from Particles to People",
    "section": "Strangely-Relevant CS Topic: The Halting Problem",
    "text": "Strangely-Relevant CS Topic: The Halting Problem\n\nKurt G√∂del \\(\\leftrightarrow\\) Alan Turing: Entscheidungsproblem\nTheorem: It is not possible to write a computer program \\(P(x)\\) that detects whether or not a computer program \\(x\\) will eventually halt (as opposed to, e.g., looping forever)\nProof: Assume \\(P(x)\\) is possible to write. Run it on mischievous_program.py. Infinite contradiction loop.\n\n\n\n\n\nhalting_problem_solver.py\n\ndef will_it_halt(program, input):\n  # Put code you think will work here\n  # Return True if program will halt, False otherwise\n\n\n\nmischievous_program.py\n\ndef my_mischievous_function(input):\n  if will_it_halt(my_mischevious_function, input):\n    while True: pass # Infinite loop\n  else:\n    return # Halt\n\n\n\n\n\n\n\n\nInput‚Üí‚ÜìProgram\n0\n1\n2\n\\(\\cdots\\)\n\n\n\n\n0\nHalt\nLoop\nLoop\n\\(\\cdots\\)\n\n\n1\nLoop\nLoop\nHalt\n\\(\\cdots\\)\n\n\n2\nLoop\nLoop\nLoop\n\\(\\cdots\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\mathbf{\\ddots}\\)\n\n\n\nLoop\nHalt\nHalt\n\\(\\mathbf{\\cdots}\\)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#the-takeaway-bayesian-humility",
    "href": "w01/index.html#the-takeaway-bayesian-humility",
    "title": "Week 1: Science from Particles to People",
    "section": "The Takeaway: [Bayesian] Humility!",
    "text": "The Takeaway: [Bayesian] Humility!\n\nSocial science, with ‚Äúscience‚Äù used in the same sense as for physics, may be a quixotic endeavor3\nInstead, we‚Äôll do social science, where we use data to‚Ä¶\n Infer tendencies: \\(\\mathcal{H}\\) = ¬´\\(X\\) tends to cause \\(Y\\)¬ª\n With some degree of veracity: \\(\\Pr(\\mathcal{H}) \\approx 0.7\\)\n Construct models that we can update with new evidence: Bayes‚Äô rule! \\(\\Pr(\\mathcal{H} \\mid E) = \\frac{\\Pr(E \\mid \\mathcal{H} ) \\Pr(\\mathcal{H})}{\\Pr(E)} \\approx 0.8\\)\nPls notice ‚Äúslippage‚Äù between aleatory probability within \\(\\mathcal{H}\\) (‚Äútends to‚Äù) vs.¬†epistemic probability ‚Äúoutside of‚Äù, talking about \\(\\mathcal{H}\\) (‚ÄúI‚Äôm 70% confident about \\(\\mathcal{H}\\)‚Äù)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#the-logic-of-violence-in-civil-war",
    "href": "w01/index.html#the-logic-of-violence-in-civil-war",
    "title": "Week 1: Science from Particles to People",
    "section": "The Logic of Violence in Civil War",
    "text": "The Logic of Violence in Civil War\n\n\n\nKalyvas (2006)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#matching-estimators",
    "href": "w01/index.html#matching-estimators",
    "title": "Week 1: Science from Particles to People",
    "section": "Matching Estimators",
    "text": "Matching Estimators\n\n\n\nImage Source",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#case-study-military-inequality-leadsto-military-success",
    "href": "w01/index.html#case-study-military-inequality-leadsto-military-success",
    "title": "Week 1: Science from Particles to People",
    "section": "Case Study: Military Inequality \\(\\leadsto\\) Military Success",
    "text": "Case Study: Military Inequality \\(\\leadsto\\) Military Success\n\nLyall (2020): ‚ÄúTreating certain ethnic groups as second-class citizens [‚Ä¶] leads victimized soldiers to subvert military authorities once war begins. The higher an army‚Äôs inequality, the greater its rates of desertion, side-switching, and casualties‚Äù\n\n\nMatching constructs pairs of belligerents that are similar across a wide range of traits thought to dictate battlefield performance but that vary in levels of prewar inequality. The more similar the belligerents, the better our estimate of inequality‚Äôs effects, as all other traits are shared and thus cannot explain observed differences in performance, helping assess how battlefield performance would have improved (declined) if the belligerent had a lower (higher) level of prewar inequality.\nSince [non-matched] cases are dropped [‚Ä¶] selected cases are more representative of average belligerents/wars than outliers with few or no matches, [providing] surer ground for testing generalizability of the book‚Äôs claims than focusing solely on canonical but unrepresentative usual suspects (Germany, the United States, Israel)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#does-inequality-cause-poor-military-performance",
    "href": "w01/index.html#does-inequality-cause-poor-military-performance",
    "title": "Week 1: Science from Particles to People",
    "section": "Does Inequality Cause Poor Military Performance?",
    "text": "Does Inequality Cause Poor Military Performance?\n\n\n\n\n\n\n\n\nCovariates\nSultanate of Morocco Spanish-Moroccan War, 1859-60\nKhanate of Kokand War with Russia, 1864-65\n\n\n\n\n\\(X\\): Military Inequality\nLow (0.01)\nExtreme (0.70)\n\n\n\\(\\mathbf{Z}\\): Matched Covariates:\n\n\n\n\nInitial relative power\n66%\n66%\n\n\nTotal fielded force\n55,000\n50,000\n\n\nRegime type\nAbsolutist Monarchy (‚àí6)\nAbsolute Monarchy (‚àí7)\n\n\nDistance from capital\n208km\n265km\n\n\nStanding army\nYes\nYes\n\n\nComposite military\nYes\nYes\n\n\nInitiator\nNo\nNo\n\n\nJoiner\nNo\nNo\n\n\nDemocratic opponent\nNo\nNo\n\n\nGreat Power\nNo\nNo\n\n\nCivil war\nNo\nNo\n\n\nCombined arms\nYes\nYes\n\n\nDoctrine\nOffensive\nOffensive\n\n\nSuperior weapons\nNo\nNo\n\n\nFortifications\nYes\nYes\n\n\nForeign advisors\nYes\nYes\n\n\nTerrain\nSemiarid coastal plain\nSemiarid grassland plain\n\n\nTopography\nRugged\nRugged\n\n\nWar duration\n126 days\n378 days\n\n\nRecent war history w/opp\nYes\nYes\n\n\nFacing colonizer\nYes\nYes\n\n\nIdentity dimension\nSunni Islam/Christian\nSunni Islam/Christian\n\n\nNew leader\nYes\nYes\n\n\nPopulation\n8‚Äì8.5 million\n5‚Äì6 million\n\n\nEthnoling fractionalization (ELF)\nHigh\nHigh\n\n\nCiv-mil relations\nRuler as commander\nRuler as commander\n\n\n\\(Y\\): Battlefield Performance:\n\n\n\n\nLoss-exchange ratio\n0.43\n0.02\n\n\nMass desertion\nNo\nYes\n\n\nMass defection\nNo\nNo\n\n\nFratricidal violence\nNo\nYes",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#particularly-fun-non-standard-examples",
    "href": "w01/index.html#particularly-fun-non-standard-examples",
    "title": "Week 1: Science from Particles to People",
    "section": "Particularly Fun Non-‚ÄúStandard‚Äù Examples",
    "text": "Particularly Fun Non-‚ÄúStandard‚Äù Examples\n\nDeDeo, French Revolution\nGrimmer, Mirrors for Sultans and Princes",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#jupyterhub",
    "href": "w01/index.html#jupyterhub",
    "title": "Week 1: Science from Particles to People",
    "section": "JupyterHub",
    "text": "JupyterHub\n\nhttps://guhub.io\n\n\n\n\nImage source",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#references",
    "href": "w01/index.html#references",
    "title": "Week 1: Science from Particles to People",
    "section": "References",
    "text": "References\n\n\nKalyvas, Stathis N. 2006. The Logic of Violence in Civil War. Cambridge University Press. https://www.dropbox.com/scl/fi/4es1r44ldnqtcroonr5g7/The-Logic-of-Violence-in-Civil-War.pdf?rlkey=uonfmw5zkx1ja5l8emv2bzfmg&dl=1.\n\n\nLyall, Jason. 2020. Divided Armies: Inequality and Battlefield Performance in Modern War. Princeton University Press.\n\n\nMontessori, Maria. 1916. Spontaneous Activity in Education: A Basic Guide to the Montessori Methods of Learning in the Classroom. Lulu Press.\n\n\nSperber, Dan. 1996. Explaining Culture: A Naturalistic Approach. Cambridge: Blackwell. https://www.dropbox.com/scl/fi/he6idaajr6vopybv13qhd/Explaining-Culture-A-Naturalistic-Approach.pdf?rlkey=8oyhnsu5ycet4osstft252jaw&dl=1.",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#footnotes",
    "href": "w01/index.html#footnotes",
    "title": "Week 1: Science from Particles to People",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúCouldn‚Äôt do classes, them lessons used to bore me, I was tryna school heads like Waldorf and Montessori‚Äù -Bar dropped by local rapper one day in freestyle that I never forgot‚Ü©Ô∏é\nObligatory quantum mechanics footnote: human agency [maybe] plays a role, in a quirky way, in physics at tiny subatomic scales‚Ä¶ but once we coarse grain to atoms (and avoid speed of light), Newton‚Äôs Laws accurate to many decimals ü§Ø‚Ü©Ô∏é\nAt least, for the time being‚Ä¶ BUT see Sperber (1996), which will come up later‚Ü©Ô∏é",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5650: Causal Inference for Computational Social Science",
    "section": "",
    "text": "Welcome to the homepage for DSAN 5650: Causal Inference for Computational Social Science at Georgetown University, for the Summer 2025 session!\nThe course meets on Wednesdays from 6:30pm to 9:00pm online, via the Zoom Link provided in the sidebar.\nCheck out the syllabus (or any other link in the sidebar) for more info! Or, use the following links to view notes for individual weeks:\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1: Science from Particles to People\n\n\nMay 21\n\n\n\n\nWeek 2: Probabilistic Graphical Models (PGMs)\n\n\nMay 28\n\n\n\n\nWeek 3: From PGMs to Causal Diagrams\n\n\nJune 4\n\n\n\n\nWeek 4: Clearing the Path from Cause to Effect\n\n\nJune 11\n\n\n\n\nWeek 5: Multilevel Madness, Closing Backdoor Paths\n\n\nJune 18\n\n\n\n\nWeek 6: Bayesian Workflow, Midterm Pre-Review\n\n\nJune 25\n\n\n\n\nWeek 7: Midterm Introduction\n\n\nJuly 2\n\n\n\n\n\nNo matching items\n\n\nCourse Description:\nThis course provides students with the opportunity to take the analytical skills, machine learning algorithms, and statistical methods learned throughout their first year in the program and explore how they can be employed towards carrying out rigorous, original research in the behavioral and social sciences. With a particular emphasis on tackling the additional challenges which arise when moving from associational to causal inference, particularly when only observational (as opposed to experimental) data is available, students will become proficient in cutting-edge causal Machine Learning techniques such as propensity score matching, synthetic controls, causal program evaluation, inverse social welfare function estimation from panel data, and Double-Debiased Machine Learning.\nIn-class examples will cover continuous, discrete-choice, and textual data from a wide swath of social and behavioral sciences: economics, political science, sociology, anthropology, quantitative history, and digital humanities. After gaining experience through in-class labs and homework assignments focused on reproducing key findings from recent journal articles in each of these disciplines, students will spend the final weeks of the course on a final project demonstrating their ability to develop, evaluate, and test the robustness of a causal hypothesis.\nPrerequisites: DSAN 5000, DSAN 5100 (DSAN 5300 recommended but not required)",
    "crumbs": [
      "<i class='bi bi-house pe-1'></i> Home"
    ]
  },
  {
    "objectID": "writeups/bayesian-workflow/Bayesian_Workflow.html",
    "href": "writeups/bayesian-workflow/Bayesian_Workflow.html",
    "title": "[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure",
    "section": "",
    "text": "%%html\n&lt;style&gt;\n@import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100..900;1,100..900&display=swap');\n.jp-RenderedHTMLCommon {\n    font-family: \"Roboto\", sans-serif;\n}\n&lt;/style&gt;\nimport os\nos.environ[ 'MPLCONFIGDIR' ] = './tmp/'\n%config IPCompleter.use_jedi = False\n\nimport pandas as pd\nimport numpy as np\nrng = np.random.default_rng(seed=5660)\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\n# sns.set_style('whitegrid')\ncb_palette = ['#e69f00','#56b4e9','#009e73']\nsns.set_palette(cb_palette)\nimport patchworklib as pw;\n\nimport pymc as pm\nimport arviz as az\n\n&lt;Figure size 100x100 with 0 Axes&gt;"
  },
  {
    "objectID": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-1-elden-coin-the-immersive-coin-flipping-rpg-thats-taking-the-world-by-storm",
    "href": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-1-elden-coin-the-immersive-coin-flipping-rpg-thats-taking-the-world-by-storm",
    "title": "[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure",
    "section": "[Part 1] Elden Coin: The Immersive Coin-Flipping RPG That‚Äôs Taking the World By Storm!",
    "text": "[Part 1] Elden Coin: The Immersive Coin-Flipping RPG That‚Äôs Taking the World By Storm!\n\n[Part 1.1] The Rules of the Game\n\nA friend \\(B\\) challenges you (\\(A\\)) to a match of Elden Coin (\\(B\\) provides their Elden Coin, as well as the table to play on, which is a key part of the game), a tabletop coin-flipping game\nA match is a series of 50 rounds\nAt the start of each round, both you and \\(B\\) put one Euro (‚Ç¨1) into the pot on the side of the table, then write a big ‚Äú0‚Äù in chalk on the other side\nThen, the round consists of you and \\(B\\) taking turns flipping the Elden Coin in the center of the table until you‚Äôve reached 13 total flips\nEvery time a flip comes up Heads, you add one to the number written in chalk (by erasing and rewriting it)\nAt the end of the round:\n\nIf the number of heads written down is above 6, you (\\(A\\)) win the entire pot (‚Ç¨2), meaning that you gain an additional Euro above the amount you entered the round with.\nOtherwise, \\(B\\) wins the entire pot, meaning that you lose one Euro relative to the amount you entered the round with.\n\n\nThe following cell defines a Game class, that we can use to store the different parameters we‚Äôll need when simulating a match.\n\nclass Game:\n    flips_per_round = 13\n    num_heads_range = (0, flips_per_round)\n    num_heads_support = list(range(0, flips_per_round + 1))\n    win_cutoff = 6\n    rounds_per_match = 50\n\nThen, on top of just the rules of the game itself, here we define a separate set of globals ‚Äúoutside of‚Äù the game, specifying the number of simulations of the game we‚Äôd like to run in our simulation cells (so that, for example, you can set it to lower values if running on a slower computer or if you just want the cells to run more quickly!)\n\nclass MyGlobals:\n    # If we're trying to get a distribution over what might happen in *one round*\n    # of the game (with outcomes Win or Lose), we'll use this many simulations\n    rounds_to_sim = 10_000\n    # Otherwise, for obtaining a distribution over what might happen in a *match*\n    # (with the outcome being the total amount of money won or lost), we'll use\n    # this many\n    matches_to_sim = 1_000\n\n\n\n[Part 1.2] Simulating a Round\nIf we truly wanted to work at the most fine-grained level, we could simulate individual coin flips, one-by-one\n\ndef sim_round_flips(p):\n    round_flips = rng.choice([0,1],p=[1-p, p], size=Game.flips_per_round)\n    return round_flips\nprint(sim_round_flips(0.5))\nprint(sim_round_flips(0.85))\n\n[0 0 1 0 0 0 0 0 1 0 1 0 1]\n[1 0 1 1 1 1 1 1 1 1 0 0 1]\n\n\nThen, to check the outcome at the level of number-of-heads, we could just use sum():\n\nsum(sim_round_flips(0.5))\n\n6\n\n\nBut, if we‚Äôre going to just sum up the individual coin flips anyways, we can achieve a pretty massive speed improvement (not noticeable for only one or a few rounds, but massive once we move to the level of simulating thousands of rounds) by using rng.binomial(), that is, by operating at the coarser-grained level of generating [number of heads out of 9 flips] rather than individual 0/1 heads/tails values:\n\ndef sim_round(p):\n    num_heads = int(rng.binomial(n=Game.flips_per_round, p=p, size=1))\n    return num_heads\nsim_round(0.5)\n\n6\n\n\nSo, let‚Äôs do some simulations (we‚Äôll use MyGlobals.rounds_to_sim, set above, as our number of simulations \\(N\\)), by running this function a bunch of times, to see how often the number of heads is above 6 when the coin is fair‚Ä¶\n\ndef get_counts(df):\n    support_df = pd.DataFrame({'num_heads': Game.num_heads_support})\n    count_df = df['num_heads'].value_counts().reset_index().sort_values(by='num_heads')\n    merged_df = support_df.merge(count_df, on='num_heads', how='left')\n    merged_df['win'] = merged_df['num_heads'] &gt; Game.win_cutoff\n    merged_df['count'] = merged_df['count'].replace(np.nan, 0).astype(int)\n    merged_df['Pr(num_heads)'] = merged_df['count'] / merged_df['count'].sum()\n    return merged_df\n\nfair_sim_result = [sim_round(0.5) for _ in range(MyGlobals.rounds_to_sim)]\nfair_sim_df = pd.DataFrame({'num_heads': fair_sim_result, 'params': 'p = 0.5'})\nfair_sim_df['win'] = fair_sim_df['num_heads'] &gt; Game.win_cutoff\nfair_count_df = get_counts(fair_sim_df)\nfair_count_df.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\nnum_heads\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\ncount\n0\n22\n116\n376\n874\n1593\n1990\n2065\n1577\n890\n382\n101\n13\n1\n\n\nwin\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\nPr(num_heads)\n0.0\n0.0022\n0.0116\n0.0376\n0.0874\n0.1593\n0.199\n0.2065\n0.1577\n0.089\n0.0382\n0.0101\n0.0013\n0.0001\n\n\n\n\n\n\n\nAnd we can plot the distribution of these simulated outcomes, assigning different colors to the outcomes where we win and where we lose:\n\nax = pw.Brick(figsize=(3.5,2.25))\ng = sns.barplot(\n    x=\"num_heads\", y=\"Pr(num_heads)\", hue=\"win\", data=fair_count_df,\n    alpha=0.75, ax=ax\n);\nax.axvline(x=6.5, ls='dashed', color='black', alpha=0.9);\nax.set_title(f\"N = {len(fair_sim_result)} Simulated Rounds, p = 0.5\")\nax.savefig()\n\n\n\n\n\n\n\n\nTo make it even more straightforward to see the fairness of the game (given a fair coin!), we can just plot win and loss proportions directly:\n\nfair_win_df = fair_count_df[['win','count']].groupby(\"win\").sum().reset_index()\nfair_win_df\n\n\n\n\n\n\n\n\nwin\ncount\n\n\n\n\n0\nFalse\n4971\n\n\n1\nTrue\n5029\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5,2.25))\nwin_plot = sns.barplot(\n    x=\"win\", y=\"count\", hue=\"win\",\n    data=fair_win_df,\n    alpha=0.75, ax=ax, legend=False\n);\nwin_plot.set_title(\"Wins vs. Losses\")\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n[Part 1.3] Checking the Math\nSo far, individual rounds of this game look pretty fair. But, can we be mathematically sure they are?\nLet‚Äôs say we didn‚Äôt even trust this \\(N = 10000\\) simulation. Something that I think is good to have in your toolkit, for checking the math on this stuff without needing to take out a piece of paper, is a symbolic mathematics system!\nSymPy, in this case, can be used to derive exact closed-form solutions (when possible) to probability theory problems, so that you don‚Äôt even have to depend on running simulations for simple enough problems! You can find the documentation for the SymPy stats module here, but the following cells should give you the gist as we check the math of the game‚Äôs fairness.\nAs a quick demonstration of why SymPy is useful, let‚Äôs say we‚Äôre trying to remember the formula for the sum of the first \\(n\\) natural numbers‚Ä¶ Is Python able to help us here on its own?\n\ndef sum_up_to(n):\n    return sum(range(1, n+1))\n\n\nsum_df = pd.DataFrame({'n': list(range(1, 7))})\nsum_df['sum_1_to_n'] = sum_df['n'].apply(sum_up_to)\nsum_df.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nn\n1\n2\n3\n4\n5\n6\n\n\nsum_1_to_n\n1\n3\n6\n10\n15\n21\n\n\n\n\n\n\n\nOne option would be to stare at this and try to figure out the pattern until we remember‚Ä¶ Which is fine but, my brain is too tired to do that, which is why I wanted the computer to do it for me in the first place! Instead, let‚Äôs let SymPy try it:\n\nfrom sympy import expand, oo, plot, simplify, symbols, Eq, Rational, Sum\n\n\ni = symbols('i', integer=True)\nn = symbols('n', integer=True)\nmy_sum = Sum(i, (i, 1, n))\nmy_sum\n\n\\(\\displaystyle \\sum_{i=1}^{n} i\\)\n\n\nThis is the thing we‚Äôre trying to remember the formula for‚Ä¶ so let‚Äôs as SymPy to .doit()!\n\nmy_result = my_sum.doit()\nmy_result\n\n\\(\\displaystyle \\frac{n^{2}}{2} + \\frac{n}{2}\\)\n\n\n‚Ä¶Already very cool, but we can get it in an even more familiar form by using simplify() as well:\n\nsimplify(my_result)\n\n\\(\\displaystyle \\frac{n \\left(n + 1\\right)}{2}\\)\n\n\nAnd we can ask it to derive exact, closed-form solutions for a bunch of other things too! For now, we can try sums of squares‚Ä¶\n\nsimplify(Sum(i**2, (i, 1, n)).doit())\n\n\\(\\displaystyle \\frac{n \\left(2 n^{2} + 3 n + 1\\right)}{6}\\)\n\n\nOr the sum of a geometric series \\(\\sum_{i=1}^{\\infty}ar^i\\)‚Ä¶\n\na = symbols('a', nonnegative=True)\nr = symbols('r', nonnegative=True)\nmy_geo_sum = Sum(a * (r ** i), (i, 0, oo))\nmy_geo_result = my_geo_sum.doit()\nmy_geo_result\n\n\\(\\displaystyle a \\left(\\begin{cases} \\frac{1}{1 - r} & \\text{for}\\: r &lt; 1 \\\\\\sum_{i=0}^{\\infty} r^{i} & \\text{otherwise} \\end{cases}\\right)\\)\n\n\nAnd now that we have the formula, we could plug in a specific value for \\(r\\) as well (here \\(r = \\frac{1}{5}\\)):\n\nmy_geo_result.subs(r, Rational(1,5))\n\n\\(\\displaystyle \\frac{5 a}{4}\\)\n\n\nNow we can import from the sympy.stats module, and compute exact values for the probabilities of each possible round result!\n\nfrom sympy.stats import P, E, density, Bernoulli, DiscreteUniform\n\nDefining a sequence of i.i.d. RVs in SymPy is a bit tough in general (meaning, for example, it‚Äôd be tough to get it to prove the Central Limit Theorem), but, for simple cases like this where we‚Äôre summing up a finite number of coin flips, the following works!\nWe‚Äôll use Z to represent the sum of the 13 coin flips, or in other words, the number of heads in a round of Elden Coin, now modeled exactly (without rounding or approximations), rather than simulated:\n\ncoins = [Bernoulli(f'X{i}', p=Rational(1,2)) for i in range(0, Game.flips_per_round)]\nZ = sum(coins)\nZ\n\n\\(\\displaystyle X_{0} + X_{1} + X_{10} + X_{11} + X_{12} + X_{2} + X_{3} + X_{4} + X_{5} + X_{6} + X_{7} + X_{8} + X_{9}\\)\n\n\nWe can use the E() function imported above to get the exact expected value of the sum of 13 coin flips:\n\nE(Z)\n\n\\(\\displaystyle \\frac{13}{2}\\)\n\n\nAnd then the P() function to compute probabilities. For example, we can use Python‚Äôs built-in inequality operator &gt; in this case to find the thing we‚Äôre looking for: the game is fair if \\(\\Pr(Z &gt; 6)\\) is exactly \\(1/2\\):\n\nP(Z &gt; Game.win_cutoff)\n\n\\(\\displaystyle \\frac{1}{2}\\)\n\n\nAs a warning if you decide to use SymPy for other stuff, though: it does not work well with Python‚Äôs equality operator ==! For example, if you want to find the probability of a dice roll coming up 5, you might think of the following:\n\nD = DiscreteUniform('D', list(range(1, 7)))\nP(D == 5)\n\n\\(\\displaystyle 0\\)\n\n\nAwful‚Ä¶ üòµ Instead, we have to import SymPy‚Äôs symbolic version of ==, namely, the equality function Eq(). If we use this instead of ==, we‚Äôll (thankfully) get the expected result!\n\nP(Eq(D, 5))\n\n\\(\\displaystyle \\frac{1}{6}\\)\n\n\nOk! Now that you have a feel for SymPy, as a final helpful use for it here let‚Äôs derive the exact probability values for the different outcomes of a round of Elden Coin. For that, we can use SymPy‚Äôs density() function on a Random Variable, in this case on \\(Z\\):\n\npmf_Z = density(Z)\npmf_Z\n\n{0: 1/8192,\n 1: 13/8192,\n 2: 39/4096,\n 3: 143/4096,\n 4: 715/8192,\n 5: 1287/8192,\n 6: 429/2048,\n 7: 429/2048,\n 8: 1287/8192,\n 9: 715/8192,\n 10: 143/4096,\n 11: 39/4096,\n 12: 13/8192,\n 13: 1/8192}\n\n\nAnd then, for easier viewing/plotting, we can form a Pandas DataFrame from these values:\n\nZ_df = pd.DataFrame({'X':pmf_Z.keys(), 'Pr(X)':pmf_Z.values()})\nZ_df.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\nX\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nPr(X)\n1/8192\n13/8192\n39/4096\n143/4096\n715/8192\n1287/8192\n429/2048\n429/2048\n1287/8192\n715/8192\n143/4096\n39/4096\n13/8192\n1/8192\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5,2.25))\nax.stem(\"X\", \"Pr(X)\", data=Z_df, basefmt='grey');\nax.set_title(\"Exact Values of Distribution\");\nax.set_xlabel(\"Num Heads\");\nax.set_ylabel(\"Probability\")\n# ax.grid(False);\n#ax.set_facecolor('white');\nax.savefig()\n\n\n\n\n\n\n\n\nSadly though, if you‚Äôre like me and you prefer seaborn to ‚Äúbase‚Äù matplotlib, seaborn doesn‚Äôt work well with the exact-numeric-value format of SymPy for fractions. So, for seaborn plotting you‚Äôll have to convert to (non-exact) Python float values. But still, you‚Äôll be plotting the closest possible float approximation to the true values!\n\napprox_df = Z_df.copy()\napprox_df['X'] = approx_df['X'].astype(int)\napprox_df['Pr(X)'] = approx_df['Pr(X)'].astype(float)\napprox_df.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\nX\n0.000000\n1.000000\n2.000000\n3.000000\n4.00000\n5.000000\n6.000000\n7.000000\n8.000000\n9.00000\n10.000000\n11.000000\n12.000000\n13.000000\n\n\nPr(X)\n0.000122\n0.001587\n0.009521\n0.034912\n0.08728\n0.157104\n0.209473\n0.209473\n0.157104\n0.08728\n0.034912\n0.009521\n0.001587\n0.000122\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5,2.25))\nsns.barplot(\n    x=\"X\", y=\"Pr(X)\", data=approx_df,\n    ax=ax, alpha=0.75\n);\nax.set_title(\"(Approximately-Exact) Values of Distribution\")\n# ax.grid(False);\n#ax.set_facecolor('white');\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n[Part 1.4] Simulating Many Rounds\nNow that we‚Äôve convinced ourselves, both via simulation and exact-math, that a single round is fair, let‚Äôs move to simulating matches, comprised of 50 rounds! This is what we really care about, since it will tell us the range our winnings/losings might take on at the end of a full match.\nThe functions in the following cell carry out simulations at more coarse-grained levels than an individual round:\nsim_match(p) simulates a 50-round match and produces a Pandas DataFrame with the results:\n\nt reprsents the round number (We add a special \\(t = 0\\) ‚Äúround‚Äù to mark the fact that we start with $0 at the beginning of the game, which will make the plots below more easy to read)\nnum_heads represents the number of heads at round t\nmoney_change is either \\(-1\\) or \\(1\\): \\(-1\\) if num_heads was less than 6, and \\(1\\) otherwise, representing the net change in your money at the end of the round\nmoney_at_round_end represents the cumulative sum of money_change from round to round, so that at any round this contains the current overall winnings/losings relative to the 0 ‚Ç¨ starting point at \\(t = 0\\)\n\nsim_outcome(p) allows us to get a sense for the ranges of values that can come out of a game, by simulating MyGlobals.matches_to_sim matches, and returning two Pandas DataFrames giving us different pieces of information:\n\nIn final_df, each row represents a single round within one of the simulated matches. We can use this to track the trajectory of money over the course of a match, and then, since we‚Äôre simulating many matches, we‚Äôll in fact get a distribution over trajectories, which will help us visualize our expectations about how much money we may win or lose in a given match.\nIn end_df, each row represents the final round of a simulated match. As we‚Äôll see in the plots below, this allows us to visualize the distribution over final winnings/losings we can expect to see for a given Elden Coin.\n\nFinally, get_result_label(money_at_end) is just a helper function for sim_outcome(p): it allows us to ‚Äúmap‚Äù the money_at_round_end values from sim_match() to a three-level Random Variable called result, which will have value:\n\n\"Gained Money\" if we ended the match with more money than we started with,\n\"Lost Money\" if we ended the match with less money than we started with, and\n\"Broke Even\" if we ended the match with exactly the same amount of money that we started with.\n\n\ndef sim_match(p):\n    round_outcomes = rng.binomial(n=Game.flips_per_round, p=p, size=Game.rounds_per_match)\n    match_df = pd.DataFrame({'t': range(1,Game.rounds_per_match+1), 'num_heads': round_outcomes})\n    match_df['money_change'] = -1 + 2 * (match_df['num_heads'] &gt; Game.win_cutoff)\n    match_df['money_at_round_end'] = match_df['money_change'].cumsum()\n    t0_df = pd.DataFrame({'t':[0],'money_at_round_end':[0],'money_change':[0]})\n    match_df = pd.concat([t0_df, match_df])\n    return(match_df)\n\nresult_order = ['Lost Money', 'Broke Even', 'Gained Money']\ndef get_result_label(money_at_end):\n    if money_at_end &gt; 0:\n        return \"Gained Money\"\n    if money_at_end &lt; 0:\n        return \"Lost Money\"\n    return \"Broke Even\"\n\ndef sim_outcome(p):  \n    all_match_dfs = []\n    for m in range(MyGlobals.matches_to_sim):\n        match_df = sim_match(p)\n        match_df['match_num'] = m\n        all_match_dfs.append(match_df)\n    # All the data\n    final_df = pd.concat(all_match_dfs)\n    final_df.insert(0, 'match_num', final_df.pop('match_num'))\n    final_df['params'] = f\"p = {p}\"\n    # Just the last round money_at_end\n    end_df = final_df[final_df['t'] == Game.rounds_per_match].copy()\n    end_df = end_df[['match_num','t','money_at_round_end']]\n    end_df['result'] = end_df['money_at_round_end'].apply(get_result_label)\n    end_df['result'] = pd.Categorical(\n        end_df['result'],\n        ordered=True,\n        categories=result_order\n    )\n    end_df['params'] = f\"p = {p}\"\n    return final_df, end_df\nfair_final_df, fair_end_df = sim_outcome(0.5)\nprint(\"fair_final_df:\")\ndisplay(fair_final_df.head())\nprint(\"fair_end_df:\")\ndisplay(fair_end_df.head())\n\nfair_final_df:\n\n\n\n\n\n\n\n\n\nmatch_num\nt\nmoney_at_round_end\nmoney_change\nnum_heads\nparams\n\n\n\n\n0\n0\n0\n0\n0\nNaN\np = 0.5\n\n\n0\n0\n1\n1\n1\n8.0\np = 0.5\n\n\n1\n0\n2\n0\n-1\n6.0\np = 0.5\n\n\n2\n0\n3\n1\n1\n9.0\np = 0.5\n\n\n3\n0\n4\n2\n1\n8.0\np = 0.5\n\n\n\n\n\n\n\nfair_end_df:\n\n\n\n\n\n\n\n\n\nmatch_num\nt\nmoney_at_round_end\nresult\nparams\n\n\n\n\n49\n0\n50\n2\nGained Money\np = 0.5\n\n\n49\n1\n50\n-10\nLost Money\np = 0.5\n\n\n49\n2\n50\n-4\nLost Money\np = 0.5\n\n\n49\n3\n50\n16\nGained Money\np = 0.5\n\n\n49\n4\n50\n-2\nLost Money\np = 0.5\n\n\n\n\n\n\n\nWith fair_final_df and fair_end_df now ready for use, we can plot the range of trajectories (from \\(t = 0\\) to \\(t = 50\\)) that a match might exhibit, as well as the distribution of final winnings/losings that we may obtain at the end of a match under the specified Elden Coin value of \\(p\\).\nFor now we‚Äôll plot what matches with a fair coin might look like, but in the next section we‚Äôll produce the same kind of plot for a biased coin!\nThe most straightforward way to visualize the trajectories is probably just to plot every ‚Äúpath‚Äù through the game space, with a low alpha value so that the more-frequent paths appear as a darker orange:\n\ndef plot_trajectories(final_df, end_df, suptitle, custom_ylim=None):\n    if custom_ylim is None:\n        grid = sns.JointGrid(height=4.5);\n    else:\n        grid = sns.JointGrid(height=4.5, ylim=custom_ylim);\n    fair_trajectory_plot = sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", data=final_df, ax=grid.ax_joint,\n        units='match_num', estimator=None, alpha=0.02,\n    );\n    # Breaking-even line\n    grid.refline(\n        y=0, color='black', lw=1, ls='solid', alpha=0.5, label=\"Breaking Even\"\n    );\n    # Add the median line in dashed black\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", # hue=\"source\",\n        data=final_df, ax=grid.ax_joint,\n        label=\"Median\",\n        estimator='median',\n        errorbar=None,\n        # errorbar=('pi',90),\n        color='black',\n        lw=1, ls='dashed',\n        err_kws=dict(alpha=0.15),\n    );\n    # And mean line in dotted black\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", # hue=\"source\",\n        data=final_df, ax=grid.ax_joint,\n        label=\"Mean\",\n        estimator='mean',\n        errorbar=None,\n        # errorbar=('pi',90),\n        color='black',\n        lw=1, ls='dotted',\n        err_kws=dict(alpha=0.15),\n    );\n    grid.ax_marg_x.remove();\n    grid.ax_marg_y.axhline(\n        y=end_df['money_at_round_end'].mean(),\n        color='#e69f00', lw=2\n    );\n    # Marginal axis\n    winnings_plot = sns.histplot(\n        y=\"money_at_round_end\", data=end_df,\n        ax=grid.ax_marg_y,\n        discrete=True,\n    );\n    # Dashed line on margin for median\n    grid.ax_marg_y.axhline(\n        y=end_df['money_at_round_end'].median(), color='black', ls='dashed', alpha=0.9\n    );\n    # Dotted line on margin for mean\n    grid.ax_marg_y.axhline(\n        y=end_df['money_at_round_end'].mean(), color='black', ls='dotted', alpha=0.9\n    );\n    grid.ax_joint.set_title(\"Round Level\");\n    grid.ax_marg_y.set_title(\"Match Level\");\n    grid.fig.set_figwidth(10);\n    grid.fig.suptitle(suptitle);\n    grid.ax_joint.set_ylabel(\"Money at Round End\");\n    grid.ax_joint.legend(loc=\"upper left\");\n    return grid\nplot_trajectories(\n    fair_final_df, fair_end_df,\n    suptitle=\"Trajectories and Winnings for 1000 Matches, p = 0.5\"\n);\n\n\n\n\n\n\n\n\nThough this plot is helpful for seeing the ‚Äúemergent‚Äù distribution of winnings relative to the underlying coin flips, it also makes it difficult to see the median or mean pathways/winnings ‚Äì these lines will be difficult to see in this case either way, because they‚Äôre close to 0 throughout, but being able to see them a bit better will be helpful when we move to biased coins.\nSo, as an alternative that we‚Äôll use from here on out, we can just plot a series of lighter ‚Äúbands‚Äù around the median pathway, containing (from darkest to lightest) the middle 50, 90, and 99 percentiles of money_at_round_end across all simulations (note that the band containing the middle 99 percentiles may be cut off, since it may reach extreme negative and positive values when we simulate many matches!)\n\ndef plot_trajectory_bands(final_df, end_df, suptitle, custom_ylim=None):\n    if custom_ylim is not None:\n        fair_grid_var = sns.JointGrid(height=5, ylim=(-22,22));\n    else:\n        fair_grid_var = sns.JointGrid(height=5);\n    # Don't need marginal plot on x-axis\n    fair_grid_var.ax_marg_x.remove();\n    # This ensures that the legend doesn't include params 3 different times\n    ftemp_df = final_df.copy()\n    ftemp_df['params'] = \"_\" + ftemp_df['params']\n    # Band containing the middle 100 percentiles of all trajectories\n    full_range_plot = sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", hue=\"params\",\n        data=ftemp_df, ax=fair_grid_var.ax_joint,\n        # color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',99),\n        lw=1,\n        err_kws=dict(alpha=0.125),\n    );\n    # Band containing middle 90 percentiles\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", hue=\"params\",\n        data=ftemp_df, ax=fair_grid_var.ax_joint,\n        # color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',90),\n        lw=1,\n        err_kws=dict(alpha=0.25),\n    );\n    # Band containing middle 50 percentiles\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", hue=\"params\",\n        data=ftemp_df, ax=fair_grid_var.ax_joint,\n        # color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',50),\n        lw=1,\n        err_kws=dict(alpha=0.5),\n    );\n    # Median trajectory as dashed black line\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", label=\"Median of Simulated Rounds\",\n        data=ftemp_df, ax=fair_grid_var.ax_joint,\n        color='black', alpha=0.9,\n        estimator='median',\n        errorbar=None,\n        lw=1, ls='dashed',\n    );\n    # Mean trajectory as dotted black line\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", label=\"Mean of Simulated Rounds\",\n        data=ftemp_df, ax=fair_grid_var.ax_joint,\n        lw=1, ls='dotted', color='black', alpha=0.9,\n        estimator='mean',\n        errorbar=None,\n    );\n    # Refline links joint and marginal plots\n    fair_grid_var.refline(\n        y=0, color='black', lw=1, ls='solid', label='Breaking Even', alpha=0.5\n    );\n    fair_grid_var.ax_joint.legend(loc=\"lower left\");\n\n    ### Marginal plot\n    margin_kde = sns.histplot(\n        y=\"money_at_round_end\", data=end_df,\n        ax=fair_grid_var.ax_marg_y,\n        discrete=True,\n        # fill=True,\n        legend=False,\n    );\n    margin_kde = sns.kdeplot(\n        y=\"money_at_round_end\", data=end_df,\n        ax=fair_grid_var.ax_marg_y,\n        fill=True,\n        legend=False,\n    );\n    # Dashed line on margin for median\n    fair_grid_var.ax_marg_y.axhline(\n        y=end_df['money_at_round_end'].median(), color='black', ls='dashed', alpha=0.9\n    );\n    # Dotted line on margin for mean\n    fair_grid_var.ax_marg_y.axhline(\n        y=end_df['money_at_round_end'].mean(), color='black', ls='dotted', alpha=0.9\n    );\n    fair_grid_var.fig.suptitle(suptitle);\n    fair_grid_var.ax_joint.set_ylabel(\"Money at Round End\");\n    fair_grid_var.ax_joint.set_title(\"Round Level\");\n    fair_grid_var.ax_marg_y.set_title(\"Match Level\");\n    fair_grid_var.fig.set_figwidth(9.5);\n    return fair_grid_var\nplot_trajectory_bands(\n    fair_final_df, fair_end_df, custom_ylim=(-25, 25),\n    suptitle=\"Distribution of Trajectories and Winnings for 1000 Matches, p = 0.5\"\n);\n\n\n\n\n\n\n\n\nAnd, like we did with Win vs.¬†Loss for a single round, here we can more simply plot the three outcomes we‚Äôve defined at the match level:\n\nfair_result_df = fair_end_df['result'].value_counts(sort=False, normalize=True).to_frame().reset_index()\nfair_result_df\n\n\n\n\n\n\n\n\nresult\nproportion\n\n\n\n\n0\nLost Money\n0.440\n\n\n1\nBroke Even\n0.117\n\n\n2\nGained Money\n0.443\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.barplot(\n    x=\"result\", y=\"proportion\", data=fair_result_df,\n    alpha=0.8, ax=ax\n);\nax.set_title(\"Outcomes for 1000 Matches, p = 0.5\");\nax.savefig()"
  },
  {
    "objectID": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-2-simulating-the-unlockable-sheisty-coin",
    "href": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-2-simulating-the-unlockable-sheisty-coin",
    "title": "[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure",
    "section": "[Part 2] Simulating the Unlockable Sheisty Coin",
    "text": "[Part 2] Simulating the Unlockable Sheisty Coin\nThe only problem is‚Ä¶ in the new Season 2 of Elden Coin, the company who makes the game (ThumbSoft) has added a new powerup shop, which means that now your friend \\(B\\) may have used their Elden Coin earnings to buy a powerup which secretly biases their Elden Coin in their favor, turning it into a new evolved form, the Shiesty Coin. When someone buys this powerup, it adds a randomly-generated amount of bias to their coin, so that its new probability of Heads is some value (uniformly) between \\(0.4\\) and \\(0.5\\).\nSo, this means there are two new ‚Äúlayers‚Äù of uncertainty in a sense:\n\nYou don‚Äôt know whether or not your friend has a Sheisty Coin at all, and\nYou don‚Äôt know how much bias was added to their coin if they did purchase the upgrade.\n\nThe eventual goal will be to use PyMC to model this uncertainty explicitly, allowing you to make optimal gameplay choices!\nFor now, let‚Äôs simulate what rounds may look like, then what match-level trajectories and winnings may look like, under a specific biased-coin scenario.\n\n[Part 2.1] Simulating Biased Round Outcomes\nThe following cells generate and then plot a distribution of simulated round outcomes for a Sheisty Coin biased exactly in the middle of the possible range, with \\(p = 0.45\\):\n\np_biased = 0.45\n\n\nbiased_sim_result = [sim_round(p_biased) for _ in range(MyGlobals.rounds_to_sim)]\nbiased_sim_df = pd.DataFrame({'num_heads': biased_sim_result, 'params': 'p = 0.45'})\nbiased_sim_df['win'] = biased_sim_df['num_heads'] &gt; Game.win_cutoff\nbiased_count_df = get_counts(biased_sim_df)\nbiased_count_df.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\nnum_heads\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\ncount\n8\n44\n242\n665\n1353\n1983\n2177\n1699\n1150\n479\n160\n38\n2\n0\n\n\nwin\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\nPr(num_heads)\n0.0008\n0.0044\n0.0242\n0.0665\n0.1353\n0.1983\n0.2177\n0.1699\n0.115\n0.0479\n0.016\n0.0038\n0.0002\n0.0\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5,2.25))\ng = sns.barplot(\n    x=\"num_heads\", y=\"Pr(num_heads)\", hue=\"win\", data=biased_count_df,\n    alpha=0.75, ax=ax\n);\nax.axvline(x=6.5, ls='dashed', color='black', alpha=0.9);\nax.set_title(f\"N = {len(biased_sim_result)} Simulated Rounds, p = 0.5\")\nax.savefig()\n\n\n\n\n\n\n\n\nAnd we can also plot the kernel densities of the two num_heads distributions, to compare the biased and fair outcomes visually:\n\ncombined_sim_df = pd.concat([fair_sim_df, biased_sim_df])\ncombined_sim_df\n\n# Histogram (confusing imo)\nax1 = pw.Brick(figsize=(4.5,2.5))\nsns.histplot(\n    x=\"num_heads\", hue=\"params\", data=combined_sim_df, ax=ax1,\n    discrete=True\n    # fill=True, bw_adjust=2\n);\nax1.axvline(x=6.5, ls='dashed', color='black', alpha=0.9);\n# KDE Plot (more helpful imo!)\nax2 = pw.Brick(figsize=(4.5, 2.5))\nsns.kdeplot(\n    x=\"num_heads\", hue=\"params\", data=combined_sim_df, ax=ax2,\n    fill=True, bw_adjust=2\n);\nax12 = ax1 | ax2\nax12.set_suptitle(f\"N = {len(biased_sim_result)} Simulated Rounds\")\nax12.savefig()\n\n\n\n\n\n\n\n\n\n\n[Part 2.2] Simulating Biased Match Outcomes\nThe above plot shows the bias relative to a single round of the game, but let‚Äôs also generate a plot to see what this looks like when we run 50 matches. We‚Äôll plot both the \\(p = 0.5\\) and \\(p = 0.45\\) trajectories so you can get a comparative sense:\n\nbiased_final_df, biased_end_df = sim_outcome(p_biased)\nprint(\"biased_final_df:\")\ndisplay(biased_final_df.head())\nprint(\"biased_end_df:\")\ndisplay(biased_end_df.head())\n\nbiased_final_df:\n\n\n\n\n\n\n\n\n\nmatch_num\nt\nmoney_at_round_end\nmoney_change\nnum_heads\nparams\n\n\n\n\n0\n0\n0\n0\n0\nNaN\np = 0.45\n\n\n0\n0\n1\n-1\n-1\n5.0\np = 0.45\n\n\n1\n0\n2\n0\n1\n11.0\np = 0.45\n\n\n2\n0\n3\n-1\n-1\n5.0\np = 0.45\n\n\n3\n0\n4\n0\n1\n7.0\np = 0.45\n\n\n\n\n\n\n\nbiased_end_df:\n\n\n\n\n\n\n\n\n\nmatch_num\nt\nmoney_at_round_end\nresult\nparams\n\n\n\n\n49\n0\n50\n-10\nLost Money\np = 0.45\n\n\n49\n1\n50\n-22\nLost Money\np = 0.45\n\n\n49\n2\n50\n-8\nLost Money\np = 0.45\n\n\n49\n3\n50\n-12\nLost Money\np = 0.45\n\n\n49\n4\n50\n-10\nLost Money\np = 0.45\n\n\n\n\n\n\n\n\nplot_trajectories(\n    biased_final_df, biased_end_df,\n    suptitle=\"Trajectories and Winnings for 1000 Matches, p = 0.45\"\n);\n\n\n\n\n\n\n\n\nAnd then, if we plot just the mean amount of money at the end of each round, averaged across simulations, we can compare the two trajectories on the same plot:\n\ncombined_final_df = pd.concat([fair_final_df, biased_final_df])\ncombined_end_df = pd.concat([fair_end_df, biased_end_df])\n\nbiased_grid = sns.JointGrid(height=5, ylim=(-30,10));\nbiased_grid.refline(y=0, color='black', lw=1, ls='solid', label='Breaking Even');\n# Mean lines\nsns.lineplot(\n    x=\"t\", y=\"money_at_round_end\", hue=\"params\",\n    data=combined_final_df, ax=biased_grid.ax_joint,\n    estimator='median',\n    #errorbar=None,\n    errorbar=('pi',50),\n    lw=1,\n    err_kws=dict(alpha=0.15),\n);\nbiased_grid.ax_marg_x.remove();\nbiased_grid.refline(\n    y=fair_end_df['money_at_round_end'].mean(),\n    color=cb_palette[0], lw=2,\n    label='Expected Payoff (p = 0.5)'\n);\nbiased_grid.refline(\n    y=biased_end_df['money_at_round_end'].mean(),\n    color=cb_palette[1], lw=2,\n    label='Expected Payoff (p = 0.45)'\n);\nbiased_grid.ax_joint.legend(loc=\"lower left\");\n# plt.legend(['Breaking Even','Your Expected Payoff'], loc=\"left\")\n\nsns.kdeplot(\n    y=\"money_at_round_end\", hue=\"params\", data=combined_end_df,\n    ax=biased_grid.ax_marg_y, fill=True, legend=False,\n    # stat='density',\n    # discrete=True,\n    # shrink=0.5,\n);\nbiased_grid.ax_joint.set_ylabel(\"Money at Round End\");\nbiased_grid.fig.set_figwidth(10)\n\n\n\n\n\n\n\n\nAnd from this plot we can see how, the median trajectories begin to noticeably depart from about \\(t = 5\\) onwards, and then even the intervals containing the middle 50% of end-of-round money amounts begin to separate from about \\(t = 26\\) onwards."
  },
  {
    "objectID": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-3-moving-to-pymc-modeling-the-season-2-dynamic",
    "href": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-3-moving-to-pymc-modeling-the-season-2-dynamic",
    "title": "[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure",
    "section": "[Part 3] Moving to PyMC: Modeling the Season 2 Dynamic",
    "text": "[Part 3] Moving to PyMC: Modeling the Season 2 Dynamic\nSo far, it seems like playing your friend is a capital-B capital-D Bad Deal for you! Which raises the question‚Ä¶\n\n[Part 3.1] Why Would You Play At All?\nThe short answer is that‚Ä¶ this is not so far removed from the structure of e.g.¬†a lottery, or just gambling in general: you get the thrill of the small chance of winning big, and meanwhile, if \\(B\\) was for example a US state, they would get the revenue from their state lottery.\nIn Elden Coin specifically, though, the idea is that you can similarly win big if you beat someone who has a Sheisty Coin, because of a game mechanic that the developers released alongside the new powerup shop:\n\nIf you beat someone without the coin, you get the ‚Äústandard‚Äù, expected money amount given by the rules of the game. However,\nIf you beat someone with the Sheisty Coin, your winnings double!\n\nWith this final addition to the game rules, it‚Äôs getting a bit complicated to think through everything in our heads ‚Äì and that‚Äôs exactly the reason why PyMC is so useful! It‚Äôs a tool for making ‚Äúoptimal‚Äù inferences in complex settings with tons of different sources of uncertainty occurring at different levels of analysis![1]\n\n\n\nHere, and throughout the class, the ‚Äúoptimality‚Äù of Bayesian inference is rooted pretty straightforwardly in De Finetti‚Äôs Theorem and its corollaries: If a person \\(i\\) decides to update probabilities in the face of evidence using any criteria besides Bayes‚Äô Rule, they are provably susceptible to what‚Äôs called a Dutch Book scheme, meaning that someone who is using Bayes‚Äô Rule can construct a betting scheme that seems good to \\(i\\) (meaning, when \\(i\\) calculates their expected outcomes using their non-Bayesian probability-updating scheme), but will actually result in \\(i\\) losing money in expectation.\n\n\n\n\n[Part 3.2] Modeling \\(\\Pr(\\textsf{Heads} \\mid \\textsf{Sheisty})\\)\nAt this point, though we could keep making our sim_outcome() function more and more complicated to adapt to these new dynamics‚Ä¶ we have PyMC right there, a library which was created for carrying out optimal (Bayesian) inferences in probabilistic systems!\nAnd specifically, since the whole point of this writeup is to introduce a standard Bayesian Workflow‚Ä¶ this is where that starts! My hope is that you‚Äôll see the benefit of having a full-on inference machine that can quickly produce the four distributions discussed in Week 6:\n\nThe Prior Distribution, which in this case incorporates:\n\n\nThe probability \\(s \\in [0, 1]\\) of \\(B\\) having a Sheisty Coin and\nThe corresponding coin bias \\(p \\in [0.4, 0.5]\\)\n\n\nThe Prior Predictive Distribution, which gives us:\n\n\nA distribution over num_heads \\(\\in \\{0, 1, \\ldots, 13\\}\\), as well as distributions over two quantities that can be derived from this:\nA distribution over the binary value win \\(\\in \\{0, 1\\}\\), and\nA distribution over the money_change value \\(\\in \\{-1, 1\\}\\).\n\n(Note how, since we‚Äôre building num_rounds into our model, PyMC will automatically generate 50 separate round samples for each draw from this prior predictive distribution, so that we can quickly just sum up these 50 values to obtain a distribution over mach outcomes as well)\n\nThe Posterior Distribution, which gives us new distributions over the parameters \\(s\\) and \\(p\\), representing the optimal updates of the Prior Distribution after observing a set of data. And lastly,\nThe Posterior Predictive Distribution, which we can use in the same way we used the Prior Predictive distribution, but in this case it will generate distributions over num_heads, win, and money_change in light of the updated values of \\(s\\) and \\(p\\).\n\nSo, let‚Äôs write out a model in PyMC that takes into account all of the possible sources of uncertainty that we want to model in this case:\n\nclass S2Game:\n    flips_per_round = 13\n    num_heads_range = (0, flips_per_round)\n    num_heads_support = list(range(0, flips_per_round + 1))\n    win_cutoff = 6\n    rounds_per_match = 50\n    bonus_multiplier = 5\n\n\ncoords = {\n    't': list(range(1, Game.rounds_per_match + 1))\n}\nwith pm.Model(coords=coords) as s2_model:\n    p_sheisty = pm.Uniform(\"p_sheisty\", 0, 1)\n    is_sheisty = pm.Bernoulli(\"is_sheisty\", p=p_sheisty)\n    fair_p_heads = 0.5\n    sheisty_p_heads = pm.Uniform(\"sheisty_p_heads\", 0.4, 0.5)\n    p_heads = pm.Deterministic(\"p_heads\", pm.math.switch(is_sheisty, sheisty_p_heads, fair_p_heads))\n    # The flips\n    num_heads = pm.Binomial(\"num_heads\", n=Game.flips_per_round, p=p_heads, dims='t')\n    # The round result\n    money_change = pm.Deterministic(\n        \"money_change\",\n        pm.math.switch(pm.math.gt(num_heads, Game.win_cutoff), 1, -1),\n        dims='t'\n    )\n    # The cumulative result\n    money_at_round_end = pm.Deterministic(\n        \"money_at_round_end\",\n        pm.math.cumsum(money_change, 0),\n        dims='t'\n    )\n    # And the doubling-at-end if we win\n    money_w_bonus = pm.Deterministic(\n        \"money_w_bonus\",\n        pm.math.switch(\n            pm.math.gt(money_at_round_end, 0),\n            S2Game.bonus_multiplier * money_at_round_end,\n            money_at_round_end\n        ),\n        dims='t'\n    )\npm.model_to_graphviz(s2_model)\n\n\n\n\n\n\n\n\n\n\n[Part 3.3] The Prior Distribution\n\nwith s2_model:\n    s2_prior_idata = pm.sample_prior_predictive(draws=MyGlobals.matches_to_sim, random_seed=5650)\n\nSampling: [is_sheisty, num_heads, p_sheisty, sheisty_p_heads]\n\n\n\ns2_prior_df = s2_prior_idata.prior.to_dataframe().reset_index().drop(columns='chain')\ns2_prior_df.head()\n\n\n\n\n\n\n\n\ndraw\nt\nnum_heads\nis_sheisty\np_heads\nmoney_w_bonus\nmoney_change\np_sheisty\nsheisty_p_heads\nmoney_at_round_end\n\n\n\n\n0\n0\n1\n6\n1\n0.432234\n-1\n-1\n0.888773\n0.432234\n-1\n\n\n1\n0\n2\n6\n1\n0.432234\n-2\n-1\n0.888773\n0.432234\n-2\n\n\n2\n0\n3\n7\n1\n0.432234\n-1\n1\n0.888773\n0.432234\n-1\n\n\n3\n0\n4\n7\n1\n0.432234\n0\n1\n0.888773\n0.432234\n0\n\n\n4\n0\n5\n6\n1\n0.432234\n-1\n-1\n0.888773\n0.432234\n-1\n\n\n\n\n\n\n\n\ns2_prior_end_df = s2_prior_df[s2_prior_df['t'] == Game.rounds_per_match].copy()\ns2_prior_end_df.head()\n\n\n\n\n\n\n\n\ndraw\nt\nnum_heads\nis_sheisty\np_heads\nmoney_w_bonus\nmoney_change\np_sheisty\nsheisty_p_heads\nmoney_at_round_end\n\n\n\n\n49\n0\n50\n5\n1\n0.432234\n-24\n-1\n0.888773\n0.432234\n-24\n\n\n99\n1\n50\n8\n0\n0.500000\n10\n1\n0.027332\n0.433957\n2\n\n\n149\n2\n50\n5\n0\n0.500000\n-10\n-1\n0.081197\n0.407078\n-10\n\n\n199\n3\n50\n5\n0\n0.500000\n-6\n-1\n0.864813\n0.400025\n-6\n\n\n249\n4\n50\n7\n1\n0.412902\n-22\n1\n0.871763\n0.412902\n-22\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"p_sheisty\", data=s2_prior_end_df, ax=ax\n);\nax.set_title(\"Prior Distribution of p_sheisty\")\nax.savefig()\n\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"p_heads\", data=s2_prior_end_df, ax=ax\n);\nax.set_title(\"Prior Distribution of p_heads\")\nax.savefig()\n\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"is_sheisty\", data=s2_prior_end_df, ax=ax\n);\nax.set_title(\"Prior Distribution of is_sheisty\");\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n[Part 3.3] The Prior Predictive Distribution\nNow, the vocabulary in this part is going to be a bit confusing at first:\n\nWith the way we have defined the model parameters ‚Äì p_sheisty and p_heads as parameters and then num_heads (as a vector) and money final (as a scalar) as outcomes ‚Äì we do have a prior predictive distribution her, since we can obtain distributions over the outcomes before actually observing any data. However‚Ä¶\nWith the way PyMC defines paramters, we don‚Äôt ‚Äúunlock‚Äù the prior-predictive or posterior-predictive distributions until we actually provide observed data to the model.\n\nSo, what that means is, don‚Äôt be fooled by the PyMC terminology (which is for computational convenience ‚Äì see e.g.¬†this post on their forum about it)! We‚Äôre going to use the .prior attribute of our inference data, but we‚Äôre using it to carry out a prior-predictive check in this part![1]\n\n\n\nI‚Äôm going to release a shorter writeup soon where we‚Äôll supply the data to our pm.Model() right away, so that we‚Äôll immediately have access to .prior, .prior_predictive, .posterior, and .posterior_predictive!\n\n\nFirst things first, we could infer just from a table of the means and medians of the two models that the bonus-money has had an interesting effect on our expected earnings:\n\ns2_prior_money_df = s2_prior_end_df[['draw','money_at_round_end','money_w_bonus']].copy().melt(id_vars=\"draw\")\ns2_prior_money_df.groupby('variable')['value'].agg(['mean','median'])\n\n\n\n\n\n\n\n\nmean\nmedian\n\n\nvariable\n\n\n\n\n\n\nmoney_at_round_end\n-6.536\n-6.0\n\n\nmoney_w_bonus\n0.792\n-6.0\n\n\n\n\n\n\n\nEven though the medians are exactly the same, the means of the two variables now differ in an important way, with one above 0 and the other well below 0. To get a visual for why this happens, let‚Äôs plot two posterior-predictive distributions here, on the same axes, to see how the doubling-of-positive-winnings dynamic affects the distribution of outcomes (as well as the ultimate expected value!)\n\nprior_mean_money_nb = s2_prior_end_df['money_at_round_end'].mean()\nprior_mean_money = s2_prior_end_df['money_w_bonus'].mean()\nax = pw.Brick(figsize=(6, 3.5));\nsns.kdeplot(\n    x=\"value\", hue=\"variable\", data=s2_prior_money_df, ax=ax,\n    fill=True\n);\nax.axvline(x=0, color='black', ls='dashed', lw=1, alpha=0.9);\nax.axvline(x=prior_mean_money_nb, color=cb_palette[1], ls='dashed', lw=1, alpha=0.9);\nax.axvline(x=prior_mean_money, color=cb_palette[0], ls='dashed', lw=1, alpha=0.9);\nax.set_title(\"Distribution of Net Earnings Before and After Bonus\");\nax.savefig()\n\n\n\n\n\n\n\n\nSo, that‚Äôs the prior-predictive distribution with respect to the final end-of-match payout, where we can already see (from the dashed vertical lines) that the bonus makes playing the game ever-so-slightly worth it, with an expected value now slightly above ‚Ç¨0. This plot also explains why only the mean was affected: the bonus only affected the final money amounts by scaling up outcomes which were already above the 50% mark, leaving the median (the observation with exactly 50% of observations above it) unchanged.\nAnd, since our model has two levels (round-level and match-level), we can also use our prior-predictive distribution to visualize the distribution of trajectories:\n\ndef plot_predictive_trajectories(df, suptitle, custom_bw_adjust=2):\n    # Add a special t=0 row to each match, so the plotted trajectories start at\n    # (t,y) = (0,0)\n    t0_df = pd.DataFrame({'t': 0, 'money_w_bonus': 0, 'draw': range(df['draw'].max())})\n    plot_df = pd.concat([t0_df, df])\n    end_df = df[df['t'] == S2Game.rounds_per_match].copy()\n    mean_money = end_df['money_w_bonus'].mean()\n    med_money = end_df['money_w_bonus'].median()\n    \n    grid = sns.JointGrid(height=5.25) #, ylim=(-30,15));\n    grid.ax_marg_x.remove();\n    # Mean lines\n    full_range_plot = sns.lineplot(\n        x=\"t\", y=\"money_w_bonus\",\n        data=df, ax=grid.ax_joint,\n        color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',100),\n        lw=1,\n        err_kws=dict(alpha=0.125),\n    );\n    sns.lineplot(\n        x=\"t\", y=\"money_w_bonus\",\n        data=df, ax=grid.ax_joint,\n        color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',75),\n        lw=1,\n        err_kws=dict(alpha=0.25),\n    );\n    sns.lineplot(\n        x=\"t\", y=\"money_w_bonus\",\n        data=df, ax=grid.ax_joint,\n        color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',50),\n        lw=1,\n        err_kws=dict(alpha=0.5),\n    );\n    # Median trajectory outline in black\n    sns.lineplot(\n        x=\"t\", y=\"money_w_bonus\", label=\"Median of Simulated Rounds\",\n        data=df, ax=grid.ax_joint,\n        color='black', alpha=0.9,\n        estimator='median',\n        errorbar=None,\n        lw=1, ls='dashed',\n    );\n    # Mean trajectory\n    sns.lineplot(\n        x=\"t\", y=\"money_w_bonus\", label=\"Mean of Simulated Rounds\",\n        data=df, ax=grid.ax_joint,\n        color='black', alpha=0.9,\n        estimator='mean',\n        errorbar=None,\n        lw=1, ls='dotted',\n    );\n    grid.refline(\n        y=0, color='black', lw=1, ls='solid', label='Breaking Even', alpha=0.9\n    );\n    # prior_grid.refline(\n    #     y=biased_end_df['money_at_round_end'].mean(),\n    #     color=cb_palette[1], lw=2,\n    #     label='Expected Payoff'\n    # );\n    grid.ax_joint.legend(loc=\"lower left\");\n    # plt.legend(['Breaking Even','Your Expected Payoff'], loc=\"left\")\n    \n    sns.kdeplot(\n        y=\"money_w_bonus\", data=df,\n        ax=grid.ax_marg_y, fill=True, legend=False,\n        bw_adjust=custom_bw_adjust,\n        # stat='density',\n        # discrete=True,\n        # shrink=0.5,\n    );\n    # Line on margin for mean\n    grid.ax_marg_y.axhline(\n        y=mean_money, color='black', ls='dotted', alpha=0.9\n    );\n    grid.ax_marg_y.axhline(\n        y=med_money, color='black', ls='dashed', alpha=0.9\n    );\n    grid.ax_joint.set_ylabel(\"Money at Round End\");\n    grid.ax_joint.set_title(\"Round Level\");\n    grid.ax_marg_y.set_title(\"Match Level\");\n    grid.fig.suptitle(suptitle);\n    grid.fig.set_figwidth(9.5);\n    return grid\nplot_predictive_trajectories(\n    s2_prior_df,\n    suptitle=\"Prior Predictive Distributions\"\n);\n\n\n\n\n\n\n\n\nSo now we can see, from this and the previous plot, that given our uncertainty about p_sheisty and p_heads, it is just barely worthwhile to play! Even though the median of the game with and without the beating-sheisty bonus is the same, the mean with the bonus is just above ‚Ç¨0.\nIn other words, given our state of knowledge about the game at this point, it is worthwhile to play (at least, on the assumption that we play if \\(\\mathbb{E}[Y] &gt; 0\\).\nHowever, each time you play a match with \\(B\\), you will obtain some information about (a) whether or not they have the Sheisty Coin, and then (b) if they do have it, what their bias level may be. So, in the next part we‚Äôll simulate playing one round and then updating based on the results."
  },
  {
    "objectID": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-4-updating-on-observed-data",
    "href": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-4-updating-on-observed-data",
    "title": "[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure",
    "section": "[Part 4] Updating On Observed Data",
    "text": "[Part 4] Updating On Observed Data\nNow, let‚Äôs say you play one match with your friend, and you‚Äôre trying to determine whether it‚Äôs worthwhile to keep playing with them‚Ä¶ How exactly would you figure this out? By what specific amounts should you update your distribution over p_sheisty and p_heads?\nYou could sit down and solve a gigantic 5100-style problem‚Ä¶ or just plug the data into your Bayesian inference machine, namely, PyMC üòä\n\n[Part 4.1] Your (Single) Observed Match\nHere we‚Äôll finally reveal the secret: though you, as player \\(A\\), don‚Äôt know it (this is the true DGP we‚Äôre simulating, finally, which is not observable to the actual agents playing the game!), player \\(B\\) does indeed have the Sheisty Coin, and the random bias they‚Äôve obtained puts their p_heads at 0.41.\nSo, let‚Äôs simulate one match with this coin, keeping in mind that you-as-player-\\(A\\) don‚Äôt know that this data was generated via \\(p = 0.48\\) ‚Äì you only know that the match ended up with the following trajectory and outcome:\n\ntrue_is_sheisty = 1\ntrue_p_heads = 0.48\n\n\ndef sim_s2_match(p, rng_seed=5650):\n    sim_rng = np.random.default_rng(seed=rng_seed)\n    round_outcomes = sim_rng.binomial(n=Game.flips_per_round, p=p, size=Game.rounds_per_match)\n    match_df = pd.DataFrame({'t': range(1,Game.rounds_per_match+1), 'num_heads': round_outcomes})\n    match_df['money_change'] = -1 + 2 * (match_df['num_heads'] &gt; Game.win_cutoff)\n    match_df['money_at_round_end'] = match_df['money_change'].cumsum()\n    # New in Season 2: Quintupling your money when it's above 0!\n    match_df['money_w_bonus'] = match_df['money_at_round_end'].apply(lambda x: S2Game.bonus_multiplier * x if x &gt; 0 else x) \n    t0_df = pd.DataFrame({'t':[0],'money_at_round_end':[0],'money_change':[0]})\n    match_df = pd.concat([t0_df, match_df])\n    return(match_df)\nobs_df = sim_s2_match(true_p_heads)\nobs_df.tail()\n\n\n\n\n\n\n\n\nt\nmoney_at_round_end\nmoney_change\nnum_heads\nmoney_w_bonus\n\n\n\n\n45\n46\n-14\n1\n8.0\n-14.0\n\n\n46\n47\n-15\n-1\n4.0\n-15.0\n\n\n47\n48\n-14\n1\n8.0\n-14.0\n\n\n48\n49\n-13\n1\n7.0\n-13.0\n\n\n49\n50\n-14\n-1\n5.0\n-14.0\n\n\n\n\n\n\n\n\nobs_grid = sns.JointGrid(height=5) # , ylim=(-30,15));\nobs_grid.refline(\n    y=0, color='black', alpha=0.9, lw=1, ls='solid', label='Breaking Even'\n);\n# Mean lines\nsns.lineplot(\n    x=\"t\", y=\"money_at_round_end\", label=\"Observed Trajectory\",\n    data=obs_df, ax=obs_grid.ax_joint,\n    estimator='median',\n    #errorbar=None,\n    errorbar=('pi',50),\n    lw=1,\n    err_kws=dict(alpha=0.15),\n);\nobs_grid.ax_marg_x.remove();\nobs_grid.refline(\n    y=obs_df.iloc[-1]['money_at_round_end'],\n    color=cb_palette[0], lw=1,\n    label='Obseved Payoff'\n);\nobs_grid.ax_joint.legend(loc=\"lower left\");\n# plt.legend(['Breaking Even','Your Expected Payoff'], loc=\"left\")\n\n# sns.kdeplot(\n#     y=\"money_at_round_end\", hue=\"params\", data=combined_end_df,\n#     ax=biased_grid.ax_marg_y, fill=True, legend=False,\n#     # stat='density',\n#     # discrete=True,\n#     # shrink=0.5,\n# );\nobs_grid.ax_joint.set_ylabel(\"Money at Round End\");\nobs_grid.fig.set_figwidth(9)\n\n\n\n\n\n\n\n\nSo, by the end of this match, you ended up with ‚Ç¨14 less than you started with. If you only knew this final amount, you would essentially have one ‚Äúbit‚Äù of information. However, you have more than this! You also know the trajectory from \\(t = 0\\) to \\(t = 50\\), which gives you much more information you can use to update your model parameters.\nLet‚Äôs use PyMC as it was fully intended, by pairing our model with this observed data, and then see how this changes whether or not it‚Äôs still worth it to play against \\(B\\)!\n\n\n[Part 4.2] Adding the Observations to Our Model and Updating\nSince our model doesn‚Äôt actually incorporate the \\(t = 0\\) row (we added that just to make the plots a bit more readable), we drop these rows and then set obs_df without these rows as the observed data for our model.\n\nobs_t1_df = obs_df[obs_df['t'] &gt; 0].copy().set_index('t')\n\n\ncoords = {\n    't': list(range(1, Game.rounds_per_match + 1))\n}\nwith pm.Model(coords=coords) as s2_model_obs:\n    # Unobservable prior parameters\n    p_sheisty = pm.Uniform(\"p_sheisty\", 0, 1)\n    is_sheisty = pm.Bernoulli(\"is_sheisty\", p=p_sheisty)\n    fair_p_heads = 0.5\n    sheisty_p_heads = pm.Uniform(\"sheisty_p_heads\", 0.4, 0.5)\n    p_heads = pm.Deterministic(\n        \"p_heads\",\n        pm.math.switch(\n            is_sheisty,\n            sheisty_p_heads,\n            fair_p_heads\n        )\n    )\n    # Observable parameters\n    # The flips\n    num_heads = pm.Binomial(\n        \"num_heads\", n=Game.flips_per_round, p=p_heads,\n        dims='t', observed=obs_t1_df['num_heads']\n    )\n    # The round result\n    money_change = pm.Deterministic(\n        \"money_change\",\n        pm.math.switch(pm.math.gt(num_heads, Game.win_cutoff), 1, -1),\n        dims='t'\n    )\n    # The cumulative result\n    money_at_round_end = pm.Deterministic(\n        \"money_at_round_end\",\n        pm.math.cumsum(money_change, 0),\n        dims='t'\n    )\n    # And the doubling-at-end if we win\n    money_w_bonus = pm.Deterministic(\n        \"money_w_bonus\",\n        pm.math.switch(\n            pm.math.gt(money_at_round_end, 0),\n            S2Game.bonus_multiplier * money_at_round_end,\n            money_at_round_end\n        ),\n        dims='t'\n    )\npm.model_to_graphviz(s2_model_obs)\n\n\n\n\n\n\n\n\n\n\n[Part 4.3] Sample from Posterior Distribution\n\nwith s2_model_obs:\n    s2_post_idata = pm.sample(random_seed=5650)\n\nMultiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;NUTS: [p_sheisty, sheisty_p_heads]\n&gt;BinaryGibbsMetropolis: [is_sheisty]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\n\n\n\ns2_post_df = s2_post_idata.posterior.to_dataframe().reset_index()\ns2_post_df\n\n\n\n\n\n\n\n\nchain\ndraw\nt\nis_sheisty\np_sheisty\nsheisty_p_heads\np_heads\nmoney_change\nmoney_at_round_end\nmoney_w_bonus\n\n\n\n\n0\n0\n0\n1\n1\n0.634665\n0.425275\n0.425275\n-1\n-1\n-1\n\n\n1\n0\n0\n2\n1\n0.634665\n0.425275\n0.425275\n1\n0\n0\n\n\n2\n0\n0\n3\n1\n0.634665\n0.425275\n0.425275\n-1\n-1\n-1\n\n\n3\n0\n0\n4\n1\n0.634665\n0.425275\n0.425275\n-1\n-2\n-2\n\n\n4\n0\n0\n5\n1\n0.634665\n0.425275\n0.425275\n1\n-1\n-1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n199995\n3\n999\n46\n1\n0.902354\n0.431539\n0.431539\n1\n-14\n-14\n\n\n199996\n3\n999\n47\n1\n0.902354\n0.431539\n0.431539\n-1\n-15\n-15\n\n\n199997\n3\n999\n48\n1\n0.902354\n0.431539\n0.431539\n1\n-14\n-14\n\n\n199998\n3\n999\n49\n1\n0.902354\n0.431539\n0.431539\n1\n-13\n-13\n\n\n199999\n3\n999\n50\n1\n0.902354\n0.431539\n0.431539\n-1\n-14\n-14\n\n\n\n\n200000 rows √ó 10 columns\n\n\n\n\ns2_post_end_df = s2_post_df[s2_post_df['t'] == S2Game.rounds_per_match].copy()\ns2_post_end_df\n\n\n\n\n\n\n\n\nchain\ndraw\nt\nis_sheisty\np_sheisty\nsheisty_p_heads\np_heads\nmoney_change\nmoney_at_round_end\nmoney_w_bonus\n\n\n\n\n49\n0\n0\n50\n1\n0.634665\n0.425275\n0.425275\n-1\n-14\n-14\n\n\n99\n0\n1\n50\n1\n0.941787\n0.466247\n0.466247\n-1\n-14\n-14\n\n\n149\n0\n2\n50\n1\n0.201438\n0.432916\n0.432916\n-1\n-14\n-14\n\n\n199\n0\n3\n50\n1\n0.910678\n0.445946\n0.445946\n-1\n-14\n-14\n\n\n249\n0\n4\n50\n1\n0.500935\n0.446615\n0.446615\n-1\n-14\n-14\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n199799\n3\n995\n50\n1\n0.204122\n0.475606\n0.475606\n-1\n-14\n-14\n\n\n199849\n3\n996\n50\n1\n0.915687\n0.422559\n0.422559\n-1\n-14\n-14\n\n\n199899\n3\n997\n50\n1\n0.321373\n0.466857\n0.466857\n-1\n-14\n-14\n\n\n199949\n3\n998\n50\n1\n0.486654\n0.461348\n0.461348\n-1\n-14\n-14\n\n\n199999\n3\n999\n50\n1\n0.902354\n0.431539\n0.431539\n-1\n-14\n-14\n\n\n\n\n4000 rows √ó 10 columns\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"p_sheisty\", data=s2_post_end_df, ax=ax\n);\nax.set_title(\"Posterior Distribution of p_sheisty\");\nax.savefig()\n\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"is_sheisty\", data=s2_post_end_df, ax=ax\n);\nax.set_title(\"Posterior Distribution of is_sheisty\");\nax.savefig()\n\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"p_heads\", data=s2_post_end_df, ax=ax\n);\nax.set_title(\"Posterior Distribution of p_heads\");\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n[Part 4.4] Sample from Posterior Predictive Distribution\n\nwith s2_model_obs:\n    s2_post_pred_idata = pm.sample_posterior_predictive(s2_post_idata, random_seed=5650)\n\nSampling: [num_heads]\n\n\n\n\n\n\n\n\n\ns2_post_pred_df = s2_post_pred_idata.posterior_predictive.to_dataframe().reset_index()\ns2_post_pred_df['source'] = \"Posterior Predictive\"\ns2_post_pred_df\n\n\n\n\n\n\n\n\nchain\ndraw\nt\nnum_heads\nmoney_change\nmoney_at_round_end\nmoney_w_bonus\nsource\n\n\n\n\n0\n0\n0\n1\n5\n-1\n-1\n-1\nPosterior Predictive\n\n\n1\n0\n0\n2\n5\n-1\n-2\n-2\nPosterior Predictive\n\n\n2\n0\n0\n3\n3\n-1\n-3\n-3\nPosterior Predictive\n\n\n3\n0\n0\n4\n0\n-1\n-4\n-4\nPosterior Predictive\n\n\n4\n0\n0\n5\n4\n-1\n-5\n-5\nPosterior Predictive\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n199995\n3\n999\n46\n7\n1\n-16\n-16\nPosterior Predictive\n\n\n199996\n3\n999\n47\n5\n-1\n-17\n-17\nPosterior Predictive\n\n\n199997\n3\n999\n48\n5\n-1\n-18\n-18\nPosterior Predictive\n\n\n199998\n3\n999\n49\n4\n-1\n-19\n-19\nPosterior Predictive\n\n\n199999\n3\n999\n50\n7\n1\n-18\n-18\nPosterior Predictive\n\n\n\n\n200000 rows √ó 8 columns\n\n\n\n\nplot_predictive_trajectories(\n    s2_post_pred_df,\n    suptitle=\"Posterior Predictive Distributions\"\n);\n\n\n\n\n\n\n\n\nSo‚Ä¶ by incorporating what we learned from the one round about the possibility that \\(B\\) has the Sheisty Coin, we‚Äôve learn that it is no longer worthwhile to keep playing against \\(B\\)!\nWe can see, up in the portion above the ‚ÄúBreaking Even‚Äù line, the tantalizing 1% of matches that end in huge fortunes for us. Taking this risk-reward tradeoff into account would take us into the even more‚Ä¶ complex-but-important realm of Bayesian Decision Theory!\nAs a motivation to take the additional deep-dive into this field, imo it is especially, incredibly important for those interested in public policy. The Minnesota Radon data mentioned in class, for example, was part of a much broader case-study of how Bayesian data analysis can be used to provide policy recommendations, and following the Bayesian Workflow beyond the contents of this notebook and into Bayesian Decision Theory will allow you to construct plots like this, providing explicit Bayes-Theorem-optimized recommendations to policymakers:\n\n\n\n\nThe Bayesian Decision Analysis plot from Gelman et al.¬†(2014), Bayesian Data Analysis (Third Edition)"
  },
  {
    "objectID": "w02/slides.html#courtney-green",
    "href": "w02/slides.html#courtney-green",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Courtney Green",
    "text": "Courtney Green\nBackground: BA in Public Policy & Leadership, now interested in using data to examine structural disparities.\nInterests & what I can help with: How sociodemographic factors (e.g., race, gender, income, immigration status, education level) shape policy outcomes and institutional practices in areas like criminal justice, housing, healthcare, education, and environment.\n\nAsk me about: Counterfactual balancing, handling messy or privacy-limited datasets, and framing causal questions around inequality.\nReach out if you‚Äôre thinking about a final project that touches social systems, systemic inequity, or fairness!"
  },
  {
    "objectID": "w02/slides.html#what-got-me-interested-in-causal-inference",
    "href": "w02/slides.html#what-got-me-interested-in-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Got Me Interested in Causal Inference",
    "text": "What Got Me Interested in Causal Inference\n5000 Project: Over-Policing and Wrongful Convictions in Illinois\n\nMotivation: Explore what demographic factors predict wrongful convictions using real exoneration data.\nChallenge: No data on ‚Äúnon-exonerated innocents,‚Äù so I (with prof‚Äôs help) created a counterfactual dataset by sampling from the incarcerated population\nMethod: Trained models (logistic regression, random forest, KNN) on a balanced dataset of 548 exonerated and 548 non-exonerated individuals\nFinding: Race and geography (esp.¬†Cook Count/Chicago) were the strongest predictors of exoneration\nTakeaway: Questions about systemic bias can‚Äôt be answered with just descriptive stats, you need counterfactual reasoning and causal inference\n\n(Optional) View the counterfactual setup"
  },
  {
    "objectID": "w02/slides.html#wendy-hu",
    "href": "w02/slides.html#wendy-hu",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Wendy Hu",
    "text": "Wendy Hu\n\nBackground: BS in Social Work, passionate about advancing social equity through computational tools\nInterest: I bring a background in social welfare, public health, and NGO data, with experience analyzing behavioral and institutional outcomes. My work spans Indigenous health, gender equity, and trauma-informed service design‚Äîalways with a focus on equity, system change, and policy relevance.\nAsk me about: Framing social justice questions in data terms, working with community or clinical datasets and interpreting model results in context.\nIf you‚Äôre designing a project around health, inequality, nonprofit impact, or any socially embedded issue‚ÄîI‚Äôd love to help bridge rigor and relevance."
  },
  {
    "objectID": "w02/slides.html#jeffs-hw1-updateapology",
    "href": "w02/slides.html#jeffs-hw1-updateapology",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Jeff‚Äôs HW1 Update/Apology",
    "text": "Jeff‚Äôs HW1 Update/Apology\n\nBasically‚Ä¶ I goofed by trying to shove a homework into the first two weeks of class üò≠\nLast week was ‚Äúsetting the table‚Äù, so there are a few (multiple-choice) questions on that\nBut, really only makes sense to have HW starting from end of today, once we‚Äôve introduced PGMs, the language we need to actually do Causal Computational Social Science!\n\n\n Due date pushed to Friday, June 6, 5:59pm"
  },
  {
    "objectID": "w02/slides.html#hw1-detail-plz-dont-hate-me",
    "href": "w02/slides.html#hw1-detail-plz-dont-hate-me",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)",
    "text": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)\n(The main reason this is taking me so long)\n\nTwo key libraries for ‚Äúmain‚Äù course content (HW2 onwards):\n\nIn R: rethinking, ‚Äúlite‚Äù version of Stan\nIn Python: PyMC\n\nBoth are overkill for current unit: lots of work required to ‚Äúturn off‚Äù fancy features and implement basic PGMs\n\\(\\leadsto\\) Only way I know to achieve two goals:\n\n Learning simple PGMs first, as tools for thinking, before adding in additional full-on coding baggage of Stan/PyMC\n Having an entire textbook on these base PGMs that you can use as reference\n\nIs to use [small, restricted] subset of JavaScript, called WebPPL, because‚Ä¶\n\n\nhttp://probmods.org/"
  },
  {
    "objectID": "w02/slides.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "href": "w02/slides.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality",
    "text": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality\n\n\n You‚Äôll no longer be able to read ‚Äúscientific‚Äù writing without striking this expression (involuntarily):\n\n ‚ÄúScientific‚Äù talks will begin to sound like the following:"
  },
  {
    "objectID": "w02/slides.html#blasting-off-into-causality",
    "href": "w02/slides.html#blasting-off-into-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Blasting Off Into Causality!",
    "text": "Blasting Off Into Causality!"
  },
  {
    "objectID": "w02/slides.html#data-generating-processes-dgps",
    "href": "w02/slides.html#data-generating-processes-dgps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Data-Generating Processes (DGPs)",
    "text": "Data-Generating Processes (DGPs)\n\n\n\n\n\n\n\nYou saw this in DSAN 5100!\n¬´\\(X_1, \\ldots, X_n\\) drawn i.i.d. Normal, mean \\(\\mu\\) variance \\(\\sigma^2\\)¬ª characterizes DGP of \\((X_1, \\ldots, X_n)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n5650: Dive into DGPs, rather than treating as black box/footnote to Law of Large Numbers, so we can move [asymptotically!]‚Ä¶\nFrom associational statements:\n\n¬´\\(\\underbrace{\\text{An increase}}_{\\small\\text{noun}}\\) in \\(X\\) by 1 is associated with increase in \\(Y\\) by \\(\\beta\\)¬ª\n\nTo causal ones: ¬´\\(\\underbrace{\\text{Increasing}}_{\\small\\text{verb}}\\) \\(X\\) by 1 causes \\(Y\\) to increase by \\(\\beta\\)¬ª"
  },
  {
    "objectID": "w02/slides.html#dgps-and-the-emergence-of-order",
    "href": "w02/slides.html#dgps-and-the-emergence-of-order",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "DGPs and the Emergence of Order",
    "text": "DGPs and the Emergence of Order\n\n\n\n\n\n\n\nWho can guess the state of this process after 10 steps, with 1 person?\n10 people? 50? 100? (If they find themselves on the same spot, they stand on each other‚Äôs heads)\n100 steps? 1000?"
  },
  {
    "objectID": "w02/slides.html#the-result-16-steps",
    "href": "w02/slides.html#the-result-16-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 16 Steps",
    "text": "The Result: 16 Steps"
  },
  {
    "objectID": "w02/slides.html#the-result-64-steps",
    "href": "w02/slides.html#the-result-64-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 64 Steps",
    "text": "The Result: 64 Steps"
  },
  {
    "objectID": "w02/slides.html#mathematicalscientific-modeling",
    "href": "w02/slides.html#mathematicalscientific-modeling",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "‚ÄúMathematical/Scientific Modeling‚Äù",
    "text": "‚ÄúMathematical/Scientific Modeling‚Äù\n\nThing we observe (poking out of water): data\nHidden but possibly discoverable via deeper dive (ecosystem under surface): DGP"
  },
  {
    "objectID": "w02/slides.html#so-whats-the-problem",
    "href": "w02/slides.html#so-whats-the-problem",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often\n\nSee Appendix Slide"
  },
  {
    "objectID": "w02/slides.html#the-shallow-problem-of-causal-inference",
    "href": "w02/slides.html#the-shallow-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Shallow Problem of Causal Inference",
    "text": "The Shallow Problem of Causal Inference\n\n\n\n\n\n\n\n\ncor(ski_df$value, law_df$value)\n[1] 0.9921178\n\n(Data from Vigen, Spurious Correlations)\n\nThis, however, is only a mini-boss. Beyond it lies the truly invincible FINAL BOSS‚Ä¶ üôÄ"
  },
  {
    "objectID": "w02/slides.html#the-fundamental-problem-of-causal-inference",
    "href": "w02/slides.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠"
  },
  {
    "objectID": "w02/slides.html#what-is-to-be-done",
    "href": "w02/slides.html#what-is-to-be-done",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?"
  },
  {
    "objectID": "w02/slides.html#probability",
    "href": "w02/slides.html#probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring."
  },
  {
    "objectID": "w02/slides.html#beyond-conditional-probability",
    "href": "w02/slides.html#beyond-conditional-probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects"
  },
  {
    "objectID": "w02/slides.html#preview-do-calculus",
    "href": "w02/slides.html#preview-do-calculus",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events"
  },
  {
    "objectID": "w02/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w02/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!"
  },
  {
    "objectID": "w02/slides.html#ulysses-and-the-computational-sirens",
    "href": "w02/slides.html#ulysses-and-the-computational-sirens",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Ulysses and the [Computational] Sirens",
    "text": "Ulysses and the [Computational] Sirens"
  },
  {
    "objectID": "w02/slides.html#bayesian-inference-but-with-pictures",
    "href": "w02/slides.html#bayesian-inference-but-with-pictures",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Bayesian Inference but with Pictures",
    "text": "Bayesian Inference but with Pictures\nA Probabilistic Graphical Model (PGM) provides us with:\n\nA formal-mathematical‚Ä¶\nBut also easily visualizable (by construction)‚Ä¶\nRepresentation of a data-generating process (DGP)!\n\nExample: Let‚Äôs model how weather \\(W\\) affects evening plans \\(Y\\): the choice between going to a party or staying in to watch movies\n\n\n\n\n The Partier‚Äôs Dilemma\n\n\n\nA person \\(i\\) wakes up with some initial affinity for partying: \\(\\Pr(Y_i = \\textsf{Go})\\)\n\\(i\\) then goes to their window and observes the weather \\(W_i\\) outside:\n\nIf the weather is sunny, \\(i\\)‚Äôs affinity increases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Sun}) &gt; \\Pr(Y = \\textsf{Go})\\)\nOtherwise, if it is rainy, \\(i\\)‚Äôs affinity decreases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Rain}) &lt; \\Pr(Y = \\textsf{Go})\\)"
  },
  {
    "objectID": "w02/slides.html#two-main-building-blocks",
    "href": "w02/slides.html#two-main-building-blocks",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Two Main ‚ÄúBuilding Blocks‚Äù",
    "text": "Two Main ‚ÄúBuilding Blocks‚Äù\n\nNodes like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}\\) denote Random Variables\n\n\\[\n\\boxed{\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}} \\simeq \\boxed{ \\begin{array}{c|cc}x & \\textsf{Tails} & \\textsf{Heads} \\\\\\hline \\Pr(X = x) & 0.5 & 0.5\\end{array}}\n\\]\n\nEdges like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) denote relationships between RVs\n\nWhat an edge ‚Äúmeans‚Äù can get [ontologically] tricky!\nRetain sanity by just remembering: an edge \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) is included in our PGM if we ‚Äúcare about‚Äù modeling the conditional probability table (CPT) of \\(Y\\) w.r.t. \\(X\\)\n\n\n\\[\n\\require{enclose}\\boxed{ \\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em} } \\simeq \\boxed{\n  \\begin{array}{c|cc}\n  x & \\Pr(Y = \\textsf{Lose} \\mid X = x) & \\Pr(Y = \\textsf{Win} \\mid X = x) \\\\\\hline\n  \\textsf{Tails} & 0.8 & 0.2 \\\\\n  \\textsf{Heads} & 0.5 & 0.5\n  \\end{array}\n}\n\\]"
  },
  {
    "objectID": "w02/slides.html#pgm-for-the-partiers-dilemma",
    "href": "w02/slides.html#pgm-for-the-partiers-dilemma",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "PGM for the Partier‚Äôs Dilemma",
    "text": "PGM for the Partier‚Äôs Dilemma\n\nA node \\(\\pnode{W}\\) denoting RV \\(W\\), which can take on values in \\(\\mathcal{R}_W = \\{\\textsf{Sun}, \\textsf{Rain}\\}\\),\nA node \\(\\pnode{Y}\\) denoting RV \\(Y\\), which can take on values in \\(\\mathcal{R}_Y = \\{\\textsf{Go}, \\textsf{Stay}\\}\\), and\nAn edge \\(\\pedge{W}{Y}\\) representing the following relationship between \\(W\\) and \\(Y\\):\n\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) = 0.8\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Sun}) = 0.2\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) = 0.1\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Rain}) = 0.9\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Our PGM of the Partier‚Äôs Dilemma\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\Pr(Y = \\textsf{Stay} \\mid W)\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W)\\)\n\n\n\n\n\\(W = \\textsf{Sun}\\)\n0.2\n0.8\n\n\n\\(W = \\textsf{Rain}\\)\n0.9\n0.1\n\n\n\n\n\nFigure¬†2: The Conditional Probability Table (CPT) for the edge \\(\\pedge{W}{Y}\\) in Figure¬†1"
  },
  {
    "objectID": "w02/slides.html#observed-vs.-latent-nodes",
    "href": "w02/slides.html#observed-vs.-latent-nodes",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed vs.¬†Latent Nodes",
    "text": "Observed vs.¬†Latent Nodes\n\nPGMs help us make valid (Bayesian) inferences about the world in the face of incomplete information!\nKey remaining tool: separation of nodes into two categories:\n\nObserved nodes (shaded)\nLatent nodes (unshaded)\n\n\\(\\Rightarrow\\) Can use our PGM as a weather-inference machine!\nIf we observe \\(i\\) at a party, what can we infer about the weather outside?"
  },
  {
    "objectID": "w02/slides.html#observed-partier-latent-weather",
    "href": "w02/slides.html#observed-partier-latent-weather",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed Partier, Latent Weather",
    "text": "Observed Partier, Latent Weather\n\nWe can draw this situation as a PGM with shaded and unshaded nodes, distinguishing what we know from what we‚Äôd like to infer:\n\n\n\n\n\n\n\n\n\n\n‚ùì\n¬†\n‚úÖ\n\n\n\n\nAnd we can now use Bayes‚Äô Rule to compute how observed information (\\(i\\) at party \\(\\Rightarrow [Y = \\textsf{Go}]\\)) ‚Äúflows‚Äù back into \\(W\\)"
  },
  {
    "objectID": "w02/slides.html#computation-via-bayes-rule",
    "href": "w02/slides.html#computation-via-bayes-rule",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Computation via Bayes‚Äô Rule",
    "text": "Computation via Bayes‚Äô Rule\n\nBayes‚Äô Rule, \\(\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}\\), tells us how to use info about \\(\\Pr(B \\mid A)\\) to obtain info about \\(\\Pr(A \\mid B)\\)!\nWe use it to obtain a distribution for \\(W\\) updated to incorporate new info \\([Y = \\textsf{Go}]\\):\n\n\\[\n\\begin{align*}\n&\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go})\n= \\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go})} \\\\\n=\\, &\\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun}) + \\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) \\Pr(W = \\textsf{Rain})}\n\\end{align*}\n\\]\n\nPlug in info from CPT to obtain our new (conditional) probability of interest:\n\n\\[\n\\begin{align*}\n\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go}) &= \\frac{(0.8)(0.5)}{(0.8)(0.5) + (0.1)(0.5)} = \\frac{0.4}{0.4 + 0.05} \\approx 0.89\n\\end{align*}\n\\]\n\nWe‚Äôve learned something interesting! Observing \\(i\\) at the party \\(\\leadsto\\) probability of sun jumps from \\(0.5\\) (‚Äúprior‚Äù estimate of \\(W\\), best guess without any other relevant info) to \\(0.89\\) (‚Äúposterior‚Äù estimate of \\(W\\), best guess after incorporating relevant info)."
  },
  {
    "objectID": "w02/slides.html#references",
    "href": "w02/slides.html#references",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nKoller, Daphne, and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press. https://www.dropbox.com/scl/fi/6qmnr9feizngaq4005isj/Probabilistic-Graphical-Models.pdf?rlkey=4803nmsffh4xtsdfiji8jppb1&dl=1.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1."
  },
  {
    "objectID": "w02/slides.html#appendix-zero-probabilities",
    "href": "w02/slides.html#appendix-zero-probabilities",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Appendix: Zero Probabilities",
    "text": "Appendix: Zero Probabilities\nFrom Koller and Friedman (2009), pp.¬†66-67:\n\nZero probabilities: A common mistake is to assign a probability of zero to an event that is extremely unlikely, but not impossible. The problem is that one can never condition away a zero probability, no matter how much evidence we get. When an event is unlikely but not impossible, giving it probability zero is guaranteed to lead to irrecoverable errors. For example, in one of the early versions of the the Pathfinder system (box 3.D), 10 percent of the misdiagnoses were due to zero probability estimates given by the expert to events that were unlikely but not impossible.\n\n‚Üê Back to slide"
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#courtney-green",
    "href": "w02/index.html#courtney-green",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Courtney Green",
    "text": "Courtney Green\nBackground: BA in Public Policy & Leadership, now interested in using data to examine structural disparities.\nInterests & what I can help with: How sociodemographic factors (e.g., race, gender, income, immigration status, education level) shape policy outcomes and institutional practices in areas like criminal justice, housing, healthcare, education, and environment.\n\nAsk me about: Counterfactual balancing, handling messy or privacy-limited datasets, and framing causal questions around inequality.\nReach out if you‚Äôre thinking about a final project that touches social systems, systemic inequity, or fairness!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#what-got-me-interested-in-causal-inference",
    "href": "w02/index.html#what-got-me-interested-in-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Got Me Interested in Causal Inference",
    "text": "What Got Me Interested in Causal Inference\n5000 Project: Over-Policing and Wrongful Convictions in Illinois\n\nMotivation: Explore what demographic factors predict wrongful convictions using real exoneration data.\nChallenge: No data on ‚Äúnon-exonerated innocents,‚Äù so I (with prof‚Äôs help) created a counterfactual dataset by sampling from the incarcerated population\nMethod: Trained models (logistic regression, random forest, KNN) on a balanced dataset of 548 exonerated and 548 non-exonerated individuals\nFinding: Race and geography (esp.¬†Cook Count/Chicago) were the strongest predictors of exoneration\nTakeaway: Questions about systemic bias can‚Äôt be answered with just descriptive stats, you need counterfactual reasoning and causal inference\n\n(Optional) View the counterfactual setup",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#wendy-hu",
    "href": "w02/index.html#wendy-hu",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Wendy Hu",
    "text": "Wendy Hu\n\nBackground: BS in Social Work, passionate about advancing social equity through computational tools\nInterest: I bring a background in social welfare, public health, and NGO data, with experience analyzing behavioral and institutional outcomes. My work spans Indigenous health, gender equity, and trauma-informed service design‚Äîalways with a focus on equity, system change, and policy relevance.\nAsk me about: Framing social justice questions in data terms, working with community or clinical datasets and interpreting model results in context.\nIf you‚Äôre designing a project around health, inequality, nonprofit impact, or any socially embedded issue‚ÄîI‚Äôd love to help bridge rigor and relevance.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#jeffs-hw1-updateapology",
    "href": "w02/index.html#jeffs-hw1-updateapology",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Jeff‚Äôs HW1 Update/Apology",
    "text": "Jeff‚Äôs HW1 Update/Apology\n\nBasically‚Ä¶ I goofed by trying to shove a homework into the first two weeks of class üò≠\nLast week was ‚Äúsetting the table‚Äù, so there are a few (multiple-choice) questions on that\nBut, really only makes sense to have HW starting from end of today, once we‚Äôve introduced PGMs, the language we need to actually do Causal Computational Social Science!\n\n\n Due date pushed to Friday, June 6, 5:59pm",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#hw1-detail-plz-dont-hate-me",
    "href": "w02/index.html#hw1-detail-plz-dont-hate-me",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)",
    "text": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)\n(The main reason this is taking me so long)\n\nTwo key libraries for ‚Äúmain‚Äù course content (HW2 onwards):\n\nIn R: rethinking, ‚Äúlite‚Äù version of Stan\nIn Python: PyMC\n\nBoth are overkill for current unit: lots of work required to ‚Äúturn off‚Äù fancy features and implement basic PGMs\n\\(\\leadsto\\) Only way I know to achieve two goals:\n\n Learning simple PGMs first, as tools for thinking, before adding in additional full-on coding baggage of Stan/PyMC\n Having an entire textbook on these base PGMs that you can use as reference\n\nIs to use [small, restricted] subset of JavaScript, called WebPPL, because‚Ä¶\n\n\nhttp://probmods.org/",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "href": "w02/index.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality",
    "text": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality\n\n\n You‚Äôll no longer be able to read ‚Äúscientific‚Äù writing without striking this expression (involuntarily):\n\n ‚ÄúScientific‚Äù talks will begin to sound like the following:",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#blasting-off-into-causality",
    "href": "w02/index.html#blasting-off-into-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Blasting Off Into Causality!",
    "text": "Blasting Off Into Causality!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#data-generating-processes-dgps",
    "href": "w02/index.html#data-generating-processes-dgps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Data-Generating Processes (DGPs)",
    "text": "Data-Generating Processes (DGPs)\n\n\n\n\n\n\n\nYou saw this in DSAN 5100!\n¬´\\(X_1, \\ldots, X_n\\) drawn i.i.d. Normal, mean \\(\\mu\\) variance \\(\\sigma^2\\)¬ª characterizes DGP of \\((X_1, \\ldots, X_n)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n5650: Dive into DGPs, rather than treating as black box/footnote to Law of Large Numbers, so we can move [asymptotically!]‚Ä¶\nFrom associational statements:\n\n¬´\\(\\underbrace{\\text{An increase}}_{\\small\\text{noun}}\\) in \\(X\\) by 1 is associated with increase in \\(Y\\) by \\(\\beta\\)¬ª\n\nTo causal ones: ¬´\\(\\underbrace{\\text{Increasing}}_{\\small\\text{verb}}\\) \\(X\\) by 1 causes \\(Y\\) to increase by \\(\\beta\\)¬ª",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#dgps-and-the-emergence-of-order",
    "href": "w02/index.html#dgps-and-the-emergence-of-order",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "DGPs and the Emergence of Order",
    "text": "DGPs and the Emergence of Order\n\n\n\n\n\n\n\nWho can guess the state of this process after 10 steps, with 1 person?\n10 people? 50? 100? (If they find themselves on the same spot, they stand on each other‚Äôs heads)\n100 steps? 1000?",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-result-16-steps",
    "href": "w02/index.html#the-result-16-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 16 Steps",
    "text": "The Result: 16 Steps",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-result-64-steps",
    "href": "w02/index.html#the-result-64-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 64 Steps",
    "text": "The Result: 64 Steps",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#mathematicalscientific-modeling",
    "href": "w02/index.html#mathematicalscientific-modeling",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "‚ÄúMathematical/Scientific Modeling‚Äù",
    "text": "‚ÄúMathematical/Scientific Modeling‚Äù\n\nThing we observe (poking out of water): data\nHidden but possibly discoverable via deeper dive (ecosystem under surface): DGP",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#so-whats-the-problem",
    "href": "w02/index.html#so-whats-the-problem",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-shallow-problem-of-causal-inference",
    "href": "w02/index.html#the-shallow-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Shallow Problem of Causal Inference",
    "text": "The Shallow Problem of Causal Inference\n\n\n\n\n\n\n\n\ncor(ski_df$value, law_df$value)\n[1] 0.9921178\n\n(Data from Vigen, Spurious Correlations)\n\nThis, however, is only a mini-boss. Beyond it lies the truly invincible FINAL BOSS‚Ä¶ üôÄ",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-fundamental-problem-of-causal-inference",
    "href": "w02/index.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#what-is-to-be-done",
    "href": "w02/index.html#what-is-to-be-done",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#probability",
    "href": "w02/index.html#probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#beyond-conditional-probability",
    "href": "w02/index.html#beyond-conditional-probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#preview-do-calculus",
    "href": "w02/index.html#preview-do-calculus",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w02/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#ulysses-and-the-computational-sirens",
    "href": "w02/index.html#ulysses-and-the-computational-sirens",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Ulysses and the [Computational] Sirens",
    "text": "Ulysses and the [Computational] Sirens",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#bayesian-inference-but-with-pictures",
    "href": "w02/index.html#bayesian-inference-but-with-pictures",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Bayesian Inference but with Pictures",
    "text": "Bayesian Inference but with Pictures\nA Probabilistic Graphical Model (PGM) provides us with:\n\nA formal-mathematical‚Ä¶\nBut also easily visualizable (by construction)‚Ä¶\nRepresentation of a data-generating process (DGP)!\n\nExample: Let‚Äôs model how weather \\(W\\) affects evening plans \\(Y\\): the choice between going to a party or staying in to watch movies\n\n\n\n\n\n\n The Partier‚Äôs Dilemma\n\n\n\n\nA person \\(i\\) wakes up with some initial affinity for partying: \\(\\Pr(Y_i = \\textsf{Go})\\)\n\\(i\\) then goes to their window and observes the weather \\(W_i\\) outside:\n\nIf the weather is sunny, \\(i\\)‚Äôs affinity increases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Sun}) &gt; \\Pr(Y = \\textsf{Go})\\)\nOtherwise, if it is rainy, \\(i\\)‚Äôs affinity decreases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Rain}) &lt; \\Pr(Y = \\textsf{Go})\\)",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#two-main-building-blocks",
    "href": "w02/index.html#two-main-building-blocks",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Two Main ‚ÄúBuilding Blocks‚Äù",
    "text": "Two Main ‚ÄúBuilding Blocks‚Äù\n\nNodes like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}\\) denote Random Variables\n\n\\[\n\\boxed{\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}} \\simeq \\boxed{ \\begin{array}{c|cc}x & \\textsf{Tails} & \\textsf{Heads} \\\\\\hline \\Pr(X = x) & 0.5 & 0.5\\end{array}}\n\\]\n\nEdges like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) denote relationships between RVs\n\nWhat an edge ‚Äúmeans‚Äù can get [ontologically] tricky!\nRetain sanity by just remembering: an edge \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) is included in our PGM if we ‚Äúcare about‚Äù modeling the conditional probability table (CPT) of \\(Y\\) w.r.t. \\(X\\)\n\n\n\\[\n\\require{enclose}\\boxed{ \\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em} } \\simeq \\boxed{\n  \\begin{array}{c|cc}\n  x & \\Pr(Y = \\textsf{Lose} \\mid X = x) & \\Pr(Y = \\textsf{Win} \\mid X = x) \\\\\\hline\n  \\textsf{Tails} & 0.8 & 0.2 \\\\\n  \\textsf{Heads} & 0.5 & 0.5\n  \\end{array}\n}\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#pgm-for-the-partiers-dilemma",
    "href": "w02/index.html#pgm-for-the-partiers-dilemma",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "PGM for the Partier‚Äôs Dilemma",
    "text": "PGM for the Partier‚Äôs Dilemma\n\nA node \\(\\pnode{W}\\) denoting RV \\(W\\), which can take on values in \\(\\mathcal{R}_W = \\{\\textsf{Sun}, \\textsf{Rain}\\}\\),\nA node \\(\\pnode{Y}\\) denoting RV \\(Y\\), which can take on values in \\(\\mathcal{R}_Y = \\{\\textsf{Go}, \\textsf{Stay}\\}\\), and\nAn edge \\(\\pedge{W}{Y}\\) representing the following relationship between \\(W\\) and \\(Y\\):\n\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) = 0.8\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Sun}) = 0.2\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) = 0.1\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Rain}) = 0.9\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Our PGM of the Partier‚Äôs Dilemma\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\Pr(Y = \\textsf{Stay} \\mid W)\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W)\\)\n\n\n\n\n\\(W = \\textsf{Sun}\\)\n0.2\n0.8\n\n\n\\(W = \\textsf{Rain}\\)\n0.9\n0.1\n\n\n\n\n\nFigure¬†2: The Conditional Probability Table (CPT) for the edge \\(\\pedge{W}{Y}\\) in Figure¬†1",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#observed-vs.-latent-nodes",
    "href": "w02/index.html#observed-vs.-latent-nodes",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed vs.¬†Latent Nodes",
    "text": "Observed vs.¬†Latent Nodes\n\nPGMs help us make valid (Bayesian) inferences about the world in the face of incomplete information!\nKey remaining tool: separation of nodes into two categories:\n\nObserved nodes (shaded)\nLatent nodes (unshaded)\n\n\\(\\Rightarrow\\) Can use our PGM as a weather-inference machine!\nIf we observe \\(i\\) at a party, what can we infer about the weather outside?",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#observed-partier-latent-weather",
    "href": "w02/index.html#observed-partier-latent-weather",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed Partier, Latent Weather",
    "text": "Observed Partier, Latent Weather\n\nWe can draw this situation as a PGM with shaded and unshaded nodes, distinguishing what we know from what we‚Äôd like to infer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùì\n¬†\n‚úÖ\n\n\n\n\nAnd we can now use Bayes‚Äô Rule to compute how observed information (\\(i\\) at party \\(\\Rightarrow [Y = \\textsf{Go}]\\)) ‚Äúflows‚Äù back into \\(W\\)",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#computation-via-bayes-rule",
    "href": "w02/index.html#computation-via-bayes-rule",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Computation via Bayes‚Äô Rule",
    "text": "Computation via Bayes‚Äô Rule\n\nBayes‚Äô Rule, \\(\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}\\), tells us how to use info about \\(\\Pr(B \\mid A)\\) to obtain info about \\(\\Pr(A \\mid B)\\)!\nWe use it to obtain a distribution for \\(W\\) updated to incorporate new info \\([Y = \\textsf{Go}]\\):\n\n\\[\n\\begin{align*}\n&\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go})\n= \\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go})} \\\\\n=\\, &\\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun}) + \\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) \\Pr(W = \\textsf{Rain})}\n\\end{align*}\n\\]\n\nPlug in info from CPT to obtain our new (conditional) probability of interest:\n\n\\[\n\\begin{align*}\n\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go}) &= \\frac{(0.8)(0.5)}{(0.8)(0.5) + (0.1)(0.5)} = \\frac{0.4}{0.4 + 0.05} \\approx 0.89\n\\end{align*}\n\\]\n\nWe‚Äôve learned something interesting! Observing \\(i\\) at the party \\(\\leadsto\\) probability of sun jumps from \\(0.5\\) (‚Äúprior‚Äù estimate of \\(W\\), best guess without any other relevant info) to \\(0.89\\) (‚Äúposterior‚Äù estimate of \\(W\\), best guess after incorporating relevant info).",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#references",
    "href": "w02/index.html#references",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nKoller, Daphne, and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press. https://www.dropbox.com/scl/fi/6qmnr9feizngaq4005isj/Probabilistic-Graphical-Models.pdf?rlkey=4803nmsffh4xtsdfiji8jppb1&dl=1.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#appendix-zero-probabilities",
    "href": "w02/index.html#appendix-zero-probabilities",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Appendix: Zero Probabilities",
    "text": "Appendix: Zero Probabilities\nFrom Koller and Friedman (2009), pp.¬†66-67:\n\nZero probabilities: A common mistake is to assign a probability of zero to an event that is extremely unlikely, but not impossible. The problem is that one can never condition away a zero probability, no matter how much evidence we get. When an event is unlikely but not impossible, giving it probability zero is guaranteed to lead to irrecoverable errors. For example, in one of the early versions of the the Pathfinder system (box 3.D), 10 percent of the misdiagnoses were due to zero probability estimates given by the expert to events that were unlikely but not impossible.\n\n‚Üê Back to slide",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#footnotes",
    "href": "w02/index.html#footnotes",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Appendix Slide‚Ü©Ô∏é",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignment Point Distributions",
    "section": "",
    "text": "Use the tabs below to view the point distributions for different assignments.\nThe distributions are imported from Google Sheets mainly for transparency: so that you can see exactly how totals are computed as a sum of the individual points allocated for each test!\n\nHW1\n\n\n\n\n\n\n\n\n\npart\nqid\npoints\nq_total\npart_total\n\n\n\n\n1\nQ1.1a\n4\n\n\n\n\n\nQ1.1b\n2\n6\n\n\n\n\nQ1.2\n8\n8\n\n\n\n\nQ1.3a\n3\n\n\n\n\n\nQ1.3b\n3\n\n\n\n\n\nQ1.3c\n3\n9\n\n\n\n\nQ1.4a\n3\n\n\n\n\n\nQ1.4b\n3\n6\n29\n\n\n2\nQ2.1\n4\n\n\n\n\n\nQ2.2\n4\n\n\n\n\n\nQ2.3\n4\n\n\n\n\n\nQ2.4\n4\n\n16\n\n\n3\nQ3.1\n4\n\n\n\n\n\nQ3.2\n4\n\n\n\n\n\nQ3.3\n4\n\n12\n\n\n4\nQ4.1\n3\n3\n\n\n\n\nQ4.2a\n4\n\n\n\n\n\nQ4.2b\n2\n6\n\n\n\n\nQ4.3a\n2\n\n\n\n\n\nQ4.3b\n2\n4\n\n\n\n\nQ4.4a\n2\n\n\n\n\n\nQ4.4b\n2\n4\n\n\n\n\nQ4.5a\n2\n\n\n\n\n\nQ4.5b\n2\n4\n\n\n\n\nQ4.6a\n2\n\n\n\n\n\nQ4.6b\n2\n4\n25\n\n\n5\nQ5.1\n4\n4\n\n\n\n\nQ5.2\n2\n2\n\n\n\n\nQ5.3\n2\n\n\n\n\n\nQ5.4a\n4\n\n\n\n\n\nQ5.4b\n3\n\n\n\n\n\nQ5.4c\n3\n10\n18\n\n\n\nTotal\n100",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "w03/slides.html#so-whats-the-problem",
    "href": "w03/slides.html#so-whats-the-problem",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often\n\nSee Appendix Slide"
  },
  {
    "objectID": "w03/slides.html#the-fundamental-problem-of-causal-inference",
    "href": "w03/slides.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠"
  },
  {
    "objectID": "w03/slides.html#what-is-to-be-done",
    "href": "w03/slides.html#what-is-to-be-done",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?"
  },
  {
    "objectID": "w03/slides.html#probability",
    "href": "w03/slides.html#probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring."
  },
  {
    "objectID": "w03/slides.html#beyond-conditional-probability",
    "href": "w03/slides.html#beyond-conditional-probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects"
  },
  {
    "objectID": "w03/slides.html#preview-do-calculus",
    "href": "w03/slides.html#preview-do-calculus",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events"
  },
  {
    "objectID": "w03/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w03/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!"
  },
  {
    "objectID": "w03/slides.html#importance-of-observed-vs.-latent-distinction",
    "href": "w03/slides.html#importance-of-observed-vs.-latent-distinction",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Importance of Observed vs.¬†Latent Distinction!",
    "text": "Importance of Observed vs.¬†Latent Distinction!\n\nAcross many different fields, hidden stumbling-block in your project may be failure to model this distinction and pursue its implications!\n\n\n\n\n\n\n\n\n\nFigure¬†1: Your model failing to achieve its goal bc you haven‚Äôt yet distinguished observed vs.¬†latent variables"
  },
  {
    "objectID": "w03/slides.html#example-from-cognitive-neuroscience-visual-perception",
    "href": "w03/slides.html#example-from-cognitive-neuroscience-visual-perception",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Example from Cognitive Neuroscience: Visual Perception",
    "text": "Example from Cognitive Neuroscience: Visual Perception\n\n\n\n\n\n\n\nWe ‚Äúsee‚Äù 3D objects like a basketballs, but our eyes are (curved) 2D surfaces!\n\\(\\Rightarrow\\) Our brains construct 3D environment by combining 2D info (observed photons-hitting-light-cones) with latent heuristic info:\n\nInstantaneous Binocular Disparity, fusing info from two slightly-offset eyes,\nShort-term Motion Parallax: How does object shift over short temporal ‚Äúwindows‚Äù of movement?\nLong-term mental models (orange-ish circle with this line pattern is usually a basketball, which is usually this big, etc.)\n\n\n\n\n\n\n\nImage source (a very cool article!)\n\n\n\n\n\n\nSimilar examples in many other fields \\(\\leadsto\\) science is a strange waltz of general models vs.¬†field-specific details, but there‚Äôs one model that is infinitely helpful imo‚Ä¶"
  },
  {
    "objectID": "w03/slides.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "href": "w03/slides.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!",
    "text": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!\n\nUsing ‚ÄúUr‚Äù in the same sense as ‚ÄúAmerica‚Äôs Ur-Choropleths‚Äù‚Ä¶\nHMMs are our ‚ÄúUr-Models‚Äù for Computational Social Science specifically\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs consider an extremely currently-popular strand of CSS research, and step through why (a) it may be harder than it initially seems, but (b) we can use HMMs to ‚Äúorganize‚Äù/manage/visualize the complexity!\n\n\nAs in, how America‚Äôs Ur-Choropleths are two visualizations you can keep at hand before launching into a specific nation-wide choropleth about a specific issue!"
  },
  {
    "objectID": "w03/slides.html#studying-fake-news",
    "href": "w03/slides.html#studying-fake-news",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Studying ‚ÄúFake News‚Äù",
    "text": "Studying ‚ÄúFake News‚Äù\n\n\n\n\n\n\n\nStudying ‚Äúfake news‚Äù with ML and/or Deep Learning and/or Big Data is very popular in Computational Social Science: let‚Äôs use HMMs to see why it might be more‚Ä¶ difficult/complicated than it seems at first üôà\n\n\n\n\n\nThe (implicit) model in studies like Iyengar and Kinder (2010) is something like:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThus allowing results to be summarized in a table like:\n\n\n\n\n\n\n\n\n\n\n\nIyengar and Kinder (2010)"
  },
  {
    "objectID": "w03/slides.html#the-devil-in-the-details-i",
    "href": "w03/slides.html#the-devil-in-the-details-i",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details I",
    "text": "The Devil in the Details I\n\nResidents of the New Haven, Connecticut area participated in one of two experiments, each of which spanned six consecutive days [‚Ä¶] took place in November 1980, shortly after the presidential election\n\n\nWe measured problem importance with four questions that appeared in both the pretreatment and posttreatment questionnaires:\n\nPlease indicate how important you consider these problems to be.\nShould the federal government do more to develop solutions to these problems, even if it means raising taxes?\nHow much do you yourself care about these problems?\nThese days how much do you talk about these problems?"
  },
  {
    "objectID": "w03/slides.html#the-devil-in-the-details-ii",
    "href": "w03/slides.html#the-devil-in-the-details-ii",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details II",
    "text": "The Devil in the Details II"
  },
  {
    "objectID": "w03/slides.html#randomization-and-fine-tuned-treatment",
    "href": "w03/slides.html#randomization-and-fine-tuned-treatment",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Randomization and Fine-Tuned Treatment",
    "text": "Randomization and Fine-Tuned Treatment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶These are the types of things we usually don‚Äôt have control over as data scientists (we‚Äôre just handed a .csv!)"
  },
  {
    "objectID": "w03/slides.html#lets-model-it",
    "href": "w03/slides.html#lets-model-it",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Let‚Äôs Model It!",
    "text": "Let‚Äôs Model It!"
  },
  {
    "objectID": "w03/slides.html#the-final-piece-plate-notation",
    "href": "w03/slides.html#the-final-piece-plate-notation",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Final Piece: Plate Notation",
    "text": "The Final Piece: Plate Notation\n\nFor describing general distributions, there is often a ‚Äúsingle node generating a bunch of nodes‚Äù structure:\n\n\n\n\n\n\n\nPGM notation has a built-in tool for this: plates!"
  },
  {
    "objectID": "w03/slides.html#crucial-css-model-we-can-now-dive-into",
    "href": "w03/slides.html#crucial-css-model-we-can-now-dive-into",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Crucial CSS Model We Can Now Dive Into!",
    "text": "Crucial CSS Model We Can Now Dive Into!\n\nImage source"
  },
  {
    "objectID": "w03/slides.html#what-does-this-give-us",
    "href": "w03/slides.html#what-does-this-give-us",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Does This Give Us?",
    "text": "What Does This Give Us?"
  },
  {
    "objectID": "w03/slides.html#before-we-branch-off-of-pgms",
    "href": "w03/slides.html#before-we-branch-off-of-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Before We Branch Off Of PGMs",
    "text": "Before We Branch Off Of PGMs\n(Even in non-causal settings)\n\nImage source\nWe don‚Äôt exactly think ‚ÄúShakespeare decided on a set of topics,‚Äù"
  },
  {
    "objectID": "w03/slides.html#getting-shot-by-a-firing-squad",
    "href": "w03/slides.html#getting-shot-by-a-firing-squad",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Getting Shot By A Firing Squad",
    "text": "Getting Shot By A Firing Squad\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(O\\)\n\\(C\\)\n\\(A\\)\n\\(B\\)\n\\(D\\)\n\n\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n\n\\[\n\\begin{align*}\n\\Pr(D) &= 0.4 \\\\\n\\Pr(D \\mid A) &= 1\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w03/slides.html#references",
    "href": "w03/slides.html#references",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nIyengar, Shanto, and Donald R. Kinder. 2010. News That Matters: Television & American Opinion. University of Chicago Press.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1."
  },
  {
    "objectID": "w03/index.html",
    "href": "w03/index.html",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#so-whats-the-problem",
    "href": "w03/index.html#so-whats-the-problem",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-fundamental-problem-of-causal-inference",
    "href": "w03/index.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#what-is-to-be-done",
    "href": "w03/index.html#what-is-to-be-done",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#probability",
    "href": "w03/index.html#probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring.",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#beyond-conditional-probability",
    "href": "w03/index.html#beyond-conditional-probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#preview-do-calculus",
    "href": "w03/index.html#preview-do-calculus",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w03/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#importance-of-observed-vs.-latent-distinction",
    "href": "w03/index.html#importance-of-observed-vs.-latent-distinction",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Importance of Observed vs.¬†Latent Distinction!",
    "text": "Importance of Observed vs.¬†Latent Distinction!\n\nAcross many different fields, hidden stumbling-block in your project may be failure to model this distinction and pursue its implications!\n\n\n\n\n\n\n\n\n\nFigure¬†1: Your model failing to achieve its goal bc you haven‚Äôt yet distinguished observed vs.¬†latent variables",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#example-from-cognitive-neuroscience-visual-perception",
    "href": "w03/index.html#example-from-cognitive-neuroscience-visual-perception",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Example from Cognitive Neuroscience: Visual Perception",
    "text": "Example from Cognitive Neuroscience: Visual Perception\n\n\n\n\n\n\n\nWe ‚Äúsee‚Äù 3D objects like a basketballs, but our eyes are (curved) 2D surfaces!\n\\(\\Rightarrow\\) Our brains construct 3D environment by combining 2D info (observed photons-hitting-light-cones) with latent heuristic info:\n\nInstantaneous Binocular Disparity, fusing info from two slightly-offset eyes,\nShort-term Motion Parallax: How does object shift over short temporal ‚Äúwindows‚Äù of movement?\nLong-term mental models (orange-ish circle with this line pattern is usually a basketball, which is usually this big, etc.)\n\n\n\n\n\n\n\nImage source (a very cool article!)\n\n\n\n\n\n\nSimilar examples in many other fields \\(\\leadsto\\) science is a strange waltz of general models vs.¬†field-specific details, but there‚Äôs one model that is infinitely helpful imo‚Ä¶",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "href": "w03/index.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!",
    "text": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!\n\nUsing ‚ÄúUr‚Äù in the same sense as ‚ÄúAmerica‚Äôs Ur-Choropleths‚Äù‚Ä¶\nHMMs are our ‚ÄúUr-Models‚Äù for Computational Social Science specifically\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs consider an extremely currently-popular strand of CSS research, and step through why (a) it may be harder than it initially seems, but (b) we can use HMMs to ‚Äúorganize‚Äù/manage/visualize the complexity!\n\n\nAs in, how America‚Äôs Ur-Choropleths are two visualizations you can keep at hand before launching into a specific nation-wide choropleth about a specific issue!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#studying-fake-news",
    "href": "w03/index.html#studying-fake-news",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Studying ‚ÄúFake News‚Äù",
    "text": "Studying ‚ÄúFake News‚Äù\n\n\n\n\n\n\n\n\n\n\nStudying ‚Äúfake news‚Äù with ML and/or Deep Learning and/or Big Data is very popular in Computational Social Science: let‚Äôs use HMMs to see why it might be more‚Ä¶ difficult/complicated than it seems at first üôà\n\n\n\n\n\nThe (implicit) model in studies like Iyengar and Kinder (2010) is something like:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThus allowing results to be summarized in a table like:\n\n\n\n\n\n\n\n\n\n\n\nIyengar and Kinder (2010)",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-devil-in-the-details-i",
    "href": "w03/index.html#the-devil-in-the-details-i",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details I",
    "text": "The Devil in the Details I\n\nResidents of the New Haven, Connecticut area participated in one of two experiments, each of which spanned six consecutive days [‚Ä¶] took place in November 1980, shortly after the presidential election\n\n\nWe measured problem importance with four questions that appeared in both the pretreatment and posttreatment questionnaires:\n\nPlease indicate how important you consider these problems to be.\nShould the federal government do more to develop solutions to these problems, even if it means raising taxes?\nHow much do you yourself care about these problems?\nThese days how much do you talk about these problems?",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-devil-in-the-details-ii",
    "href": "w03/index.html#the-devil-in-the-details-ii",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details II",
    "text": "The Devil in the Details II",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#randomization-and-fine-tuned-treatment",
    "href": "w03/index.html#randomization-and-fine-tuned-treatment",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Randomization and Fine-Tuned Treatment",
    "text": "Randomization and Fine-Tuned Treatment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶These are the types of things we usually don‚Äôt have control over as data scientists (we‚Äôre just handed a .csv!)",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#lets-model-it",
    "href": "w03/index.html#lets-model-it",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Let‚Äôs Model It!",
    "text": "Let‚Äôs Model It!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-final-piece-plate-notation",
    "href": "w03/index.html#the-final-piece-plate-notation",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Final Piece: Plate Notation",
    "text": "The Final Piece: Plate Notation\n\nFor describing general distributions, there is often a ‚Äúsingle node generating a bunch of nodes‚Äù structure:\n\n\n\n\n\n\n\nPGM notation has a built-in tool for this: plates!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#crucial-css-model-we-can-now-dive-into",
    "href": "w03/index.html#crucial-css-model-we-can-now-dive-into",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Crucial CSS Model We Can Now Dive Into!",
    "text": "Crucial CSS Model We Can Now Dive Into!\n\n\n\nImage source",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#what-does-this-give-us",
    "href": "w03/index.html#what-does-this-give-us",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Does This Give Us?",
    "text": "What Does This Give Us?",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#before-we-branch-off-of-pgms",
    "href": "w03/index.html#before-we-branch-off-of-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Before We Branch Off Of PGMs",
    "text": "Before We Branch Off Of PGMs\n(Even in non-causal settings)\n\n\n\nImage source\n\n\n\nWe don‚Äôt exactly think ‚ÄúShakespeare decided on a set of topics,‚Äù",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#getting-shot-by-a-firing-squad",
    "href": "w03/index.html#getting-shot-by-a-firing-squad",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Getting Shot By A Firing Squad",
    "text": "Getting Shot By A Firing Squad\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(O\\)\n\\(C\\)\n\\(A\\)\n\\(B\\)\n\\(D\\)\n\n\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n\n\\[\n\\begin{align*}\n\\Pr(D) &= 0.4 \\\\\n\\Pr(D \\mid A) &= 1\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#references",
    "href": "w03/index.html#references",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nIyengar, Shanto, and Donald R. Kinder. 2010. News That Matters: Television & American Opinion. University of Chicago Press.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1.",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#footnotes",
    "href": "w03/index.html#footnotes",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Appendix Slide‚Ü©Ô∏é",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html",
    "href": "w04/index.html",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "href": "w04/index.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?",
    "text": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?\n\nMy answer: allows you to adapt to specifics/idiosyncrasies of your problem!\nLanguage metaphor: Learning models vs.¬†learning modeling language \\(\\Leftrightarrow\\) Learning phrases in a language vs.¬†learning to speak the language\n‚ÄúHello, one hamburger please‚Äù is good, but what if you‚Ä¶\n Are allergic to ketchup and need to make sure it‚Äôs removed\n Want to replace sesame seed bun with poppy seed bun, if they have it\n Prefer spicy, but not too spicy, mustard  Bun only  Animal style  ‚Ä¶\n\n\n\n\n\n\n\n\nLanguages give us a syntax‚Ä¶\n\n\n\nS\n\\(\\rightarrow\\)\nNP VP\n\n\nNP\n\\(\\rightarrow\\)\nDetP N | AdjP NP\n\n\nVP\n\\(\\rightarrow\\)\nV NP\n\n\nAdjP\n\\(\\rightarrow\\)\nAdj | Adv AdjP\n\n\nN\n\\(\\rightarrow\\)\nfrog | tadpole\n\n\nV\n\\(\\rightarrow\\)\nsees | likes\n\n\nAdj\n\\(\\rightarrow\\)\nbig | small\n\n\nAdv\n\\(\\rightarrow\\)\nvery\n\n\nDetP\n\\(\\rightarrow\\)\na | the\n\n\n\n\n\n\n\n‚Ä¶For expressing arbitrary (infinitely many!) sentences",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "href": "w04/index.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)",
    "text": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)\nNeed a language that can communicate the following info to estimation algorithm:\n\n Unit of observation is tadpole, but unit of analysis is tank\n Ultimately, I care about \\(Y =\\) survival rate (dependent var), as function of \\(X =\\) tank properties (independent var)\n ‚Ä¶But the \\(n_i = 48\\) tanks actually come in \\(n_j = 3\\) types: small (10 bois), medium (25), large (35) (Bonus: What if there are different numbers of tanks per type?)\n I need it to account for impact of tank size, then  pool info across tank sizes\n\n\n\n\nFrom McElreath (2020)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#example-2-dissertation-nightmare",
    "href": "w04/index.html#example-2-dissertation-nightmare",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 2: Dissertation Nightmare",
    "text": "Example 2: Dissertation Nightmare\n\n\n\n\n\n\n\n\n\nAbove: Data from Soviet archives; Above Right: US Military archives; Below Right: NATO archives",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#nightmarish-without-a-modeling-language",
    "href": "w04/index.html#nightmarish-without-a-modeling-language",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Nightmarish Without a Modeling Language!",
    "text": "Nightmarish Without a Modeling Language!\n\nModeling language \\(\\Rightarrow\\) Unambiguously ‚Äúencode‚Äù idiosyncratic domain knowledge\nDissertation: Cold War \\(\\times\\) ‚ÄúThird World‚Äù \\(\\leadsto\\) Cuban üá®üá∫ trans-continental operations1\nMain narrative (for estimation): 1975 (South Africa invades Angola, 14 Oct ‚Üí üá®üá∫ intervention, 4 Nov) to 1979 (USSR requests üá®üá∫ troops to Ethiopia for Ogaden War)\n[Ontology] Fix 1979 geographic entities at National level (as modeling choice, like fixing 2000 USD to measure inflation): \\(\\textsf{Cuba}_{1979}\\), \\(\\textsf{Angola}_{1979}\\), \\(\\textsf{PDRY}_{1979}\\), \\(\\textsf{YAR}_{1979}\\)\nDifferent tokens (Think NLP: \"Congo\", \"DRC\", \"Republic of Congo\") can then be contextualized: can ‚Äútrack‚Äù and link data appropriately despite splits, merges, name changes\nSay we have data on ‚ÄúNumber of Communist Militants in \\(X\\)‚Äù (Hoover Yearbook)‚Ä¶\n\n\n\n\nEntity\nData from 1947-1971 at...\nData from 1971-Present at...\n\n\n\n\n\\(\\textsf{Pakistan}_{1979}\\)\nNational Level:\n\\(\\frac{62}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúPakistan‚Äù\n\n\nSubnational Level:\n‚ÄúWest Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)\n\n\n\\(\\textsf{Bangladesh}_{1979}\\)\nNational Level:\n\\(\\frac{70}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúBangladesh‚Äù\n\n\nSubnational Level:\n‚ÄúEast Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#the-fork-x-leftarrow-z-rightarrow-y",
    "href": "w04/index.html#the-fork-x-leftarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)",
    "text": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\nfork_df &lt;- tibble(\n    Z = rbern(n_d),\n    X = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\nplot_freqs &lt;- function(df, plot_title, y_lab=TRUE) {\n  df_cor &lt;- cor(df$X, df$Y)\n  df_label &lt;- paste0(\"Cor(X,Y) = \",round(df_cor,3))\n  freq_df &lt;- df |&gt;\n    group_by(X, Y) |&gt;\n    summarize(count=n())\n  freq_plot &lt;- freq_df |&gt;\n    ggplot(\n      aes(x=factor(X), y=factor(Y), fill=count)\n    ) +\n    geom_tile() +\n    coord_equal() +\n    scale_fill_distiller(\n      palette=\"Greens\", direction=1,\n      limits=c(0,5000)\n    ) +\n    geom_label(\n      aes(label=count),\n      fill=\"white\", color=\"black\", size=7\n    ) +\n    labs(\n      title = plot_title,\n      subtitle = df_label,\n      x=\"X\", y=\"Y\"\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      plot.title = element_text(size=21),\n      plot.subtitle = element_text(size=18)\n    ) +\n    remove_legend()\n  if (!y_lab) {\n    freq_plot &lt;- freq_plot + theme(\n      axis.title.y = element_blank()\n    )\n  }\n  return(freq_plot)\n}\n# The full df\nfull_label &lt;- paste0(\"Raw Data (n = 10K)\")\nfull_plot &lt;- plot_freqs(fork_df, full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 0\nz0_df &lt;- fork_df |&gt; filter(Z == 0)\nz0_n &lt;- nrow(z0_df)\nz0_label &lt;- paste0(\"Z == 0 (\",z0_n,\" obs)\")\nz0_plot &lt;- plot_freqs(z0_df, z0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 1\nz1_df &lt;- fork_df |&gt; filter(Z == 1)\nz1_n &lt;- nrow(z1_df)\nz1_label &lt;- paste0(\"Z == 1 (\",z1_n,\" obs)\")\nz1_plot &lt;- plot_freqs(z1_df, z1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\nfull_plot | z0_plot | z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"$Slope_{Z=0} = \",z0_slope,\"$\")\nz0_leg_label &lt;- TeX(paste0(\"0 $(m=\",z0_slope,\")$\"))\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"$Slope_{Z=1} = \",z1_slope,\"$\")\nz_texlabel &lt;- TeX(paste0(z0_label, \" | \", z1_label))\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "href": "w04/index.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)",
    "text": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\npipe_df &lt;- tibble(\n    X = rbern(n_d),\n    Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\n# The full df\npipe_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\npipe_full_plot &lt;- plot_freqs(pipe_df, pipe_full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 0\npipe_z0_df &lt;- pipe_df |&gt; filter(Z == 0)\npipe_z0_n &lt;- nrow(pipe_z0_df)\npipe_z0_label &lt;- paste0(\"Z == 0 (\",pipe_z0_n,\" obs)\")\npipe_z0_plot &lt;- plot_freqs(pipe_z0_df, pipe_z0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 1\npipe_z1_df &lt;- pipe_df |&gt; filter(Z == 1)\npipe_z1_n &lt;- nrow(pipe_z1_df)\npipe_z1_label &lt;- paste0(\"Z == 1 (\",pipe_z1_n,\" obs)\")\npipe_z1_plot &lt;- plot_freqs(pipe_z1_df, pipe_z1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\npipe_full_plot | pipe_z0_plot | pipe_z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",cpipe_z0_slope,\"$\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",cpipe_z1_slope,\"$\")\ncpipe_z_texlabel &lt;- TeX(paste0(cpipe_z0_label, \" | \", cpipe_z1_label))\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    subtitle=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#the-collider-x-rightarrow-z-leftarrow-y",
    "href": "w04/index.html#the-collider-x-rightarrow-z-leftarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)",
    "text": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)\n\n\n\n\n\nCode\nset.seed(5650)\ncoll_df &lt;- tibble(\n    X = rbern(n_d),\n    Y = rbern(n_d),\n    Z = rbern(n_d, ifelse(X + Y &gt; 0, 0.9, 0.2)),\n)\n\n\n\n\nCode\n# The full df\ncoll_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\ncoll_full_plot &lt;- plot_freqs(coll_df, coll_full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 0\ncoll_z0_df &lt;- coll_df |&gt; filter(Z == 0)\ncoll_z0_n &lt;- nrow(coll_z0_df)\ncoll_z0_label &lt;- paste0(\"Z == 0 (\",coll_z0_n,\" obs)\")\ncoll_z0_plot &lt;- plot_freqs(coll_z0_df, coll_z0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 1\ncoll_z1_df &lt;- coll_df |&gt; filter(Z == 1)\ncoll_z1_n &lt;- nrow(coll_z1_df)\ncoll_z1_label &lt;- paste0(\"Z == 1 (\",coll_z1_n,\" obs)\")\ncoll_z1_plot &lt;- plot_freqs(coll_z1_df, coll_z1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\ncoll_full_plot | coll_z0_plot | coll_z1_plot\n\n\n\n\n\n\n\n\n\n\nConditioning on colliders induces correlation where there previously was none ‚ò†Ô∏è\n\n\n\n\n\nCode\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\n\n\n\n\nCode\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",ccoll_z0_slope,\"$\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",ccoll_z1_slope,\"$\")\nccoll_z_texlabel &lt;- TeX(paste0(ccoll_z0_label, \" | \", ccoll_z1_label))\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",ccoll_slope\n    ),\n    subtitle=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶This is why we have to think, rather than just ‚Äúcontrol for everything‚Äù! üò≠",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#proxies-for-z",
    "href": "w04/index.html#proxies-for-z",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Proxies for \\(Z\\)",
    "text": "Proxies for \\(Z\\)\n\n\n\n\n\nCode\nset.seed(5650)\nprox_df &lt;- tibble(\n  X = rbern(n_d),\n  Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n  Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n  A = rbern(n_d, (1-Z)*0.1 + Z*0.9)\n)\n\n\n\n\nCode\n# The full df\nprox_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\nprox_full_plot &lt;- plot_freqs(prox_df, prox_full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on A == 0\nprox_a0_df &lt;- prox_df |&gt; filter(A == 0)\nprox_a0_n &lt;- nrow(prox_a0_df)\nprox_a0_label &lt;- paste0(\"A == 0 (\",prox_a0_n,\" obs)\")\nprox_a0_plot &lt;- plot_freqs(prox_a0_df, prox_a0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on A == 1\nprox_a1_df &lt;- prox_df |&gt; filter(A == 1)\nprox_a1_n &lt;- nrow(prox_a1_df)\nprox_a1_label &lt;- paste0(\"A == 1 (\",prox_a1_n,\" obs)\")\nprox_a1_plot &lt;- plot_freqs(prox_a1_df, prox_a1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\nprox_full_plot | prox_a0_plot | prox_a1_plot\n\n\n\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A\\) gives us some (not all!) information about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\nn_c &lt;- 300\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"$Slope_{A=0} = \",cprox_a0_slope,\"$\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"$Slope_{A=1} = \",cprox_a1_slope,\"$\")\ncprox_a_texlabel &lt;- TeX(paste0(cprox_a0_label, \" | \", cprox_a1_label))\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_text(size=18)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#references",
    "href": "w04/index.html#references",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "References",
    "text": "References\n\n\nBezuidenhout, Dana, Sarah Forthal, Kara Rudolph, and Matthew R Lamb. 2024. ‚ÄúSingle World Intervention Graphs (SWIGs): A Practical Guide.‚Äù American Journal of Epidemiology, September, kwae353. https://doi.org/10.1093/aje/kwae353.\n\n\nGleijeses, Piero. 2013. Visions of Freedom: Havana, Washington, Pretoria, and the Struggle for Southern Africa, 1976-1991. UNC Press Books.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and STAN. CRC Press.",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#footnotes",
    "href": "w04/index.html#footnotes",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHelpful metaphor (Gleijeses 2013): Cuba \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for USSR (Soviet $ but Cuban training of PAIGC ‚Üí MPLA), as Israel \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for US (US $ but Israeli training of SAVAK ‚Üí SADF)‚Ü©Ô∏é",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/slides.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "href": "w04/slides.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?",
    "text": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?\n\nMy answer: allows you to adapt to specifics/idiosyncrasies of your problem!\nLanguage metaphor: Learning models vs.¬†learning modeling language \\(\\Leftrightarrow\\) Learning phrases in a language vs.¬†learning to speak the language\n‚ÄúHello, one hamburger please‚Äù is good, but what if you‚Ä¶\n Are allergic to ketchup and need to make sure it‚Äôs removed\n Want to replace sesame seed bun with poppy seed bun, if they have it\n Prefer spicy, but not too spicy, mustard  Bun only  Animal style  ‚Ä¶\n\n\n\n\n\n\n\n\nLanguages give us a syntax‚Ä¶\n\n\n\nS\n\\(\\rightarrow\\)\nNP VP\n\n\nNP\n\\(\\rightarrow\\)\nDetP N | AdjP NP\n\n\nVP\n\\(\\rightarrow\\)\nV NP\n\n\nAdjP\n\\(\\rightarrow\\)\nAdj | Adv AdjP\n\n\nN\n\\(\\rightarrow\\)\nfrog | tadpole\n\n\nV\n\\(\\rightarrow\\)\nsees | likes\n\n\nAdj\n\\(\\rightarrow\\)\nbig | small\n\n\nAdv\n\\(\\rightarrow\\)\nvery\n\n\nDetP\n\\(\\rightarrow\\)\na | the\n\n\n\n\n\n\n\n‚Ä¶For expressing arbitrary (infinitely many!) sentences"
  },
  {
    "objectID": "w04/slides.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "href": "w04/slides.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)",
    "text": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)\nNeed a language that can communicate the following info to estimation algorithm:\n\n Unit of observation is tadpole, but unit of analysis is tank\n Ultimately, I care about \\(Y =\\) survival rate (dependent var), as function of \\(X =\\) tank properties (independent var)\n ‚Ä¶But the \\(n_i = 48\\) tanks actually come in \\(n_j = 3\\) types: small (10 bois), medium (25), large (35) (Bonus: What if there are different numbers of tanks per type?)\n I need it to account for impact of tank size, then  pool info across tank sizes\n\n\nFrom McElreath (2020)"
  },
  {
    "objectID": "w04/slides.html#example-2-dissertation-nightmare",
    "href": "w04/slides.html#example-2-dissertation-nightmare",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 2: Dissertation Nightmare",
    "text": "Example 2: Dissertation Nightmare\n\n\n\n\n\n\n\n\n\nAbove: Data from Soviet archives; Above Right: US Military archives; Below Right: NATO archives"
  },
  {
    "objectID": "w04/slides.html#nightmarish-without-a-modeling-language",
    "href": "w04/slides.html#nightmarish-without-a-modeling-language",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Nightmarish Without a Modeling Language!",
    "text": "Nightmarish Without a Modeling Language!\n\nModeling language \\(\\Rightarrow\\) Unambiguously ‚Äúencode‚Äù idiosyncratic domain knowledge\nDissertation: Cold War \\(\\times\\) ‚ÄúThird World‚Äù \\(\\leadsto\\) Cuban üá®üá∫ trans-continental operations1\nMain narrative (for estimation): 1975 (South Africa invades Angola, 14 Oct ‚Üí üá®üá∫ intervention, 4 Nov) to 1979 (USSR requests üá®üá∫ troops to Ethiopia for Ogaden War)\n[Ontology] Fix 1979 geographic entities at National level (as modeling choice, like fixing 2000 USD to measure inflation): \\(\\textsf{Cuba}_{1979}\\), \\(\\textsf{Angola}_{1979}\\), \\(\\textsf{PDRY}_{1979}\\), \\(\\textsf{YAR}_{1979}\\)\nDifferent tokens (Think NLP: \"Congo\", \"DRC\", \"Republic of Congo\") can then be contextualized: can ‚Äútrack‚Äù and link data appropriately despite splits, merges, name changes\nSay we have data on ‚ÄúNumber of Communist Militants in \\(X\\)‚Äù (Hoover Yearbook)‚Ä¶\n\n\n\n\nEntity\nData from 1947-1971 at...\nData from 1971-Present at...\n\n\n\n\n\\(\\textsf{Pakistan}_{1979}\\)\nNational Level:\n\\(\\frac{62}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúPakistan‚Äù\n\n\nSubnational Level:\n‚ÄúWest Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)\n\n\n\\(\\textsf{Bangladesh}_{1979}\\)\nNational Level:\n\\(\\frac{70}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúBangladesh‚Äù\n\n\nSubnational Level:\n‚ÄúEast Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)\n\n\n\nHelpful metaphor (Gleijeses 2013): Cuba \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for USSR (Soviet $ but Cuban training of PAIGC ‚Üí MPLA), as Israel \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for US (US $ but Israeli training of SAVAK ‚Üí SADF)"
  },
  {
    "objectID": "w04/slides.html#the-fork-x-leftarrow-z-rightarrow-y",
    "href": "w04/slides.html#the-fork-x-leftarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)",
    "text": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\nfork_df &lt;- tibble(\n    Z = rbern(n_d),\n    X = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\nplot_freqs &lt;- function(df, plot_title, y_lab=TRUE) {\n  df_cor &lt;- cor(df$X, df$Y)\n  df_label &lt;- paste0(\"Cor(X,Y) = \",round(df_cor,3))\n  freq_df &lt;- df |&gt;\n    group_by(X, Y) |&gt;\n    summarize(count=n())\n  freq_plot &lt;- freq_df |&gt;\n    ggplot(\n      aes(x=factor(X), y=factor(Y), fill=count)\n    ) +\n    geom_tile() +\n    coord_equal() +\n    scale_fill_distiller(\n      palette=\"Greens\", direction=1,\n      limits=c(0,5000)\n    ) +\n    geom_label(\n      aes(label=count),\n      fill=\"white\", color=\"black\", size=7\n    ) +\n    labs(\n      title = plot_title,\n      subtitle = df_label,\n      x=\"X\", y=\"Y\"\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      plot.title = element_text(size=21),\n      plot.subtitle = element_text(size=18)\n    ) +\n    remove_legend()\n  if (!y_lab) {\n    freq_plot &lt;- freq_plot + theme(\n      axis.title.y = element_blank()\n    )\n  }\n  return(freq_plot)\n}\n# The full df\nfull_label &lt;- paste0(\"Raw Data (n = 10K)\")\nfull_plot &lt;- plot_freqs(fork_df, full_label)\n# Conditioning on Z = 0\nz0_df &lt;- fork_df |&gt; filter(Z == 0)\nz0_n &lt;- nrow(z0_df)\nz0_label &lt;- paste0(\"Z == 0 (\",z0_n,\" obs)\")\nz0_plot &lt;- plot_freqs(z0_df, z0_label, y_lab=FALSE)\n# Conditioning on Z = 1\nz1_df &lt;- fork_df |&gt; filter(Z == 1)\nz1_n &lt;- nrow(z1_df)\nz1_label &lt;- paste0(\"Z == 1 (\",z1_n,\" obs)\")\nz1_plot &lt;- plot_freqs(z1_df, z1_label, y_lab=FALSE)\nfull_plot | z0_plot | z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"$Slope_{Z=0} = \",z0_slope,\"$\")\nz0_leg_label &lt;- TeX(paste0(\"0 $(m=\",z0_slope,\")$\"))\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"$Slope_{Z=1} = \",z1_slope,\"$\")\nz_texlabel &lt;- TeX(paste0(z0_label, \" | \", z1_label))\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w04/slides.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "href": "w04/slides.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)",
    "text": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\npipe_df &lt;- tibble(\n    X = rbern(n_d),\n    Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\n# The full df\npipe_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\npipe_full_plot &lt;- plot_freqs(pipe_df, pipe_full_label)\n# Conditioning on Z = 0\npipe_z0_df &lt;- pipe_df |&gt; filter(Z == 0)\npipe_z0_n &lt;- nrow(pipe_z0_df)\npipe_z0_label &lt;- paste0(\"Z == 0 (\",pipe_z0_n,\" obs)\")\npipe_z0_plot &lt;- plot_freqs(pipe_z0_df, pipe_z0_label, y_lab=FALSE)\n# Conditioning on Z = 1\npipe_z1_df &lt;- pipe_df |&gt; filter(Z == 1)\npipe_z1_n &lt;- nrow(pipe_z1_df)\npipe_z1_label &lt;- paste0(\"Z == 1 (\",pipe_z1_n,\" obs)\")\npipe_z1_plot &lt;- plot_freqs(pipe_z1_df, pipe_z1_label, y_lab=FALSE)\npipe_full_plot | pipe_z0_plot | pipe_z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",cpipe_z0_slope,\"$\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",cpipe_z1_slope,\"$\")\ncpipe_z_texlabel &lt;- TeX(paste0(cpipe_z0_label, \" | \", cpipe_z1_label))\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    subtitle=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w04/slides.html#the-collider-x-rightarrow-z-leftarrow-y",
    "href": "w04/slides.html#the-collider-x-rightarrow-z-leftarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)",
    "text": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)\n\n\n\n\n\nCode\nset.seed(5650)\ncoll_df &lt;- tibble(\n    X = rbern(n_d),\n    Y = rbern(n_d),\n    Z = rbern(n_d, ifelse(X + Y &gt; 0, 0.9, 0.2)),\n)\n\n\n\n\nCode\n# The full df\ncoll_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\ncoll_full_plot &lt;- plot_freqs(coll_df, coll_full_label)\n# Conditioning on Z = 0\ncoll_z0_df &lt;- coll_df |&gt; filter(Z == 0)\ncoll_z0_n &lt;- nrow(coll_z0_df)\ncoll_z0_label &lt;- paste0(\"Z == 0 (\",coll_z0_n,\" obs)\")\ncoll_z0_plot &lt;- plot_freqs(coll_z0_df, coll_z0_label, y_lab=FALSE)\n# Conditioning on Z = 1\ncoll_z1_df &lt;- coll_df |&gt; filter(Z == 1)\ncoll_z1_n &lt;- nrow(coll_z1_df)\ncoll_z1_label &lt;- paste0(\"Z == 1 (\",coll_z1_n,\" obs)\")\ncoll_z1_plot &lt;- plot_freqs(coll_z1_df, coll_z1_label, y_lab=FALSE)\ncoll_full_plot | coll_z0_plot | coll_z1_plot\n\n\n\n\n\n\n\n\n\n\nConditioning on colliders induces correlation where there previously was none ‚ò†Ô∏è\n\n\n\n\n\nCode\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\n\n\n\n\nCode\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",ccoll_z0_slope,\"$\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",ccoll_z1_slope,\"$\")\nccoll_z_texlabel &lt;- TeX(paste0(ccoll_z0_label, \" | \", ccoll_z1_label))\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",ccoll_slope\n    ),\n    subtitle=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶This is why we have to think, rather than just ‚Äúcontrol for everything‚Äù! üò≠"
  },
  {
    "objectID": "w04/slides.html#proxies-for-z",
    "href": "w04/slides.html#proxies-for-z",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Proxies for \\(Z\\)",
    "text": "Proxies for \\(Z\\)\n\n\n\n\n\nCode\nset.seed(5650)\nprox_df &lt;- tibble(\n  X = rbern(n_d),\n  Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n  Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n  A = rbern(n_d, (1-Z)*0.1 + Z*0.9)\n)\n\n\n\n\nCode\n# The full df\nprox_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\nprox_full_plot &lt;- plot_freqs(prox_df, prox_full_label)\n# Conditioning on A == 0\nprox_a0_df &lt;- prox_df |&gt; filter(A == 0)\nprox_a0_n &lt;- nrow(prox_a0_df)\nprox_a0_label &lt;- paste0(\"A == 0 (\",prox_a0_n,\" obs)\")\nprox_a0_plot &lt;- plot_freqs(prox_a0_df, prox_a0_label, y_lab=FALSE)\n# Conditioning on A == 1\nprox_a1_df &lt;- prox_df |&gt; filter(A == 1)\nprox_a1_n &lt;- nrow(prox_a1_df)\nprox_a1_label &lt;- paste0(\"A == 1 (\",prox_a1_n,\" obs)\")\nprox_a1_plot &lt;- plot_freqs(prox_a1_df, prox_a1_label, y_lab=FALSE)\nprox_full_plot | prox_a0_plot | prox_a1_plot\n\n\n\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A\\) gives us some (not all!) information about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\nn_c &lt;- 300\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"$Slope_{A=0} = \",cprox_a0_slope,\"$\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"$Slope_{A=1} = \",cprox_a1_slope,\"$\")\ncprox_a_texlabel &lt;- TeX(paste0(cprox_a0_label, \" | \", cprox_a1_label))\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_text(size=18)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )"
  },
  {
    "objectID": "w04/slides.html#references",
    "href": "w04/slides.html#references",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "References",
    "text": "References\n\n\nBezuidenhout, Dana, Sarah Forthal, Kara Rudolph, and Matthew R Lamb. 2024. ‚ÄúSingle World Intervention Graphs (SWIGs): A Practical Guide.‚Äù American Journal of Epidemiology, September, kwae353. https://doi.org/10.1093/aje/kwae353.\n\n\nGleijeses, Piero. 2013. Visions of Freedom: Havana, Washington, Pretoria, and the Struggle for Southern Africa, 1976-1991. UNC Press Books.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and STAN. CRC Press."
  },
  {
    "objectID": "w05/index.html",
    "href": "w05/index.html",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#pooling-none-full-and-adaptive",
    "href": "w05/index.html#pooling-none-full-and-adaptive",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pooling: None, Full, and Adaptive",
    "text": "Pooling: None, Full, and Adaptive\n\n\n\nFrom Gelman and Hill (2007)\n\n\n\\[\n\\hat{\\alpha}_j^{\\text {multilevel }} = \\frac{\\frac{n_j}{\\sigma_y^2} \\bar{y}_j+\\frac{1}{\\sigma_\\alpha^2} \\bar{y}_{\\text {all }}}{\\frac{n_j}{\\sigma_y^2}+\\frac{1}{\\sigma_\\alpha^2}}\n\\]",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#why-adaptive-none-or-full",
    "href": "w05/index.html#why-adaptive-none-or-full",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Why Adaptive >> None or Full?",
    "text": "Why Adaptive &gt;&gt; None or Full?\n\n\n\n\n\n\\[\n\\alpha_j \\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha), \\text{ for }j = 1, \\ldots, J\n\\]\n\nIn the limit of \\(\\sigma_\\alpha \\rightarrow \\infty\\), the soft constraints do nothing, and there is no pooling;\nAs \\(\\sigma_\\alpha \\rightarrow 0\\), they pull the estimates all the way to zero, yielding the complete-pooling estimate",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/index.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag_closed, Z=\"Z\", lwd=5)\nadj_sets_closed &lt;- adjustmentSets(\n    pipe_dag_closed\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_closed\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/index.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#conditioning-on-a-proxy-for-z",
    "href": "w05/index.html#conditioning-on-a-proxy-for-z",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Conditioning on a Proxy for \\(Z\\)",
    "text": "Conditioning on a Proxy for \\(Z\\)\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A \\Rightarrow\\) some (not all!) info about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cprox_a0_slope,\"&lt;/span&gt;\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cprox_a1_slope,\"&lt;/span&gt;\")\ncprox_a_texlabel &lt;- paste0(cprox_a0_label, \" | \", cprox_a1_label)\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.5*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_markdown(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "href": "w05/index.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening",
    "text": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\ncoll_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(coll_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(coll_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(coll_dag, lwd=5)\nadj_sets_coll &lt;- adjustmentSets(\n    coll_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_coll\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#is-this-just-a-corner-case",
    "href": "w05/index.html#is-this-just-a-corner-case",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Is This Just a Corner Case?",
    "text": "Is This Just a Corner Case?\n\nGrant applications are regularly judged on two criteria: novelty and rigor\nJudges grade proposal separately on the two criteria, grant is awarded to top \\(N\\) applications based on combined scores‚Ä¶\n\n\nCode\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = runif(n_c, 0, 10),\n    rigorous = runif(n_c, 0, 10),\n    awarded = ifelse(newsworthy + rigorous &gt; 10, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = rnorm(n_c, 5, 1),\n    rigorous = rnorm(n_c, 5, 1),\n    awarded = ifelse(newsworthy + rigorous &gt; 12, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#blocking-backdoor-paths",
    "href": "w05/index.html#blocking-backdoor-paths",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Blocking Backdoor Paths",
    "text": "Blocking Backdoor Paths\ndagitty: R interface to dagitty.net\n\n\nCode\nrt_dag &lt;- dagitty(\"dag{\nX [exposure]\nY [outcome]\nU [unobserved]\nX -&gt; Y\nX &lt;- U &lt;- A -&gt; C -&gt; Y\nU -&gt; B &lt;- C\n}\")\ncoordinates(rt_dag) &lt;- list(\n    x=c(U=0, X=0, A=0.5, B=0.5, C=1, Y=1),\n    y=c(X=0.75, Y=0.75, B=0.5, U=0.25, C=0.25, A=0)\n)\ndrawdag(rt_dag, cex=2, lwd=4, radius=6)\n\n\n\n\n\n\n\n\n\n\n\nTwo backdoor paths!\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\leftarrow A \\rightarrow C \\rightarrow Y\\): Open or closed?\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\rightarrow B \\leftarrow C \\rightarrow Y\\): Open or closed?\n\n\n\nCode\nadjustmentSets(rt_dag)\n\n\n{ C }\n{ A }",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#references",
    "href": "w05/index.html#references",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. https://www.dropbox.com/scl/fi/asbumi3g0gqa4xl9va7wp/Andrew-Gelman-Jennifer-Hill-Data-Analysis-Using-Regression-and-Multilevel_Hierarchical-Models.pdf?rlkey=zf8icjhm7rswvxrpm7d10m65o&dl=1.",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/slides.html#pooling-none-full-and-adaptive",
    "href": "w05/slides.html#pooling-none-full-and-adaptive",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pooling: None, Full, and Adaptive",
    "text": "Pooling: None, Full, and Adaptive\n\nFrom Gelman and Hill (2007)\\[\n\\hat{\\alpha}_j^{\\text {multilevel }} = \\frac{\\frac{n_j}{\\sigma_y^2} \\bar{y}_j+\\frac{1}{\\sigma_\\alpha^2} \\bar{y}_{\\text {all }}}{\\frac{n_j}{\\sigma_y^2}+\\frac{1}{\\sigma_\\alpha^2}}\n\\]"
  },
  {
    "objectID": "w05/slides.html#why-adaptive-none-or-full",
    "href": "w05/slides.html#why-adaptive-none-or-full",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Why Adaptive >> None or Full?",
    "text": "Why Adaptive &gt;&gt; None or Full?\n\n\\[\n\\alpha_j \\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha), \\text{ for }j = 1, \\ldots, J\n\\]\n\nIn the limit of \\(\\sigma_\\alpha \\rightarrow \\infty\\), the soft constraints do nothing, and there is no pooling;\nAs \\(\\sigma_\\alpha \\rightarrow 0\\), they pull the estimates all the way to zero, yielding the complete-pooling estimate"
  },
  {
    "objectID": "w05/slides.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/slides.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag_closed, Z=\"Z\", lwd=5)\nadj_sets_closed &lt;- adjustmentSets(\n    pipe_dag_closed\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_closed\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}"
  },
  {
    "objectID": "w05/slides.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/slides.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w05/slides.html#conditioning-on-a-proxy-for-z",
    "href": "w05/slides.html#conditioning-on-a-proxy-for-z",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Conditioning on a Proxy for \\(Z\\)",
    "text": "Conditioning on a Proxy for \\(Z\\)\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A \\Rightarrow\\) some (not all!) info about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cprox_a0_slope,\"&lt;/span&gt;\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cprox_a1_slope,\"&lt;/span&gt;\")\ncprox_a_texlabel &lt;- paste0(cprox_a0_label, \" | \", cprox_a1_label)\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.5*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_markdown(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )"
  },
  {
    "objectID": "w05/slides.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "href": "w05/slides.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening",
    "text": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\ncoll_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(coll_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(coll_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(coll_dag, lwd=5)\nadj_sets_coll &lt;- adjustmentSets(\n    coll_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_coll\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w05/slides.html#is-this-just-a-corner-case",
    "href": "w05/slides.html#is-this-just-a-corner-case",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Is This Just a Corner Case?",
    "text": "Is This Just a Corner Case?\n\nGrant applications are regularly judged on two criteria: novelty and rigor\nJudges grade proposal separately on the two criteria, grant is awarded to top \\(N\\) applications based on combined scores‚Ä¶\n\n\nCode\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = runif(n_c, 0, 10),\n    rigorous = runif(n_c, 0, 10),\n    awarded = ifelse(newsworthy + rigorous &gt; 10, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = rnorm(n_c, 5, 1),\n    rigorous = rnorm(n_c, 5, 1),\n    awarded = ifelse(newsworthy + rigorous &gt; 12, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )"
  },
  {
    "objectID": "w05/slides.html#blocking-backdoor-paths",
    "href": "w05/slides.html#blocking-backdoor-paths",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Blocking Backdoor Paths",
    "text": "Blocking Backdoor Paths\ndagitty: R interface to dagitty.net\n\n\n\nCode\nrt_dag &lt;- dagitty(\"dag{\nX [exposure]\nY [outcome]\nU [unobserved]\nX -&gt; Y\nX &lt;- U &lt;- A -&gt; C -&gt; Y\nU -&gt; B &lt;- C\n}\")\ncoordinates(rt_dag) &lt;- list(\n    x=c(U=0, X=0, A=0.5, B=0.5, C=1, Y=1),\n    y=c(X=0.75, Y=0.75, B=0.5, U=0.25, C=0.25, A=0)\n)\ndrawdag(rt_dag, cex=2, lwd=4, radius=6)\n\n\n\n\n\n\n\n\n\n\n\n\nTwo backdoor paths!\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\leftarrow A \\rightarrow C \\rightarrow Y\\): Open or closed?\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\rightarrow B \\leftarrow C \\rightarrow Y\\): Open or closed?\n\n\n\nCode\nadjustmentSets(rt_dag)\n\n\n{ C }\n{ A }"
  },
  {
    "objectID": "w05/slides.html#references",
    "href": "w05/slides.html#references",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. https://www.dropbox.com/scl/fi/asbumi3g0gqa4xl9va7wp/Andrew-Gelman-Jennifer-Hill-Data-Analysis-Using-Regression-and-Multilevel_Hierarchical-Models.pdf?rlkey=zf8icjhm7rswvxrpm7d10m65o&dl=1."
  },
  {
    "objectID": "writeups/index.html",
    "href": "writeups/index.html",
    "title": "Extra Writeups",
    "section": "",
    "text": "Order By\n       Default\n         \n          Last Updated - Oldest\n        \n         \n          Last Updated - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nLast Updated\n\n\nCategory\n\n\n\n\n\n\n[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure\n\n\nSaturday, June 28, 2025\n\n\nLabs\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Writeups"
    ]
  },
  {
    "objectID": "final.html",
    "href": "final.html",
    "title": "Final Project Specifications",
    "section": "",
    "text": "Our goal is to make the final project as open-ended as possible, to give you the space to explore any particular topic that may have piqued your interest throughout the semester! At the same time, we hope to provide you with guidance and mentorship so that you don‚Äôt feel lost as to how to start, how to proceed, and/or what to submit for the final deliverable!1"
  },
  {
    "objectID": "final.html#overview",
    "href": "final.html#overview",
    "title": "Final Project Specifications",
    "section": "",
    "text": "Our goal is to make the final project as open-ended as possible, to give you the space to explore any particular topic that may have piqued your interest throughout the semester! At the same time, we hope to provide you with guidance and mentorship so that you don‚Äôt feel lost as to how to start, how to proceed, and/or what to submit for the final deliverable!1"
  },
  {
    "objectID": "final.html#references",
    "href": "final.html#references",
    "title": "Final Project Specifications",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "final.html#footnotes",
    "href": "final.html#footnotes",
    "title": "Final Project Specifications",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you can‚Äôt tell, my whole educational philosophy here is just the Montessori system‚Äîthis approach was originally developed for younger (primary school) children, but lots and lots of recent educational research indicates that it‚Äôs an actually an extremely effective way to learn, and to motivate self-learning, for people of any age üòé‚Ü©Ô∏é"
  },
  {
    "objectID": "w07/index.html",
    "href": "w07/index.html",
    "title": "Week 7: Midterm Introduction",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#from-hws-to-midterm",
    "href": "w07/index.html#from-hws-to-midterm",
    "title": "Week 7: Midterm Introduction",
    "section": "From HWs to Midterm",
    "text": "From HWs to Midterm\n\nHWs: More focus on social science / more ‚Äúin the weeds‚Äù (checking details, manipulating prior parameters, debugging, etc.)\nMidterm: More focus on causality, bc, in 2 hours I can test you on concepts and things like ‚ÄúWhat are the backdoor paths here? How would you close them?‚Äù more easily than on loading datasets, cleaning, fitting models, etc.",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#looking-forwards-post-midterm",
    "href": "w07/index.html#looking-forwards-post-midterm",
    "title": "Week 7: Midterm Introduction",
    "section": "Looking Forwards (Post-Midterm)",
    "text": "Looking Forwards (Post-Midterm)\n\nMore text analysis!\nMaking the connection between modeling and predicting",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#modeling-how-trees-become-forests",
    "href": "w07/index.html#modeling-how-trees-become-forests",
    "title": "Week 7: Midterm Introduction",
    "section": "Modeling How Trees Become Forests",
    "text": "Modeling How Trees Become Forests\n\nYour model, if it‚Äôs generative (which‚Ä¶ nearly all in 5650 are), relates observed data \\(\\mathbf{D}\\) to a set of underlying parameters \\(\\boldsymbol{\\theta}\\) that are hypothesized as ‚Äúgiving rise‚Äù to \\(\\mathbf{D}\\)\nWhen you specify how exactly this ‚Äúgiving rise‚Äù works, you‚Äôre specifying a DGP!\nPGMs: human-brain-friendly (bc graphical) language for writing DGPs; then move to PyAgrum \\(\\rightarrow\\) Stan \\(\\rightarrow\\) PyMC to ‚Äúencode‚Äù PGMs (in 0100101) so computer can‚Ä¶\n\n\n\n\n Super-charge your EDA/modeling\n Estimate \\(\\boldsymbol{\\theta}\\) from data\n\n\n\n\n\\(\\leadsto\\) Prior distributions\n\\(\\leadsto\\) Posterior distributions\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggExtra)\ngen_walk_plot &lt;- function(walk_data, a=0.0075) {\n  # print(end_df)\n  grid_color &lt;- rgb(0, 0, 0, 0.1)\n  # And plot!\n  walkplot &lt;- ggplot() +\n    geom_line(\n      data = walk_data$long_df,\n      aes(x = t, y = pos, group = pid),\n      linewidth = g_linewidth,\n      alpha = a,\n      #color = cb_palette[2]\n      #color = \"#cf8f00\"\n      color = \"black\"\n    ) +\n    geom_point(\n      data = walk_data$end_df,\n      aes(x = t, y = endpos),\n      alpha = 0\n    ) +\n    scale_x_continuous(\n      breaks = seq(\n        0,\n        walk_data$num_steps,\n        walk_data$num_steps / 4\n      )\n    ) +\n    scale_y_continuous(\n      breaks = seq(-20, 20, 10)\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      legend.position = \"none\",\n      # title = element_text(size = 16)\n    ) +\n    theme(\n      panel.grid.major.y = element_line(\n        color = grid_color,\n        linewidth = 1,\n        linetype = 1\n      )\n    ) +\n    labs(\n      title = paste0(\n        walk_data$num_people, \" Random Walks, \",\n        walk_data$num_steps, \" Steps\"\n      ),\n      x = \"Number of Steps\",\n      y = \"Position\"\n    )\n}\nwalk_data &lt;- readRDS(\"assets/walk_data.rds\")\n# 16 steps\n# wp1 &lt;- gen_walkplot(500, 16, 0.05)\n# ggMarginal(wp1, margins = \"y\", type = \"histogram\", yparams = list(binwidth = 1))\nwp &lt;- gen_walk_plot(walk_data) + ylim(-30,30)\nggMarginal(\n  wp, margins = \"y\",\n  type = \"histogram\",\n  yparams = list(binwidth = 1)\n)",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#prior-stage-distributions",
    "href": "w07/index.html#prior-stage-distributions",
    "title": "Week 7: Midterm Introduction",
    "section": "Prior ‚ÄúStage‚Äù Distributions",
    "text": "Prior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\nEnter Prior World (Before observing any data): Since we have priors over \\(\\boldsymbol\\theta\\) \\(\\leadsto\\) we can sample values of \\(\\boldsymbol\\theta\\), then generate synthetic data on the basis of these values\n\n\n\n\n\n Prior Distribution: \\(\\Pr(\\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat can I guess about values of my parameters from background knowledge of the world? e.g.:\n\nHuman heights can‚Äôt be negative\nData collected at a bar \\(\\Rightarrow\\) Age \\(\\geq\\) 18, \\(\\Pr(\\text{Age} = x)\\) decreases as \\(x\\) goes above 30\n\n\n\n\n\n\n\n\n Prior Predictive Distribution:  \\(\\Pr(\\mathbf{X}^{‚ùì} \\mid \\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat could the outcomes look like if I ran my guesses through the DGP?\n\n100 simulated heights, none are negative\n1K sim bar-goers; 80% have this haircut \\(\\rightarrow\\)",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#posterior-stage-distributions",
    "href": "w07/index.html#posterior-stage-distributions",
    "title": "Week 7: Midterm Introduction",
    "section": "Posterior ‚ÄúStage‚Äù Distributions",
    "text": "Posterior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\n\n\n\n\n Posterior Distribution: \\(\\Pr\\left(\\boldsymbol{\\theta} \\; \\middle| \\; \\mathbf{X} = \\ponode{\\mathbf{X}}\\right)\\)\n\nNow we observe data: \\(\\pnode{\\mathbf{X}} \\leadsto \\ponode{\\mathbf{X}}\\), which means we can use Bayes‚Äô Rule to infer distribution over \\(\\boldsymbol{\\theta}\\): what values are most likely to produce \\(\\ponode{\\mathbf{X}}\\)?\n\n60% of observed bar-goers have that haircut \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta}_{\\text{post}} \\approx \\frac{0.6 + 0.8}{2} = 0.7\\)\nCoin = \\(\\textsf{H}\\) 4/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 0.8}{2} = 0.65\\)\nCoin = \\(\\textsf{H}\\) 5/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 1.0}{2} = 0.75\\)\n\n\n\n\n\n\n\n\n Posterior Predictive Distribution: \\(\\Pr(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)\n\nNow that we‚Äôve fit \\(\\Pr(\\boldsymbol{\\theta})\\) to data, can generate as much new data as we want, e.g.¬†to evaluate how well model predicts outcomes for test data\n\n\n\nFrom PyMC documentation",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#why-do-we-need-subjective-priors",
    "href": "w07/index.html#why-do-we-need-subjective-priors",
    "title": "Week 7: Midterm Introduction",
    "section": "Why Do We Need ‚ÄúSubjective‚Äù Priors?",
    "text": "Why Do We Need ‚ÄúSubjective‚Äù Priors?\n\nUnder frequentism‚Ä¶ (tldr) literally no method for dealing with shades of uncertainty\nFrequentist assumption: For a given coin, \\(\\Pr(\\textsf{Heads})\\) is not a Random Variable! It‚Äôs some number, like 0.5 or 0.8. It‚Äôs the asymptote of \\(\\frac{\\#[\\textsf{Heads}]}{\\#[\\textsf{Heads}] + \\#[\\textsf{Tails}]}\\) as \\(n \\rightarrow \\infty\\)\nSo, if we flip coin 1 time, and get \\(\\textsf{Heads}\\), no basis for inferring \\(\\Pr(\\textsf{Coin is Fair})\\) vs.¬†\\(\\Pr(\\textsf{Coin is Biased})\\): Both are undefined. We‚Äôve‚Ä¶ ‚Äúdiscovered‚Äù that \\(\\Pr(\\textsf{Heads}) = 1\\)\nBy using Bayesian inference, we can bring prior knowledge into our studies, which we‚Äôll need to do especially for complex emergent systems: societies, economies, etc.!\nIn fact, as a guy named Laplace discovered in the nineteenth century, frequentism = Bayesian inference with ‚Äúflat priors‚Äù‚Ä¶",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#flat-vs.-informative-priors",
    "href": "w07/index.html#flat-vs.-informative-priors",
    "title": "Week 7: Midterm Introduction",
    "section": "Flat vs.¬†Informative Priors",
    "text": "Flat vs.¬†Informative Priors\n\nCode\nlibrary(tidyverse)\nflat_df &lt;- tibble(x=seq(0, 1, 0.1), y=0)\nflat_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=cb_palette[1],\n    linewidth=g_linewidth\n  ) +\n  ylim(0, 1) +\n  labs(\n    title=\"Flat Prior on Pr(Heads)\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Observed Data\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\nlibrary(latex2exp)\nw_label &lt;- TeX(\"Width = $1/n$\")\nh_label &lt;- TeX(\"Height = $n$\")\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth,\n    color=cb_palette[1], arrow=arrow()\n  ) +\n  geom_segment(\n    x=0, y=0, xend=1, linewidth=g_linewidth,\n    color=cb_palette[1]\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Posterior of Pr(Heads)\",\n    y = \"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20)) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.8, \n    label = w_label, hjust = 0, vjust = 1, size = 8\n  ) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.7, \n    label = h_label, hjust = 0, vjust = 1, size = 8\n  )\n\n\n\n\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî lubridate 1.9.4     ‚úî tibble    3.3.0\n‚úî purrr     1.0.4     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]\n\n\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nunif_df &lt;- tibble(x=seq(0, 1, 0.1), y=1)\nunif_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=\"#e69f00\", linewidth=g_linewidth\n  ) +\n  annotate('rect', xmin=0, xmax=1, ymin=0, ymax=1, fill='#e69f00', alpha=0.3) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(title=\"Uniform Prior on Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Observed Data\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\nx_vals &lt;- seq(0, 1, 0.01)\nmy_exp &lt;- function(x) exp(1-1/(x^2))\ny_vals &lt;- sapply(x_vals, my_exp)\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\nrib_df &lt;- tibble(x=x_vals, ymax=y_vals, ymin=0)\nggplot() +\n  # stat_function(fun=my_exp, linewidth=g_linewidth, color=cb_palette[1]) +\n  geom_line(\n    data=data_df,\n    aes(x=x, y=y),\n    linewidth=g_linewidth, color=cb_palette[1]\n  ) +\n  geom_ribbon(\n    data=rib_df,\n    aes(x=x, ymin=ymin, ymax=ymax),\n    fill=cb_palette[1], alpha=0.3\n  ) +\n  # geom_segment(\n  #   x=1, y=0, yend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1], arrow=arrow()\n  # ) +\n  # geom_segment(\n  #   x=0, y=0, xend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1]\n  # ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Posterior of Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#references",
    "href": "w07/index.html#references",
    "title": "Week 7: Midterm Introduction",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/slides.html#from-hws-to-midterm",
    "href": "w07/slides.html#from-hws-to-midterm",
    "title": "Week 7: Midterm Introduction",
    "section": "From HWs to Midterm",
    "text": "From HWs to Midterm\n\nHWs: More focus on social science / more ‚Äúin the weeds‚Äù (checking details, manipulating prior parameters, debugging, etc.)\nMidterm: More focus on causality, bc, in 2 hours I can test you on concepts and things like ‚ÄúWhat are the backdoor paths here? How would you close them?‚Äù more easily than on loading datasets, cleaning, fitting models, etc."
  },
  {
    "objectID": "w07/slides.html#looking-forwards-post-midterm",
    "href": "w07/slides.html#looking-forwards-post-midterm",
    "title": "Week 7: Midterm Introduction",
    "section": "Looking Forwards (Post-Midterm)",
    "text": "Looking Forwards (Post-Midterm)\n\nMore text analysis!\nMaking the connection between modeling and predicting"
  },
  {
    "objectID": "w07/slides.html#modeling-how-trees-become-forests",
    "href": "w07/slides.html#modeling-how-trees-become-forests",
    "title": "Week 7: Midterm Introduction",
    "section": "Modeling How Trees Become Forests",
    "text": "Modeling How Trees Become Forests\n\nYour model, if it‚Äôs generative (which‚Ä¶ nearly all in 5650 are), relates observed data \\(\\mathbf{D}\\) to a set of underlying parameters \\(\\boldsymbol{\\theta}\\) that are hypothesized as ‚Äúgiving rise‚Äù to \\(\\mathbf{D}\\)\nWhen you specify how exactly this ‚Äúgiving rise‚Äù works, you‚Äôre specifying a DGP!\nPGMs: human-brain-friendly (bc graphical) language for writing DGPs; then move to PyAgrum \\(\\rightarrow\\) Stan \\(\\rightarrow\\) PyMC to ‚Äúencode‚Äù PGMs (in 0100101) so computer can‚Ä¶\n\n\n\n\n Super-charge your EDA/modeling\n Estimate \\(\\boldsymbol{\\theta}\\) from data\n\n\n\n\n\\(\\leadsto\\) Prior distributions\n\\(\\leadsto\\) Posterior distributions\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggExtra)\ngen_walk_plot &lt;- function(walk_data, a=0.0075) {\n  # print(end_df)\n  grid_color &lt;- rgb(0, 0, 0, 0.1)\n  # And plot!\n  walkplot &lt;- ggplot() +\n    geom_line(\n      data = walk_data$long_df,\n      aes(x = t, y = pos, group = pid),\n      linewidth = g_linewidth,\n      alpha = a,\n      #color = cb_palette[2]\n      #color = \"#cf8f00\"\n      color = \"black\"\n    ) +\n    geom_point(\n      data = walk_data$end_df,\n      aes(x = t, y = endpos),\n      alpha = 0\n    ) +\n    scale_x_continuous(\n      breaks = seq(\n        0,\n        walk_data$num_steps,\n        walk_data$num_steps / 4\n      )\n    ) +\n    scale_y_continuous(\n      breaks = seq(-20, 20, 10)\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      legend.position = \"none\",\n      # title = element_text(size = 16)\n    ) +\n    theme(\n      panel.grid.major.y = element_line(\n        color = grid_color,\n        linewidth = 1,\n        linetype = 1\n      )\n    ) +\n    labs(\n      title = paste0(\n        walk_data$num_people, \" Random Walks, \",\n        walk_data$num_steps, \" Steps\"\n      ),\n      x = \"Number of Steps\",\n      y = \"Position\"\n    )\n}\nwalk_data &lt;- readRDS(\"assets/walk_data.rds\")\n# 16 steps\n# wp1 &lt;- gen_walkplot(500, 16, 0.05)\n# ggMarginal(wp1, margins = \"y\", type = \"histogram\", yparams = list(binwidth = 1))\nwp &lt;- gen_walk_plot(walk_data) + ylim(-30,30)\nggMarginal(\n  wp, margins = \"y\",\n  type = \"histogram\",\n  yparams = list(binwidth = 1)\n)"
  },
  {
    "objectID": "w07/slides.html#prior-stage-distributions",
    "href": "w07/slides.html#prior-stage-distributions",
    "title": "Week 7: Midterm Introduction",
    "section": "Prior ‚ÄúStage‚Äù Distributions",
    "text": "Prior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\nEnter Prior World (Before observing any data): Since we have priors over \\(\\boldsymbol\\theta\\) \\(\\leadsto\\) we can sample values of \\(\\boldsymbol\\theta\\), then generate synthetic data on the basis of these values\n\n\n\n\n\n Prior Distribution: \\(\\Pr(\\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat can I guess about values of my parameters from background knowledge of the world? e.g.:\n\nHuman heights can‚Äôt be negative\nData collected at a bar \\(\\Rightarrow\\) Age \\(\\geq\\) 18, \\(\\Pr(\\text{Age} = x)\\) decreases as \\(x\\) goes above 30\n\n\n\n\n\n\n\n\n Prior Predictive Distribution:  \\(\\Pr(\\mathbf{X}^{‚ùì} \\mid \\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat could the outcomes look like if I ran my guesses through the DGP?\n\n100 simulated heights, none are negative\n1K sim bar-goers; 80% have this haircut \\(\\rightarrow\\)"
  },
  {
    "objectID": "w07/slides.html#posterior-stage-distributions",
    "href": "w07/slides.html#posterior-stage-distributions",
    "title": "Week 7: Midterm Introduction",
    "section": "Posterior ‚ÄúStage‚Äù Distributions",
    "text": "Posterior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\n\n\n\n\n Posterior Distribution: \\(\\Pr\\left(\\boldsymbol{\\theta} \\; \\middle| \\; \\mathbf{X} = \\ponode{\\mathbf{X}}\\right)\\)\n\nNow we observe data: \\(\\pnode{\\mathbf{X}} \\leadsto \\ponode{\\mathbf{X}}\\), which means we can use Bayes‚Äô Rule to infer distribution over \\(\\boldsymbol{\\theta}\\): what values are most likely to produce \\(\\ponode{\\mathbf{X}}\\)?\n\n60% of observed bar-goers have that haircut \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta}_{\\text{post}} \\approx \\frac{0.6 + 0.8}{2} = 0.7\\)\nCoin = \\(\\textsf{H}\\) 4/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 0.8}{2} = 0.65\\)\nCoin = \\(\\textsf{H}\\) 5/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 1.0}{2} = 0.75\\)\n\n\n\n\n\n\n\n\n Posterior Predictive Distribution: \\(\\Pr(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)\n\nNow that we‚Äôve fit \\(\\Pr(\\boldsymbol{\\theta})\\) to data, can generate as much new data as we want, e.g.¬†to evaluate how well model predicts outcomes for test data\n\n\n\nFrom PyMC documentation"
  },
  {
    "objectID": "w07/slides.html#why-do-we-need-subjective-priors",
    "href": "w07/slides.html#why-do-we-need-subjective-priors",
    "title": "Week 7: Midterm Introduction",
    "section": "Why Do We Need ‚ÄúSubjective‚Äù Priors?",
    "text": "Why Do We Need ‚ÄúSubjective‚Äù Priors?\n\nUnder frequentism‚Ä¶ (tldr) literally no method for dealing with shades of uncertainty\nFrequentist assumption: For a given coin, \\(\\Pr(\\textsf{Heads})\\) is not a Random Variable! It‚Äôs some number, like 0.5 or 0.8. It‚Äôs the asymptote of \\(\\frac{\\#[\\textsf{Heads}]}{\\#[\\textsf{Heads}] + \\#[\\textsf{Tails}]}\\) as \\(n \\rightarrow \\infty\\)\nSo, if we flip coin 1 time, and get \\(\\textsf{Heads}\\), no basis for inferring \\(\\Pr(\\textsf{Coin is Fair})\\) vs.¬†\\(\\Pr(\\textsf{Coin is Biased})\\): Both are undefined. We‚Äôve‚Ä¶ ‚Äúdiscovered‚Äù that \\(\\Pr(\\textsf{Heads}) = 1\\)\nBy using Bayesian inference, we can bring prior knowledge into our studies, which we‚Äôll need to do especially for complex emergent systems: societies, economies, etc.!\nIn fact, as a guy named Laplace discovered in the nineteenth century, frequentism = Bayesian inference with ‚Äúflat priors‚Äù‚Ä¶"
  },
  {
    "objectID": "w07/slides.html#flat-vs.-informative-priors",
    "href": "w07/slides.html#flat-vs.-informative-priors",
    "title": "Week 7: Midterm Introduction",
    "section": "Flat vs.¬†Informative Priors",
    "text": "Flat vs.¬†Informative Priors\n\nCode\nlibrary(tidyverse)\nflat_df &lt;- tibble(x=seq(0, 1, 0.1), y=0)\nflat_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=cb_palette[1],\n    linewidth=g_linewidth\n  ) +\n  ylim(0, 1) +\n  labs(\n    title=\"Flat Prior on Pr(Heads)\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Observed Data\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\nlibrary(latex2exp)\nw_label &lt;- TeX(\"Width = $1/n$\")\nh_label &lt;- TeX(\"Height = $n$\")\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth,\n    color=cb_palette[1], arrow=arrow()\n  ) +\n  geom_segment(\n    x=0, y=0, xend=1, linewidth=g_linewidth,\n    color=cb_palette[1]\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Posterior of Pr(Heads)\",\n    y = \"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20)) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.8, \n    label = w_label, hjust = 0, vjust = 1, size = 8\n  ) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.7, \n    label = h_label, hjust = 0, vjust = 1, size = 8\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nunif_df &lt;- tibble(x=seq(0, 1, 0.1), y=1)\nunif_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=\"#e69f00\", linewidth=g_linewidth\n  ) +\n  annotate('rect', xmin=0, xmax=1, ymin=0, ymax=1, fill='#e69f00', alpha=0.3) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(title=\"Uniform Prior on Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Observed Data\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\nx_vals &lt;- seq(0, 1, 0.01)\nmy_exp &lt;- function(x) exp(1-1/(x^2))\ny_vals &lt;- sapply(x_vals, my_exp)\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\nrib_df &lt;- tibble(x=x_vals, ymax=y_vals, ymin=0)\nggplot() +\n  # stat_function(fun=my_exp, linewidth=g_linewidth, color=cb_palette[1]) +\n  geom_line(\n    data=data_df,\n    aes(x=x, y=y),\n    linewidth=g_linewidth, color=cb_palette[1]\n  ) +\n  geom_ribbon(\n    data=rib_df,\n    aes(x=x, ymin=ymin, ymax=ymax),\n    fill=cb_palette[1], alpha=0.3\n  ) +\n  # geom_segment(\n  #   x=1, y=0, yend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1], arrow=arrow()\n  # ) +\n  # geom_segment(\n  #   x=0, y=0, xend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1]\n  # ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Posterior of Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]"
  },
  {
    "objectID": "w07/slides.html#references",
    "href": "w07/slides.html#references",
    "title": "Week 7: Midterm Introduction",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "w06/index.html",
    "href": "w06/index.html",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#from-hws-to-midterm",
    "href": "w06/index.html#from-hws-to-midterm",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "From HWs to Midterm",
    "text": "From HWs to Midterm\n\nHWs: More focus on social science / more ‚Äúin the weeds‚Äù (checking details, manipulating prior parameters, debugging, etc.)\nMidterm: More focus on causality, bc, in 2 hours I can test you on concepts and things like ‚ÄúWhat are the backdoor paths here? How would you close them?‚Äù more easily than on loading datasets, cleaning, fitting models, etc.",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#looking-forwards-post-midterm",
    "href": "w06/index.html#looking-forwards-post-midterm",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Looking Forwards (Post-Midterm)",
    "text": "Looking Forwards (Post-Midterm)\n\nMore text analysis!\nMaking the connection between modeling and predicting",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#modeling-how-trees-become-forests",
    "href": "w06/index.html#modeling-how-trees-become-forests",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Modeling How Trees Become Forests",
    "text": "Modeling How Trees Become Forests\n\nYour model, if it‚Äôs generative (which‚Ä¶ nearly all in 5650 are), relates observed data \\(\\mathbf{D}\\) to a set of underlying parameters \\(\\boldsymbol{\\theta}\\) that are hypothesized as ‚Äúgiving rise‚Äù to \\(\\mathbf{D}\\)\nWhen you specify how exactly this ‚Äúgiving rise‚Äù works, you‚Äôre specifying a DGP!\nPGMs: human-brain-friendly (bc graphical) language for writing DGPs; then move to PyAgrum \\(\\rightarrow\\) Stan \\(\\rightarrow\\) PyMC to ‚Äúencode‚Äù PGMs (in 0100101) so computer can‚Ä¶\n\n\n\n\n Super-charge your EDA/modeling\n Estimate \\(\\boldsymbol{\\theta}\\) from data\n\n\n\n\n\\(\\leadsto\\) Prior distributions\n\\(\\leadsto\\) Posterior distributions\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggExtra)\ngen_walk_plot &lt;- function(walk_data, a=0.0075) {\n  # print(end_df)\n  grid_color &lt;- rgb(0, 0, 0, 0.1)\n  # And plot!\n  walkplot &lt;- ggplot() +\n    geom_line(\n      data = walk_data$long_df,\n      aes(x = t, y = pos, group = pid),\n      linewidth = g_linewidth,\n      alpha = a,\n      #color = cb_palette[2]\n      #color = \"#cf8f00\"\n      color = \"black\"\n    ) +\n    geom_point(\n      data = walk_data$end_df,\n      aes(x = t, y = endpos),\n      alpha = 0\n    ) +\n    scale_x_continuous(\n      breaks = seq(\n        0,\n        walk_data$num_steps,\n        walk_data$num_steps / 4\n      )\n    ) +\n    scale_y_continuous(\n      breaks = seq(-20, 20, 10)\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      legend.position = \"none\",\n      # title = element_text(size = 16)\n    ) +\n    theme(\n      panel.grid.major.y = element_line(\n        color = grid_color,\n        linewidth = 1,\n        linetype = 1\n      )\n    ) +\n    labs(\n      title = paste0(\n        walk_data$num_people, \" Random Walks, \",\n        walk_data$num_steps, \" Steps\"\n      ),\n      x = \"Number of Steps\",\n      y = \"Position\"\n    )\n}\nwalk_data &lt;- readRDS(\"assets/walk_data.rds\")\n# 16 steps\n# wp1 &lt;- gen_walkplot(500, 16, 0.05)\n# ggMarginal(wp1, margins = \"y\", type = \"histogram\", yparams = list(binwidth = 1))\nwp &lt;- gen_walk_plot(walk_data) + ylim(-30,30)\nggMarginal(\n  wp, margins = \"y\",\n  type = \"histogram\",\n  yparams = list(binwidth = 1)\n)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#prior-stage-distributions",
    "href": "w06/index.html#prior-stage-distributions",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Prior ‚ÄúStage‚Äù Distributions",
    "text": "Prior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\nEnter Prior World (Before observing any data): Since we have priors over \\(\\boldsymbol\\theta\\) \\(\\leadsto\\) we can sample values of \\(\\boldsymbol\\theta\\), then generate synthetic data on the basis of these values\n\n\n\n\n\n Prior Distribution: \\(\\Pr(\\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat can I guess about values of my parameters from background knowledge of the world? e.g.:\n\nHuman heights can‚Äôt be negative\nData collected at a bar \\(\\Rightarrow\\) Age \\(\\geq\\) 18, \\(\\Pr(\\text{Age} = x)\\) decreases as \\(x\\) goes above 30\n\n\n\n\n\n\n\n\n Prior Predictive Distribution:  \\(\\Pr(\\mathbf{X}^{‚ùì} \\mid \\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat could the outcomes look like if I ran my guesses through the DGP?\n\n100 simulated heights, none are negative\n1K sim bar-goers; 80% have this haircut \\(\\rightarrow\\)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#posterior-stage-distributions",
    "href": "w06/index.html#posterior-stage-distributions",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Posterior ‚ÄúStage‚Äù Distributions",
    "text": "Posterior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\n\n\n\n\n Posterior Distribution: \\(\\Pr\\left(\\boldsymbol{\\theta} \\; \\middle| \\; \\mathbf{X} = \\ponode{\\mathbf{X}}\\right)\\)\n\nNow we observe data: \\(\\pnode{\\mathbf{X}} \\leadsto \\ponode{\\mathbf{X}}\\), which means we can use Bayes‚Äô Rule to infer distribution over \\(\\boldsymbol{\\theta}\\): what values are most likely to produce \\(\\ponode{\\mathbf{X}}\\)?\n\n60% of observed bar-goers have that haircut \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta}_{\\text{post}} \\approx \\frac{0.6 + 0.8}{2} = 0.7\\)\nCoin = \\(\\textsf{H}\\) 4/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 0.8}{2} = 0.65\\)\nCoin = \\(\\textsf{H}\\) 5/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 1.0}{2} = 0.75\\)\n\n\n\n\n\n\n\n\n Posterior Predictive Distribution: \\(\\Pr(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)\n\nNow that we‚Äôve fit \\(\\Pr(\\boldsymbol{\\theta})\\) to data, can generate as much new data as we want, e.g.¬†to evaluate how well model predicts outcomes for test data\n\n\n\nFrom PyMC documentation",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#why-do-we-need-subjective-priors",
    "href": "w06/index.html#why-do-we-need-subjective-priors",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Why Do We Need ‚ÄúSubjective‚Äù Priors?",
    "text": "Why Do We Need ‚ÄúSubjective‚Äù Priors?\n\nUnder frequentism‚Ä¶ (tldr) literally no method for dealing with shades of uncertainty\nFrequentist assumption: For a given coin, \\(\\Pr(\\textsf{Heads})\\) is not a Random Variable! It‚Äôs some number, like 0.5 or 0.8. It‚Äôs the asymptote of \\(\\frac{\\#[\\textsf{Heads}]}{\\#[\\textsf{Heads}] + \\#[\\textsf{Tails}]}\\) as \\(n \\rightarrow \\infty\\)\nSo, if we flip coin 1 time, and get \\(\\textsf{Heads}\\), no basis for inferring \\(\\Pr(\\textsf{Coin is Fair})\\) vs.¬†\\(\\Pr(\\textsf{Coin is Biased})\\): Both are undefined. We‚Äôve‚Ä¶ ‚Äúdiscovered‚Äù that \\(\\Pr(\\textsf{Heads}) = 1\\)\nBy using Bayesian inference, we can bring prior knowledge into our studies, which we‚Äôll need to do especially for complex emergent systems: societies, economies, etc.!\nIn fact, as a guy named Laplace discovered in the nineteenth century, frequentism = Bayesian inference with ‚Äúflat priors‚Äù‚Ä¶",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#flat-vs.-informative-priors",
    "href": "w06/index.html#flat-vs.-informative-priors",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Flat vs.¬†Informative Priors",
    "text": "Flat vs.¬†Informative Priors\n\nCode\nlibrary(tidyverse)\nflat_df &lt;- tibble(x=seq(0, 1, 0.1), y=0)\nflat_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=cb_palette[1],\n    linewidth=g_linewidth\n  ) +\n  ylim(0, 1) +\n  labs(\n    title=\"Flat Prior on Pr(Heads)\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Observed Data\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\nlibrary(latex2exp)\nw_label &lt;- TeX(\"Width = $1/n$\")\nh_label &lt;- TeX(\"Height = $n$\")\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth,\n    color=cb_palette[1], arrow=arrow()\n  ) +\n  geom_segment(\n    x=0, y=0, xend=1, linewidth=g_linewidth,\n    color=cb_palette[1]\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Posterior of Pr(Heads)\",\n    y = \"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20)) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.8, \n    label = w_label, hjust = 0, vjust = 1, size = 8\n  ) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.7, \n    label = h_label, hjust = 0, vjust = 1, size = 8\n  )\n\n\n\n\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî lubridate 1.9.4     ‚úî tibble    3.3.0\n‚úî purrr     1.0.4     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]\n\n\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nunif_df &lt;- tibble(x=seq(0, 1, 0.1), y=1)\nunif_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=\"#e69f00\", linewidth=g_linewidth\n  ) +\n  annotate('rect', xmin=0, xmax=1, ymin=0, ymax=1, fill='#e69f00', alpha=0.3) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(title=\"Uniform Prior on Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Observed Data\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\nx_vals &lt;- seq(0, 1, 0.01)\nmy_exp &lt;- function(x) exp(1-1/(x^2))\ny_vals &lt;- sapply(x_vals, my_exp)\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\nrib_df &lt;- tibble(x=x_vals, ymax=y_vals, ymin=0)\nggplot() +\n  # stat_function(fun=my_exp, linewidth=g_linewidth, color=cb_palette[1]) +\n  geom_line(\n    data=data_df,\n    aes(x=x, y=y),\n    linewidth=g_linewidth, color=cb_palette[1]\n  ) +\n  geom_ribbon(\n    data=rib_df,\n    aes(x=x, ymin=ymin, ymax=ymax),\n    fill=cb_palette[1], alpha=0.3\n  ) +\n  # geom_segment(\n  #   x=1, y=0, yend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1], arrow=arrow()\n  # ) +\n  # geom_segment(\n  #   x=0, y=0, xend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1]\n  # ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Posterior of Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#references",
    "href": "w06/index.html#references",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/slides.html#from-hws-to-midterm",
    "href": "w06/slides.html#from-hws-to-midterm",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "From HWs to Midterm",
    "text": "From HWs to Midterm\n\nHWs: More focus on social science / more ‚Äúin the weeds‚Äù (checking details, manipulating prior parameters, debugging, etc.)\nMidterm: More focus on causality, bc, in 2 hours I can test you on concepts and things like ‚ÄúWhat are the backdoor paths here? How would you close them?‚Äù more easily than on loading datasets, cleaning, fitting models, etc."
  },
  {
    "objectID": "w06/slides.html#looking-forwards-post-midterm",
    "href": "w06/slides.html#looking-forwards-post-midterm",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Looking Forwards (Post-Midterm)",
    "text": "Looking Forwards (Post-Midterm)\n\nMore text analysis!\nMaking the connection between modeling and predicting"
  },
  {
    "objectID": "w06/slides.html#modeling-how-trees-become-forests",
    "href": "w06/slides.html#modeling-how-trees-become-forests",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Modeling How Trees Become Forests",
    "text": "Modeling How Trees Become Forests\n\nYour model, if it‚Äôs generative (which‚Ä¶ nearly all in 5650 are), relates observed data \\(\\mathbf{D}\\) to a set of underlying parameters \\(\\boldsymbol{\\theta}\\) that are hypothesized as ‚Äúgiving rise‚Äù to \\(\\mathbf{D}\\)\nWhen you specify how exactly this ‚Äúgiving rise‚Äù works, you‚Äôre specifying a DGP!\nPGMs: human-brain-friendly (bc graphical) language for writing DGPs; then move to PyAgrum \\(\\rightarrow\\) Stan \\(\\rightarrow\\) PyMC to ‚Äúencode‚Äù PGMs (in 0100101) so computer can‚Ä¶\n\n\n\n\n Super-charge your EDA/modeling\n Estimate \\(\\boldsymbol{\\theta}\\) from data\n\n\n\n\n\\(\\leadsto\\) Prior distributions\n\\(\\leadsto\\) Posterior distributions\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggExtra)\ngen_walk_plot &lt;- function(walk_data, a=0.0075) {\n  # print(end_df)\n  grid_color &lt;- rgb(0, 0, 0, 0.1)\n  # And plot!\n  walkplot &lt;- ggplot() +\n    geom_line(\n      data = walk_data$long_df,\n      aes(x = t, y = pos, group = pid),\n      linewidth = g_linewidth,\n      alpha = a,\n      #color = cb_palette[2]\n      #color = \"#cf8f00\"\n      color = \"black\"\n    ) +\n    geom_point(\n      data = walk_data$end_df,\n      aes(x = t, y = endpos),\n      alpha = 0\n    ) +\n    scale_x_continuous(\n      breaks = seq(\n        0,\n        walk_data$num_steps,\n        walk_data$num_steps / 4\n      )\n    ) +\n    scale_y_continuous(\n      breaks = seq(-20, 20, 10)\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      legend.position = \"none\",\n      # title = element_text(size = 16)\n    ) +\n    theme(\n      panel.grid.major.y = element_line(\n        color = grid_color,\n        linewidth = 1,\n        linetype = 1\n      )\n    ) +\n    labs(\n      title = paste0(\n        walk_data$num_people, \" Random Walks, \",\n        walk_data$num_steps, \" Steps\"\n      ),\n      x = \"Number of Steps\",\n      y = \"Position\"\n    )\n}\nwalk_data &lt;- readRDS(\"assets/walk_data.rds\")\n# 16 steps\n# wp1 &lt;- gen_walkplot(500, 16, 0.05)\n# ggMarginal(wp1, margins = \"y\", type = \"histogram\", yparams = list(binwidth = 1))\nwp &lt;- gen_walk_plot(walk_data) + ylim(-30,30)\nggMarginal(\n  wp, margins = \"y\",\n  type = \"histogram\",\n  yparams = list(binwidth = 1)\n)"
  },
  {
    "objectID": "w06/slides.html#prior-stage-distributions",
    "href": "w06/slides.html#prior-stage-distributions",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Prior ‚ÄúStage‚Äù Distributions",
    "text": "Prior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\nEnter Prior World (Before observing any data): Since we have priors over \\(\\boldsymbol\\theta\\) \\(\\leadsto\\) we can sample values of \\(\\boldsymbol\\theta\\), then generate synthetic data on the basis of these values\n\n\n\n\n\n Prior Distribution: \\(\\Pr(\\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat can I guess about values of my parameters from background knowledge of the world? e.g.:\n\nHuman heights can‚Äôt be negative\nData collected at a bar \\(\\Rightarrow\\) Age \\(\\geq\\) 18, \\(\\Pr(\\text{Age} = x)\\) decreases as \\(x\\) goes above 30\n\n\n\n\n\n\n\n\n Prior Predictive Distribution:  \\(\\Pr(\\mathbf{X}^{‚ùì} \\mid \\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat could the outcomes look like if I ran my guesses through the DGP?\n\n100 simulated heights, none are negative\n1K sim bar-goers; 80% have this haircut \\(\\rightarrow\\)"
  },
  {
    "objectID": "w06/slides.html#posterior-stage-distributions",
    "href": "w06/slides.html#posterior-stage-distributions",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Posterior ‚ÄúStage‚Äù Distributions",
    "text": "Posterior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\n\n\n\n\n Posterior Distribution: \\(\\Pr\\left(\\boldsymbol{\\theta} \\; \\middle| \\; \\mathbf{X} = \\ponode{\\mathbf{X}}\\right)\\)\n\nNow we observe data: \\(\\pnode{\\mathbf{X}} \\leadsto \\ponode{\\mathbf{X}}\\), which means we can use Bayes‚Äô Rule to infer distribution over \\(\\boldsymbol{\\theta}\\): what values are most likely to produce \\(\\ponode{\\mathbf{X}}\\)?\n\n60% of observed bar-goers have that haircut \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta}_{\\text{post}} \\approx \\frac{0.6 + 0.8}{2} = 0.7\\)\nCoin = \\(\\textsf{H}\\) 4/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 0.8}{2} = 0.65\\)\nCoin = \\(\\textsf{H}\\) 5/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 1.0}{2} = 0.75\\)\n\n\n\n\n\n\n\n\n Posterior Predictive Distribution: \\(\\Pr(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)\n\nNow that we‚Äôve fit \\(\\Pr(\\boldsymbol{\\theta})\\) to data, can generate as much new data as we want, e.g.¬†to evaluate how well model predicts outcomes for test data\n\n\n\nFrom PyMC documentation"
  },
  {
    "objectID": "w06/slides.html#why-do-we-need-subjective-priors",
    "href": "w06/slides.html#why-do-we-need-subjective-priors",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Why Do We Need ‚ÄúSubjective‚Äù Priors?",
    "text": "Why Do We Need ‚ÄúSubjective‚Äù Priors?\n\nUnder frequentism‚Ä¶ (tldr) literally no method for dealing with shades of uncertainty\nFrequentist assumption: For a given coin, \\(\\Pr(\\textsf{Heads})\\) is not a Random Variable! It‚Äôs some number, like 0.5 or 0.8. It‚Äôs the asymptote of \\(\\frac{\\#[\\textsf{Heads}]}{\\#[\\textsf{Heads}] + \\#[\\textsf{Tails}]}\\) as \\(n \\rightarrow \\infty\\)\nSo, if we flip coin 1 time, and get \\(\\textsf{Heads}\\), no basis for inferring \\(\\Pr(\\textsf{Coin is Fair})\\) vs.¬†\\(\\Pr(\\textsf{Coin is Biased})\\): Both are undefined. We‚Äôve‚Ä¶ ‚Äúdiscovered‚Äù that \\(\\Pr(\\textsf{Heads}) = 1\\)\nBy using Bayesian inference, we can bring prior knowledge into our studies, which we‚Äôll need to do especially for complex emergent systems: societies, economies, etc.!\nIn fact, as a guy named Laplace discovered in the nineteenth century, frequentism = Bayesian inference with ‚Äúflat priors‚Äù‚Ä¶"
  },
  {
    "objectID": "w06/slides.html#flat-vs.-informative-priors",
    "href": "w06/slides.html#flat-vs.-informative-priors",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Flat vs.¬†Informative Priors",
    "text": "Flat vs.¬†Informative Priors\n\nCode\nlibrary(tidyverse)\nflat_df &lt;- tibble(x=seq(0, 1, 0.1), y=0)\nflat_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=cb_palette[1],\n    linewidth=g_linewidth\n  ) +\n  ylim(0, 1) +\n  labs(\n    title=\"Flat Prior on Pr(Heads)\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Observed Data\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\nlibrary(latex2exp)\nw_label &lt;- TeX(\"Width = $1/n$\")\nh_label &lt;- TeX(\"Height = $n$\")\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth,\n    color=cb_palette[1], arrow=arrow()\n  ) +\n  geom_segment(\n    x=0, y=0, xend=1, linewidth=g_linewidth,\n    color=cb_palette[1]\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Posterior of Pr(Heads)\",\n    y = \"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20)) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.8, \n    label = w_label, hjust = 0, vjust = 1, size = 8\n  ) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.7, \n    label = h_label, hjust = 0, vjust = 1, size = 8\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nunif_df &lt;- tibble(x=seq(0, 1, 0.1), y=1)\nunif_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=\"#e69f00\", linewidth=g_linewidth\n  ) +\n  annotate('rect', xmin=0, xmax=1, ymin=0, ymax=1, fill='#e69f00', alpha=0.3) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(title=\"Uniform Prior on Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Observed Data\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\nx_vals &lt;- seq(0, 1, 0.01)\nmy_exp &lt;- function(x) exp(1-1/(x^2))\ny_vals &lt;- sapply(x_vals, my_exp)\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\nrib_df &lt;- tibble(x=x_vals, ymax=y_vals, ymin=0)\nggplot() +\n  # stat_function(fun=my_exp, linewidth=g_linewidth, color=cb_palette[1]) +\n  geom_line(\n    data=data_df,\n    aes(x=x, y=y),\n    linewidth=g_linewidth, color=cb_palette[1]\n  ) +\n  geom_ribbon(\n    data=rib_df,\n    aes(x=x, ymin=ymin, ymax=ymax),\n    fill=cb_palette[1], alpha=0.3\n  ) +\n  # geom_segment(\n  #   x=1, y=0, yend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1], arrow=arrow()\n  # ) +\n  # geom_segment(\n  #   x=0, y=0, xend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1]\n  # ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Posterior of Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]"
  },
  {
    "objectID": "w06/slides.html#references",
    "href": "w06/slides.html#references",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to DSAN 5650: Causal Inference for Computational Social Science at Georgetown University!\nThe course meets on Wednesdays from 6:30-9pm online via Zoom",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-staff",
    "href": "syllabus.html#course-staff",
    "title": "Syllabus",
    "section": "Course Staff",
    "text": "Course Staff\n\nProf.¬†Jeff Jacobs, jj1088@georgetown.edu\n\nOffice hours (Click to schedule): Tuesdays, 3:30-6pm\n\nTA Courtney Green, crg123@georgetown.edu\n\nOffice hours by appointment\n\nTA Wendy Hu, lh1078@georgetown.edu\n\nOffice hours by appointment",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThis course provides students with the opportunity to take the analytical skills, machine learning algorithms, and statistical methods learned throughout their first year in the program and explore how they can be employed to carry out rigorous, original research in the behavioral and social sciences. With a particular emphasis on tackling the additional challenges which arise when moving from associational to causal inference, particularly when only observational (as opposed to experimental) data is available, students will become proficient in cutting-edge causal Machine Learning techniques such as propensity score matching, synthetic controls, causal program evaluation, inverse social welfare function estimation from panel data, and Double-Debiased Machine Learning.\nIn-class examples will cover continuous, discrete-choice, and textual data from a wide swath of social and behavioral sciences: economics, political science, sociology, anthropology, quantitative history, and digital humanities. After gaining experience through in-class labs and homework assignments focused on reproducing key findings from recent journal articles in each of these disciplines, students will spend the final weeks of the course on a final project demonstrating their ability to develop, evaluate, and test the robustness of a causal hypothesis.\nPrerequisites: DSAN 5000, DSAN 5100 (DSAN 5300 recommended but not required)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "Syllabus",
    "section": "Course Overview",
    "text": "Course Overview\nThe fundamental building block for the course is the idea of a Data-Generating Process (DGP). You may have encountered this concept in passing during other DSAN courses (for example, in DSAN 5100, a phrase like ‚ÄúAssume \\(X\\) is drawn i.i.d. from a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\)‚Äù is a statement characterizing the DGP of a Random Variable \\(X\\)), but in this course we will ‚Äúzoom in‚Äù on this concept rather than treating it like a black box or a footnote to e.g.¬†a theorem like the Law of Large Numbers.\nThis deep dive into DGPs is necessary for us here, since our goal in the course is to move from associational statements like ‚Äúan increase of \\(X\\) by one unit is associated with an increase of \\(Y\\) by \\(\\beta\\) units‚Äù to causal statements like ‚Äúincreasing \\(X\\) by one unit causes \\(Y\\) to increase by \\(\\beta\\) units‚Äù. As you‚Äôll see in Week 1, the tools from probability theory and statistics that you learned in DSAN 5100‚ÄîRandom Variables, Cumulative Distribution Functions, Conditional Probability, and so on‚Äîare necessary but not sufficient to analyze data from a causal perspective.\nFor example, if we use our tools from DSAN 5000 and DSAN 5100 on some dataset to discover that:\n\nThe probability that some event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\), and\nThe probability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\),\n\nwe unfortunately cannot infer from these two pieces of information that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring.\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶ In recent decades, however, researchers have built up what amounts to an additional ‚Äúlayer‚Äù of modeling tools which augment the existing machinery of probability theory to address causality head-on!1\nFor instance, a modeling approach called ‚Äú\\(\\textsf{do}\\)-Calculus‚Äù, that we will learn in this class, extends the core operations and definitions of probability theory to allow such a move towards inferring causality! It does this by introducing a \\(\\textsf{do}(\\cdot)\\) operator that can be applied to Random Variables like \\(X\\), with e.g.¬†\\(\\textsf{do}(X = 5)\\) representing the event wherein someone has intervened in a Data-Generating Process to force the value of \\(X\\) to be 5.\nWith this operator in hand (that is, used alongside an explicit model of a DGP satisfying a set of underlying axioms which are slightly more strict than the axioms of probability theory), it turns out that we can make causal inferences using a very similar pair of facts! If we know that:\n\nThe probability that some event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\), and\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nnow we can actually draw the inference that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!\nThis stylized comparison (between what‚Äôs possible using ‚Äúcore‚Äù probability theory and what‚Äôs possible when we augment it with additional causal modeling tools) serves as our basic motivation for the course, so that from Week 2 onwards we build upon this foundation to reach the three learning goals described in the next section!",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#main-textbooks-resources",
    "href": "syllabus.html#main-textbooks-resources",
    "title": "Syllabus",
    "section": "Main Textbooks / Resources",
    "text": "Main Textbooks / Resources\nUnlike the case for topics like calculus or statistical learning, this field is too new (and exciting! with new methods being developed month-to-month) to have a single set of ‚Äúestablished‚Äù textbooks. Thus, the main collection of resources (books, papers, and explanatory videos) we‚Äôll draw on for this class are available on the resources page. However, there are three ‚Äúcore‚Äù textbooks you can draw on which best align with the topics in this course:\n\nMorgan and Winship, Counterfactuals and Causal Inference: Methods and Principles for Social Research (Morgan and Winship 2015) [PDF]\n\nThe book which comes closest to being an all-encompassing, single textbook for the class. It brings together different ‚Äústrands‚Äù of causal modeling research (since each field‚Äîeconomics, bioinformatics, sociology, etc.‚Äîtends to use its own notation and vocabulary), unifying them into a single approach. The only reason we can‚Äôt use it as the main textbook is because it hasn‚Äôt been updated since 2015, and most of the assignments in this class use computational tools from 2018 onwards!\n\nAngrist and Pischke, Mastering ‚ÄôMetrics: The Path from Cause to Effect (Angrist and Pischke 2014) [PDF]\n\nThis book is included as the second of the three ‚Äúcore‚Äù texts mainly because, it uses the language of causality specific to Econometrics, the language that is most familiar to me from my PhD training in Political Economy. However, if you tend to learn better by example, it also does a good job of foregrounding specific examples (like evaluating charter schools and community policing policies), so that the methods emerging naturally from attempts to solve these puzzles when association methods like linear regression fail to capture their causal linkages.\n\nPearl and Mackenzie, The Book of Why: The New Science of Cause and Effect (Pearl and Mackenzie 2018) [EPUB]\n\nThis book contrasts with the Angrist and Pischke book in using the language of causality formed within Computer Science rather than Economics. It can be a good starting point especially if you‚Äôre unfamiliar with the heavy use of diagrams for scientific modeling‚Äîbasically, whereas Angrist and Pischke‚Äôs first instinct is to use (sometimes informal) equations like \\(y = mx + b\\) to explain steps in the procedures, Pearl and Mackenzie‚Äôs instinct would be to instead use something like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~x~\\kern .01em} \\overset{\\small m, b}{\\longrightarrow} \\enclose{circle}{\\kern.01em y~\\kern .01em}\\) to represent the same concept (in this case, a line with slope \\(m\\) and intercept \\(b\\)!).",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule\nThe following is a rough map of what we will work through together throughout the semester; given that everyone learns at a different pace, my aim is to leave us with a good amount of flexibility in terms of how much time we spend on each topic: if I find that it takes me longer than a week to convey a certain topic in sufficient depth, for example, then I view it as a strength rather than a weakness of the course that we can then rearrange the calendar below by adding an extra week on that particular topic! Similarly, if it seems like I am spending too much time on a topic, to the point that students seem bored or impatient to move onto the next topic, we can move a topic intended for the next week to the current week!\n\n\n\nUnit\nWeek\nDate\nTopic\n\n\n\n\nUnit 1: The Language of Causal Modeling\n1\nMay 21\nFrom a Science of Particles to a Science of PeopleRelevant Supplementary Material:Santa Fe Institute online course: Introduction to Renormalization, up through Video 5, ‚ÄúCoarse Graining II: Entropy‚Äù2\n\n\n\n2\nMay 28\nProbabilistic Graphical Models (PGMs) as Data-Generating Processes (DGPs)Relevant Reading:Koller and Friedman (2009), Chapter 2: ‚ÄúFoundations‚Äù. Especially pp.¬†15-34, as a DSAN 5100 refresher!\n\n\nUnit 2: Doin Thangs\n3\nJun 4\nFrom PGMs to Causal Diagrams[Deliverable, 11:59pm EDT] HW1: Causal Modeling with DAGs and PGMs: Direct and Indirect Effects\n\n\n\n4\nJun 11\nClearing the Path from Cause to Effect\n\n\nUnit 3: Matching Apples to Apples\n5\nJun 18\nMultilevel Modeling, Closing Backdoor Paths\n\n\n\n\nJun 22 (Sunday)\nHW2 Released on JupyterHub\n\n\n\n6\nJun 25\nMidterm Pre-Review\n\n\nMidterm Week\n7\nJul 1 (Tuesday)\n[Deliverable, 11:59pm EDT] HW2: Using Our Modeling Languages\n\n\n\n\nJul 2\nMidterm Introduction27-Hour Take-Home Midterm released, 9:00pm EDT\n\n\n\n\nJul 3 (Thursday)\n[Deliverable, 11:59pm EDT] 27-Hour Take-Home Midterm\n\n\nUnit 4: Machine Learning for Causal Inference\n8\nJul 9\nATEs, ATTs, HTEs, and OBAs (Other Beautiful Acronyms)\n\n\n\n9\nJul 16\nDouble-Debiased Machine Learning\n\n\n\n10\nJul 23\nCausal Forests for Heterogeneous Treatment Effects\n\n\n\n11\nJul 30\nFancier Causal Inference at the Fancy Computational Methods Frontier\n\n\nFinal Project Zone\n12\nAug 6\nFinal Project Presentations\n\n\n\n\nAug 8 (Friday), 5:59pm EDT\n[Deliverable] Final Project",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assignments-and-grading",
    "href": "syllabus.html#assignments-and-grading",
    "title": "Syllabus",
    "section": "Assignments and Grading",
    "text": "Assignments and Grading\nThe main assignment in the course will be your final project, submitted at the end of the semester. However, there will also be a (virtual) in-class midterm exam and a series of assignments which exist to let you explore each of the modules of the course, in turn.\n\n\n\n\n\n\n\n\nAssignment\nDue Date\n% of Grade\n\n\n\n\nHW1: Causal Modeling with DAGs and PGMs: Direct and Indirect Effects\nMonday, June 9\n9%\n\n\nHW2: Using Our Modeling Languages\nTuesday, July 1\n9%\n\n\nHW3: Statistical Matching: Apples to Apples and Oranges to Oranges\nFriday, June 27\n9%\n\n\nIn-Class Midterm\nWednesday, July 2\n25%\n\n\nHW4: Basic Computational Causal Methods\nFriday, July 11\n9%\n\n\nHW5: Fancy Computational Causal Methods\nFriday, July 25\n9%\n\n\nFinal Project\nFriday, August 15\n30%\n\n\n\n\nHomework Lateness Policy\nAfter the due date, for each assignment besides the midterm, you will have a grace period of 24 hours to submit the assignment without a lateness penalty. After this 24-hour grace period, late penalties will be applied based on the following scale (unless you obtain an excused lateness from one of the instructional staff!):\n\n0 to 24 hours late: no penalty\n24 to 30 hours late: 2.5% penalty\n30 to 42 hours late: 5% penalty\n42 to 54 hours late: 10% penalty\n54 to 66 hours late: 20% penalty\nMore than 66 hours late: Assignment submissions no longer accepted (without instructor approval)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPearl (2000) represents a key work in this field of research, as it essentially brought together different pieces of causal models into one unified, rigorous framework.‚Ü©Ô∏é\nChallenge yourself to keep watching to the end of the 5th video here, if you can! Even if you feel frustrated/scared! Because, the examples (e.g., macro vs.¬†microeconomics) are what we mainly care about here, more so than e.g.¬†the mathematical definition of entropy (which we will go towards at a slower pace!)‚Ü©Ô∏é",
    "crumbs": [
      "Syllabus"
    ]
  }
]