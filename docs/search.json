[
  {
    "objectID": "w12/slides.html#final-project-end-of-term-things",
    "href": "w12/slides.html#final-project-end-of-term-things",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "Final Project / End of Term Things",
    "text": "Final Project / End of Term Things\n\nMidterm grading will be done TONIGHT\nSubmit button for HW3 / HW4 TONIGHT"
  },
  {
    "objectID": "w12/slides.html#sensitivity-analysis",
    "href": "w12/slides.html#sensitivity-analysis",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\nSo you‚Äôve got an estimate of a causal effect! What now? Two possible next steps‚Ä¶\n Sensitivity Analysis: Check robustness of your results\n\nModeling choices ‚Üí one point in param space (garden of forking paths)\nIf results ‚Äútruly‚Äù hold, shouldn‚Äôt disappear with small changes in parameters/choices\nLast week: Can re-estimate with informative ‚Üí weak ‚Üí skeptical priors\nThis week: Simulate how ‚Äústrong‚Äù omissions would have to be to ‚Äúruin‚Äù finding\n\n Heterogeneous Treatment Effects: How does the effect vary btwn subgroups?\n\nExample today: PROGRESA (Cash transfer poverty-alleviation program in Mexico)\nProgram has an overall causal effect, but also a greater causal effect on indigenous households, relative to non-indigenous\nHow can we find group-specific effects? Causal forests!"
  },
  {
    "objectID": "w12/slides.html#does-aging-cause-sadness",
    "href": "w12/slides.html#does-aging-cause-sadness",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "Does Aging Cause Sadness?",
    "text": "Does Aging Cause Sadness?\n\n\n\n\nEach year, 20 people are born with uniformly-distributed happiness values\n\n\n\nEach year, each person ages one year; Happiness does not change\n\n\n\nAt age 18, individuals can become married; Odds of marriage each year proportional to individual's happiness; Once married, they remain married\n\n\n\nAt age 70 individuals leave the sample (They move to Boca Raton, Florida)\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nrng = np.random.default_rng(seed=5650)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncb_palette = ['#e69f00','#56b4e9','#009e73']\nsns.set_palette(cb_palette)\nimport patchworklib as pw\nfrom scipy.special import expit\n\n# The original R code:\n# sim_happiness &lt;- function( seed=1977 , N_years=1000 , max_age=65 , N_births=20 , aom=18 ) {\n#     set.seed(seed)\n#     H &lt;- M &lt;- A &lt;- c()\n#     for ( t in 1:N_years ) {\n#         A &lt;- A + 1 # age existing individuals\n#         A &lt;- c( A , rep(1,N_births) ) # newborns\n#         H &lt;- c( H , seq(from=-2,to=2,length.out=N_births) ) # sim happiness trait - never changes\n#         M &lt;- c( M , rep(0,N_births) ) # not yet married\n#         # for each person over 17, chance get married\n#         for ( i in 1:length(A) ) {\n#             if ( A[i] &gt;= aom & M[i]==0 ) {\n#                 M[i] &lt;- rbern(1,inv_logit(H[i]-4))\n#             }\n#         }\n#         # mortality\n#         deaths &lt;- which( A &gt; max_age )\n#         if ( length(deaths)&gt;0 ) {\n#             A &lt;- A[ -deaths ]\n#             H &lt;- H[ -deaths ]\n#             M &lt;- M[ -deaths ]\n#        }\n#     }\n#     d &lt;- data.frame(age=A,married=M,happiness=H)\n#     return(d)\n\n# DGP: happiness -&gt; marriage &lt;- age\nyears = 70\nnum_births = 41\ncolnames = ['age','a','h','m']\nsim_dfs = []\nA = np.zeros(shape=(num_births,1))\nH = np.linspace(-2, 2, num=num_births)\nM = np.zeros(shape=(num_births,1))\ndef update_m(row):\n  if row['m'] == 0:\n    return int(rng.binomial(\n      n=1,\n      p=expit(row['h'] - 3.875),\n      size=1,\n    ))\n  return 1\ndef sim_cohort_to(max_age):\n  sim_df = pd.DataFrame({\n      'age': [1 for _ in range(num_births)],\n      'h': np.linspace(-2, 2, num=num_births),\n      'm': [0 for _ in range(num_births)],\n    }\n  )\n  for t in range(2, max_age + 1):\n    sim_df['age'] = sim_df['age'] + 1\n    if t &gt;= 18:\n      sim_df['m'] = sim_df.apply(update_m, axis=1)\n  return sim_df\nall_sim_dfs = []\nfor cur_max_age in range(1, 71):\n  cur_sim_df = sim_cohort_to(cur_max_age)\n  all_sim_dfs.append(cur_sim_df)\nfull_sim_df = pd.concat(all_sim_dfs)\n\n# full_sim_df.head()\ncbg_palette = ['#c6c6c666'] + cb_palette\nfull_sim_df['m_label'] = full_sim_df['m'].apply(lambda x: \"Unmarried\" if x == 0 else \"Married\")\nfull_sim_df = full_sim_df.rename(columns={'age': 'Age', 'h': 'Happiness'})\nax = pw.Brick(figsize=(5.25,2.75));\nsns.scatterplot(\n  x='Age', y='Happiness', hue='m_label',\n  data=full_sim_df,\n  ax=ax,\n  palette=cbg_palette,\n  s=22,\n  legend=True,\n);\nax.move_legend(\"upper center\", bbox_to_anchor=(0.5, 1.15), ncol=2);\nax.legend_.set_title(\"\");\nax.axvline(x=17.5, color='black', ls='dashed', lw=1);\nax.savefig()\nmean_hap_df = full_sim_df.groupby('Age')['Happiness'].mean().reset_index()\nmean_hap_df['m_label'] = \"All\"\nmean_hap_df['Happiness_mean'] = 0\ngroup_hap_df = full_sim_df[full_sim_df['Age'] &gt;= 18].groupby(['Age','m_label'])['Happiness'].mean().reset_index()\nmarried_df = group_hap_df[group_hap_df['m_label'] == \"Married\"].copy()\nunmarried_df = group_hap_df[group_hap_df['m_label'] == \"Unmarried\"].copy()\n# Moving averages\nwin_size = 3\nmarried_df['Happiness_mean'] = married_df['Happiness'].rolling(window=win_size).mean()\nunmarried_df['Happiness_mean'] = unmarried_df['Happiness'].rolling(window=win_size).mean()\n# Merge back together\ncombined_df = pd.concat([mean_hap_df, married_df, unmarried_df], ignore_index=True)\n# display(combined_df.tail())\n\nax = pw.Brick(figsize=(3.75,2.4));\nax.set_xlim(0, 70);\n# ax.set_ylim(-1.2, 1.2);\ncbg_palette = ['black', cb_palette[0], '#b2b2b2ff']\n\n# And plot\nsns.lineplot(\n  x='Age', y='Happiness_mean',\n  hue='m_label',\n  # marker=\".\",\n  # s=20,\n  data=combined_df,\n  palette=cbg_palette,\n  # color=cbg_palette[0],\n  # order=3,\n  ax=ax\n);\nax.set_title(\"Mean Happiness by Status\");\nax.legend_.set_title(\"\");\nax.set_xlabel(\"Mean Happiness\");\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Happiness by Age, 70 birth cohorts of size 41 each(DC minimum marriage age = 18)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFig¬†1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶What‚Äôs happening here? \\(\\textsf{Happy} \\rightarrow {}^{ü§î}\\textsf{Marriage}^{ü§î} \\leftarrow \\textsf{Age}\\)"
  },
  {
    "objectID": "w12/slides.html#assessing-the-impact-of-omitted-included-vars",
    "href": "w12/slides.html#assessing-the-impact-of-omitted-included-vars",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "Assessing the Impact of Omitted / Included Vars",
    "text": "Assessing the Impact of Omitted / Included Vars\n\nWorking example from Cinelli and Hazlett (2020): War in Darfur (2004-2020)\nWhat is the causal impact of experiencing violence on support for a peace deal\nResearcher A models this scenario as:\n\n\\[\n\\textsf{PeaceIndex} = \\tau_{\\text{res}} \\textsf{DirectHarm} + \\hat{\\beta}_{\\text{f},\\text{res}}\\textsf{Female} + \\textsf{Village} \\hat{\\boldsymbol \\beta}_{\\text{v},\\text{res}} + \\mathbf{X} \\hat{\\boldsymbol \\beta}_{\\text{res}} + \\hat{\\varepsilon}_{\\text{res}}\n\\]\n\nResearcher B instead prefers:\n\n\\[\n\\textsf{PeaceIndex} = \\tau \\textsf{DirectHarm} + \\hat{\\beta}_{\\text{f}}\\textsf{Female} + \\textsf{Village} \\hat{\\boldsymbol \\beta}_{\\text{v}} + \\mathbf{X} \\hat{\\boldsymbol \\beta} + \\boxed{\\hat{\\gamma}\\textsf{Center}} + \\hat{\\varepsilon}_{\\text{full}}\n\\]\n\nOur earlier estimate \\(\\tau_{\\text{res}}\\) would differ from our target quantity \\(\\tau\\): but how badly? [‚Ä¶] How strong would the confounder(s) have to be to change the estimates in such a way to affect the main conclusions of a study?"
  },
  {
    "objectID": "w12/slides.html#the-classical-ovb-equation",
    "href": "w12/slides.html#the-classical-ovb-equation",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "The Classical OVB Equation",
    "text": "The Classical OVB Equation\n\nRemember that \\(D\\) is our treatment variable! (\\(\\mathbf{X}\\) = covariates, \\(Y\\) = outcome)\nIn a perfect world, we would estimate \\(Y = \\hat{\\tau}D + \\mathbf{X} \\hat{\\beta} + \\hat{\\gamma}Z+ \\varepsilon_{\\text{full}}\\)\nBut, \\(Z\\) is unobserved \\(\\leadsto\\) ‚Äúrestricted‚Äù model \\(Y = \\hat{\\tau}_{\\text{res}}D + \\mathbf{X} \\hat{\\beta}_{\\text{res}} + \\varepsilon_{\\text{res}}\\)\nWhat is the ‚Äúgap‚Äù between \\(\\hat{\\tau}\\) and \\(\\hat{\\tau}^{\\text{res}}\\)? \\(\\text{OVB} = \\hat{\\tau}_{\\text{res}} - \\hat{\\tau}\\)\n\n\n\n\\[\n\\begin{align*}\n\\hat{\\tau}_{\\text{res}} &= \\frac{\n  \\text{Cov}[D^{\\top \\mathbf{X}}, Y^{\\top \\mathbf{X}}]\n}{\n  \\text{Var}[D^{\\top \\mathbf{X}}]\n} \\\\\n&= \\frac{\n  \\text{Cov}[D^{\\top \\mathbf{X}}, \\hat{\\tau}D^{\\top \\mathbf{X}} + \\hat{\\gamma}Z^{\\top \\mathbf{X}}]\n}{\n  \\text{Var}[D^{\\top \\mathbf{X}}]\n} \\\\\n&= \\hat{\\tau} + \\hat{\\gamma}\\frac{\n  \\text{Cov}[D^{\\top \\mathbf{X}}, Z^{\\top \\mathbf{X}}]\n}{\n  \\text{Var}[D^{\\top \\mathbf{X}}]\n} \\\\\n&= \\hat{\\tau} + \\hat{\\gamma}\\hat{\\delta} \\\\\n\\implies \\text{OVB} &= \\hat{\\tau}_{\\text{res}} - \\hat{\\tau} = \\overbrace{\n  \\boxed{\\hat{\\gamma} \\times \\hat{\\delta}}\n}^{\\mathclap{\\text{Impact} \\, \\times \\, \\text{Imbalance}}}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nThe ability to produce orthogonalized (\\(\\top \\mathbf{X}\\)) versions of vars in the model utilizes the Frisch-Waugh-Lovell Theorem"
  },
  {
    "objectID": "w12/slides.html#more-generally",
    "href": "w12/slides.html#more-generally",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "More Generally",
    "text": "More Generally\n\nWhat if we don‚Äôt have a specific omitted variable in mind? We just want to know the expected impact if there were omitted vars‚Ä¶ Enter ‚ÄúPartial \\(R^2\\)‚Äù"
  },
  {
    "objectID": "w12/slides.html#progresa",
    "href": "w12/slides.html#progresa",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "PROGRESA",
    "text": "PROGRESA"
  },
  {
    "objectID": "w12/slides.html#references",
    "href": "w12/slides.html#references",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "References",
    "text": "References\n\n\nCinelli, Carlos, and Chad Hazlett. 2020. ‚ÄúMaking Sense of Sensitivity: Extending Omitted Variable Bias.‚Äù Journal of the Royal Statistical Society Series B: Statistical Methodology 82 (1): 39‚Äì67. https://doi.org/10.1111/rssb.12348."
  },
  {
    "objectID": "w12/index.html",
    "href": "w12/index.html",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#final-project-end-of-term-things",
    "href": "w12/index.html#final-project-end-of-term-things",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "Final Project / End of Term Things",
    "text": "Final Project / End of Term Things\n\nMidterm grading will be done TONIGHT\nSubmit button for HW3 / HW4 TONIGHT",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#sensitivity-analysis",
    "href": "w12/index.html#sensitivity-analysis",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\nSo you‚Äôve got an estimate of a causal effect! What now? Two possible next steps‚Ä¶\n Sensitivity Analysis: Check robustness of your results\n\nModeling choices ‚Üí one point in param space (garden of forking paths)\nIf results ‚Äútruly‚Äù hold, shouldn‚Äôt disappear with small changes in parameters/choices\nLast week: Can re-estimate with informative ‚Üí weak ‚Üí skeptical priors\nThis week: Simulate how ‚Äústrong‚Äù omissions would have to be to ‚Äúruin‚Äù finding\n\n Heterogeneous Treatment Effects: How does the effect vary btwn subgroups?\n\nExample today: PROGRESA (Cash transfer poverty-alleviation program in Mexico)\nProgram has an overall causal effect, but also a greater causal effect on indigenous households, relative to non-indigenous\nHow can we find group-specific effects? Causal forests!",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#does-aging-cause-sadness",
    "href": "w12/index.html#does-aging-cause-sadness",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "Does Aging Cause Sadness?",
    "text": "Does Aging Cause Sadness?\n\n\n\n\nEach year, 20 people are born with uniformly-distributed happiness values\n\n\n\nEach year, each person ages one year; Happiness does not change\n\n\n\nAt age 18, individuals can become married; Odds of marriage each year proportional to individual's happiness; Once married, they remain married\n\n\n\nAt age 70 individuals leave the sample (They move to Boca Raton, Florida)\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nrng = np.random.default_rng(seed=5650)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncb_palette = ['#e69f00','#56b4e9','#009e73']\nsns.set_palette(cb_palette)\nimport patchworklib as pw\nfrom scipy.special import expit\n\n# The original R code:\n# sim_happiness &lt;- function( seed=1977 , N_years=1000 , max_age=65 , N_births=20 , aom=18 ) {\n#     set.seed(seed)\n#     H &lt;- M &lt;- A &lt;- c()\n#     for ( t in 1:N_years ) {\n#         A &lt;- A + 1 # age existing individuals\n#         A &lt;- c( A , rep(1,N_births) ) # newborns\n#         H &lt;- c( H , seq(from=-2,to=2,length.out=N_births) ) # sim happiness trait - never changes\n#         M &lt;- c( M , rep(0,N_births) ) # not yet married\n#         # for each person over 17, chance get married\n#         for ( i in 1:length(A) ) {\n#             if ( A[i] &gt;= aom & M[i]==0 ) {\n#                 M[i] &lt;- rbern(1,inv_logit(H[i]-4))\n#             }\n#         }\n#         # mortality\n#         deaths &lt;- which( A &gt; max_age )\n#         if ( length(deaths)&gt;0 ) {\n#             A &lt;- A[ -deaths ]\n#             H &lt;- H[ -deaths ]\n#             M &lt;- M[ -deaths ]\n#        }\n#     }\n#     d &lt;- data.frame(age=A,married=M,happiness=H)\n#     return(d)\n\n# DGP: happiness -&gt; marriage &lt;- age\nyears = 70\nnum_births = 41\ncolnames = ['age','a','h','m']\nsim_dfs = []\nA = np.zeros(shape=(num_births,1))\nH = np.linspace(-2, 2, num=num_births)\nM = np.zeros(shape=(num_births,1))\ndef update_m(row):\n  if row['m'] == 0:\n    return int(rng.binomial(\n      n=1,\n      p=expit(row['h'] - 3.875),\n      size=1,\n    ))\n  return 1\ndef sim_cohort_to(max_age):\n  sim_df = pd.DataFrame({\n      'age': [1 for _ in range(num_births)],\n      'h': np.linspace(-2, 2, num=num_births),\n      'm': [0 for _ in range(num_births)],\n    }\n  )\n  for t in range(2, max_age + 1):\n    sim_df['age'] = sim_df['age'] + 1\n    if t &gt;= 18:\n      sim_df['m'] = sim_df.apply(update_m, axis=1)\n  return sim_df\nall_sim_dfs = []\nfor cur_max_age in range(1, 71):\n  cur_sim_df = sim_cohort_to(cur_max_age)\n  all_sim_dfs.append(cur_sim_df)\nfull_sim_df = pd.concat(all_sim_dfs)\n\n# full_sim_df.head()\ncbg_palette = ['#c6c6c666'] + cb_palette\nfull_sim_df['m_label'] = full_sim_df['m'].apply(lambda x: \"Unmarried\" if x == 0 else \"Married\")\nfull_sim_df = full_sim_df.rename(columns={'age': 'Age', 'h': 'Happiness'})\nax = pw.Brick(figsize=(5.25,2.75));\nsns.scatterplot(\n  x='Age', y='Happiness', hue='m_label',\n  data=full_sim_df,\n  ax=ax,\n  palette=cbg_palette,\n  s=22,\n  legend=True,\n);\nax.move_legend(\"upper center\", bbox_to_anchor=(0.5, 1.15), ncol=2);\nax.legend_.set_title(\"\");\nax.axvline(x=17.5, color='black', ls='dashed', lw=1);\nax.savefig()\nmean_hap_df = full_sim_df.groupby('Age')['Happiness'].mean().reset_index()\nmean_hap_df['m_label'] = \"All\"\nmean_hap_df['Happiness_mean'] = 0\ngroup_hap_df = full_sim_df[full_sim_df['Age'] &gt;= 18].groupby(['Age','m_label'])['Happiness'].mean().reset_index()\nmarried_df = group_hap_df[group_hap_df['m_label'] == \"Married\"].copy()\nunmarried_df = group_hap_df[group_hap_df['m_label'] == \"Unmarried\"].copy()\n# Moving averages\nwin_size = 3\nmarried_df['Happiness_mean'] = married_df['Happiness'].rolling(window=win_size).mean()\nunmarried_df['Happiness_mean'] = unmarried_df['Happiness'].rolling(window=win_size).mean()\n# Merge back together\ncombined_df = pd.concat([mean_hap_df, married_df, unmarried_df], ignore_index=True)\n# display(combined_df.tail())\n\nax = pw.Brick(figsize=(3.75,2.4));\nax.set_xlim(0, 70);\n# ax.set_ylim(-1.2, 1.2);\ncbg_palette = ['black', cb_palette[0], '#b2b2b2ff']\n\n# And plot\nsns.lineplot(\n  x='Age', y='Happiness_mean',\n  hue='m_label',\n  # marker=\".\",\n  # s=20,\n  data=combined_df,\n  palette=cbg_palette,\n  # color=cbg_palette[0],\n  # order=3,\n  ax=ax\n);\nax.set_title(\"Mean Happiness by Status\");\nax.legend_.set_title(\"\");\nax.set_xlabel(\"Mean Happiness\");\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Happiness by Age, 70 birth cohorts of size 41 each(DC minimum marriage age = 18)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFig¬†1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶What‚Äôs happening here? \\(\\textsf{Happy} \\rightarrow {}^{ü§î}\\textsf{Marriage}^{ü§î} \\leftarrow \\textsf{Age}\\)",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#assessing-the-impact-of-omitted-included-vars",
    "href": "w12/index.html#assessing-the-impact-of-omitted-included-vars",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "Assessing the Impact of Omitted / Included Vars",
    "text": "Assessing the Impact of Omitted / Included Vars\n\nWorking example from Cinelli and Hazlett (2020): War in Darfur (2004-2020)\nWhat is the causal impact of experiencing violence on support for a peace deal\nResearcher A models this scenario as:\n\n\\[\n\\textsf{PeaceIndex} = \\tau_{\\text{res}} \\textsf{DirectHarm} + \\hat{\\beta}_{\\text{f},\\text{res}}\\textsf{Female} + \\textsf{Village} \\hat{\\boldsymbol \\beta}_{\\text{v},\\text{res}} + \\mathbf{X} \\hat{\\boldsymbol \\beta}_{\\text{res}} + \\hat{\\varepsilon}_{\\text{res}}\n\\]\n\nResearcher B instead prefers:\n\n\\[\n\\textsf{PeaceIndex} = \\tau \\textsf{DirectHarm} + \\hat{\\beta}_{\\text{f}}\\textsf{Female} + \\textsf{Village} \\hat{\\boldsymbol \\beta}_{\\text{v}} + \\mathbf{X} \\hat{\\boldsymbol \\beta} + \\boxed{\\hat{\\gamma}\\textsf{Center}} + \\hat{\\varepsilon}_{\\text{full}}\n\\]\n\nOur earlier estimate \\(\\tau_{\\text{res}}\\) would differ from our target quantity \\(\\tau\\): but how badly? [‚Ä¶] How strong would the confounder(s) have to be to change the estimates in such a way to affect the main conclusions of a study?",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#the-classical-ovb-equation",
    "href": "w12/index.html#the-classical-ovb-equation",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "The Classical OVB Equation",
    "text": "The Classical OVB Equation\n\nRemember that \\(D\\) is our treatment variable! (\\(\\mathbf{X}\\) = covariates, \\(Y\\) = outcome)\nIn a perfect world, we would estimate \\(Y = \\hat{\\tau}D + \\mathbf{X} \\hat{\\beta} + \\hat{\\gamma}Z+ \\varepsilon_{\\text{full}}\\)\nBut, \\(Z\\) is unobserved \\(\\leadsto\\) ‚Äúrestricted‚Äù model \\(Y = \\hat{\\tau}_{\\text{res}}D + \\mathbf{X} \\hat{\\beta}_{\\text{res}} + \\varepsilon_{\\text{res}}\\)\nWhat is the ‚Äúgap‚Äù between \\(\\hat{\\tau}\\) and \\(\\hat{\\tau}^{\\text{res}}\\)? \\(\\text{OVB} = \\hat{\\tau}_{\\text{res}} - \\hat{\\tau}\\)\n\n\n\n\\[\n\\begin{align*}\n\\hat{\\tau}_{\\text{res}} &= \\frac{\n  \\text{Cov}[D^{\\top \\mathbf{X}}, Y^{\\top \\mathbf{X}}]\n}{\n  \\text{Var}[D^{\\top \\mathbf{X}}]\n} \\\\\n&= \\frac{\n  \\text{Cov}[D^{\\top \\mathbf{X}}, \\hat{\\tau}D^{\\top \\mathbf{X}} + \\hat{\\gamma}Z^{\\top \\mathbf{X}}]\n}{\n  \\text{Var}[D^{\\top \\mathbf{X}}]\n} \\\\\n&= \\hat{\\tau} + \\hat{\\gamma}\\frac{\n  \\text{Cov}[D^{\\top \\mathbf{X}}, Z^{\\top \\mathbf{X}}]\n}{\n  \\text{Var}[D^{\\top \\mathbf{X}}]\n} \\\\\n&= \\hat{\\tau} + \\hat{\\gamma}\\hat{\\delta} \\\\\n\\implies \\text{OVB} &= \\hat{\\tau}_{\\text{res}} - \\hat{\\tau} = \\overbrace{\n  \\boxed{\\hat{\\gamma} \\times \\hat{\\delta}}\n}^{\\mathclap{\\text{Impact} \\, \\times \\, \\text{Imbalance}}}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\nThe ability to produce orthogonalized (\\(\\top \\mathbf{X}\\)) versions of vars in the model utilizes the Frisch-Waugh-Lovell Theorem",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#more-generally",
    "href": "w12/index.html#more-generally",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "More Generally",
    "text": "More Generally\n\nWhat if we don‚Äôt have a specific omitted variable in mind? We just want to know the expected impact if there were omitted vars‚Ä¶ Enter ‚ÄúPartial \\(R^2\\)‚Äù",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#progresa",
    "href": "w12/index.html#progresa",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "PROGRESA",
    "text": "PROGRESA",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "w12/index.html#references",
    "href": "w12/index.html#references",
    "title": "Week 12: Causal Forests for Heterogeneous Treatment Effects",
    "section": "References",
    "text": "References\n\n\nCinelli, Carlos, and Chad Hazlett. 2020. ‚ÄúMaking Sense of Sensitivity: Extending Omitted Variable Bias.‚Äù Journal of the Royal Statistical Society Series B: Statistical Methodology 82 (1): 39‚Äì67. https://doi.org/10.1111/rssb.12348.",
    "crumbs": [
      "Week 12: {{< var w12.date-md >}}"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to DSAN 5650: Causal Inference for Computational Social Science at Georgetown University!\nThe course meets on Wednesdays from 6:30-9pm online via Zoom",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-staff",
    "href": "syllabus.html#course-staff",
    "title": "Syllabus",
    "section": "Course Staff",
    "text": "Course Staff\n\nProf.¬†Jeff Jacobs, jj1088@georgetown.edu\n\nOffice hours (Click to schedule): Tuesdays, 3:30-6pm\n\nTA Courtney Green, crg123@georgetown.edu\n\nOffice hours by appointment\n\nTA Wendy Hu, lh1078@georgetown.edu\n\nOffice hours by appointment",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThis course provides students with the opportunity to take the analytical skills, machine learning algorithms, and statistical methods learned throughout their first year in the program and explore how they can be employed to carry out rigorous, original research in the behavioral and social sciences. With a particular emphasis on tackling the additional challenges which arise when moving from associational to causal inference, particularly when only observational (as opposed to experimental) data is available, students will become proficient in cutting-edge causal Machine Learning techniques such as propensity score matching, synthetic controls, causal program evaluation, inverse social welfare function estimation from panel data, and Double-Debiased Machine Learning.\nIn-class examples will cover continuous, discrete-choice, and textual data from a wide swath of social and behavioral sciences: economics, political science, sociology, anthropology, quantitative history, and digital humanities. After gaining experience through in-class labs and homework assignments focused on reproducing key findings from recent journal articles in each of these disciplines, students will spend the final weeks of the course on a final project demonstrating their ability to develop, evaluate, and test the robustness of a causal hypothesis.\nPrerequisites: DSAN 5000, DSAN 5100 (DSAN 5300 recommended but not required)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "Syllabus",
    "section": "Course Overview",
    "text": "Course Overview\nThe fundamental building block for the course is the idea of a Data-Generating Process (DGP). You may have encountered this concept in passing during other DSAN courses (for example, in DSAN 5100, a phrase like ‚ÄúAssume \\(X\\) is drawn i.i.d. from a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\)‚Äù is a statement characterizing the DGP of a Random Variable \\(X\\)), but in this course we will ‚Äúzoom in‚Äù on this concept rather than treating it like a black box or a footnote to e.g.¬†a theorem like the Law of Large Numbers.\nThis deep dive into DGPs is necessary for us here, since our goal in the course is to move from associational statements like ‚Äúan increase of \\(X\\) by one unit is associated with an increase of \\(Y\\) by \\(\\beta\\) units‚Äù to causal statements like ‚Äúincreasing \\(X\\) by one unit causes \\(Y\\) to increase by \\(\\beta\\) units‚Äù. As you‚Äôll see in Week 1, the tools from probability theory and statistics that you learned in DSAN 5100‚ÄîRandom Variables, Cumulative Distribution Functions, Conditional Probability, and so on‚Äîare necessary but not sufficient to analyze data from a causal perspective.\nFor example, if we use our tools from DSAN 5000 and DSAN 5100 on some dataset to discover that:\n\nThe probability that some event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\), and\nThe probability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\),\n\nwe unfortunately cannot infer from these two pieces of information that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring.\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶ In recent decades, however, researchers have built up what amounts to an additional ‚Äúlayer‚Äù of modeling tools which augment the existing machinery of probability theory to address causality head-on!1\nFor instance, a modeling approach called ‚Äú\\(\\textsf{do}\\)-Calculus‚Äù, that we will learn in this class, extends the core operations and definitions of probability theory to allow such a move towards inferring causality! It does this by introducing a \\(\\textsf{do}(\\cdot)\\) operator that can be applied to Random Variables like \\(X\\), with e.g.¬†\\(\\textsf{do}(X = 5)\\) representing the event wherein someone has intervened in a Data-Generating Process to force the value of \\(X\\) to be 5.\nWith this operator in hand (that is, used alongside an explicit model of a DGP satisfying a set of underlying axioms which are slightly more strict than the axioms of probability theory), it turns out that we can make causal inferences using a very similar pair of facts! If we know that:\n\nThe probability that some event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\), and\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nnow we can actually draw the inference that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!\nThis stylized comparison (between what‚Äôs possible using ‚Äúcore‚Äù probability theory and what‚Äôs possible when we augment it with additional causal modeling tools) serves as our basic motivation for the course, so that from Week 2 onwards we build upon this foundation to reach the three learning goals described in the next section!",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#main-textbooks-resources",
    "href": "syllabus.html#main-textbooks-resources",
    "title": "Syllabus",
    "section": "Main Textbooks / Resources",
    "text": "Main Textbooks / Resources\nUnlike the case for topics like calculus or statistical learning, this field is too new (and exciting! with new methods being developed month-to-month) to have a single set of ‚Äúestablished‚Äù textbooks. Thus, the main collection of resources (books, papers, and explanatory videos) we‚Äôll draw on for this class are available on the resources page. However, there are three ‚Äúcore‚Äù textbooks you can draw on which best align with the topics in this course:\n\nMorgan and Winship, Counterfactuals and Causal Inference: Methods and Principles for Social Research (Morgan and Winship 2015) [PDF]\n\nThe book which comes closest to being an all-encompassing, single textbook for the class. It brings together different ‚Äústrands‚Äù of causal modeling research (since each field‚Äîeconomics, bioinformatics, sociology, etc.‚Äîtends to use its own notation and vocabulary), unifying them into a single approach. The only reason we can‚Äôt use it as the main textbook is because it hasn‚Äôt been updated since 2015, and most of the assignments in this class use computational tools from 2018 onwards!\n\nAngrist and Pischke, Mastering ‚ÄôMetrics: The Path from Cause to Effect (Angrist and Pischke 2014) [PDF]\n\nThis book is included as the second of the three ‚Äúcore‚Äù texts mainly because, it uses the language of causality specific to Econometrics, the language that is most familiar to me from my PhD training in Political Economy. However, if you tend to learn better by example, it also does a good job of foregrounding specific examples (like evaluating charter schools and community policing policies), so that the methods emerging naturally from attempts to solve these puzzles when association methods like linear regression fail to capture their causal linkages.\n\nPearl and Mackenzie, The Book of Why: The New Science of Cause and Effect (Pearl and Mackenzie 2018) [EPUB]\n\nThis book contrasts with the Angrist and Pischke book in using the language of causality formed within Computer Science rather than Economics. It can be a good starting point especially if you‚Äôre unfamiliar with the heavy use of diagrams for scientific modeling‚Äîbasically, whereas Angrist and Pischke‚Äôs first instinct is to use (sometimes informal) equations like \\(y = mx + b\\) to explain steps in the procedures, Pearl and Mackenzie‚Äôs instinct would be to instead use something like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~x~\\kern .01em} \\overset{\\small m, b}{\\longrightarrow} \\enclose{circle}{\\kern.01em y~\\kern .01em}\\) to represent the same concept (in this case, a line with slope \\(m\\) and intercept \\(b\\)!).",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule\nThe following is a rough map of what we will work through together throughout the semester; given that everyone learns at a different pace, my aim is to leave us with a good amount of flexibility in terms of how much time we spend on each topic: if I find that it takes me longer than a week to convey a certain topic in sufficient depth, for example, then I view it as a strength rather than a weakness of the course that we can then rearrange the calendar below by adding an extra week on that particular topic! Similarly, if it seems like I am spending too much time on a topic, to the point that students seem bored or impatient to move onto the next topic, we can move a topic intended for the next week to the current week!\n\n\n\nUnit\nWeek\nDate\nTopic\n\n\n\n\nUnit 1: The Language of Causal Modeling\n1\nMay 21\nFrom a Science of Particles to a Science of PeopleRelevant Supplementary Material:Santa Fe Institute online course: Introduction to Renormalization, up through Video 5, ‚ÄúCoarse Graining II: Entropy‚Äù2\n\n\n\n2\nMay 28\nProbabilistic Graphical Models (PGMs) as Data-Generating Processes (DGPs)Relevant Reading:Koller and Friedman (2009), Chapter 2: ‚ÄúFoundations‚Äù. Especially pp.¬†15-34, as a DSAN 5100 refresher!\n\n\nUnit 2: Doin Thangs\n3\nJun 4\nFrom PGMs to Causal Diagrams[Deliverable, 11:59pm EDT] HW1: Causal Modeling with DAGs and PGMs: Direct and Indirect Effects\n\n\n\n4\nJun 11\nClearing the Path from Cause to Effect\n\n\nUnit 3: Matching Apples to Apples\n5\nJun 18\nMultilevel Modeling, Closing Backdoor Paths\n\n\n\n\nJun 22 (Sunday)\nHW2 Released on JupyterHub\n\n\n\n6\nJun 25\nMidterm Pre-Review\n\n\nMidterm Week\n7\nJul 1 (Tuesday)\n[Deliverable, 11:59pm EDT] HW2: Using Our Modeling Languages\n\n\n\n\nJul 2\nMidterm Introduction27-Hour Take-Home Midterm released, 9:00pm EDT\n\n\n\n\nJul 3 (Thursday)\n[Deliverable, 11:59pm EDT] 27-Hour Take-Home Midterm\n\n\nUnit 4: Machine Learning for Causal Inference\n8\nJul 9\nPropensity Score Weighting\n\n\n\n9\nJul 16\nDoubly-Robust Estimation and Instrumental Variables\n\n\n\n10\nJul 23\nText-as-Data\n\n\n\n11\nJul 30\nSensitivity Analysis\n\n\nFinal Project Zone\n12\nAug 6\nCausal Forests for Heterogeneous Treatment Effects\n\n\n\n\nAug 8 (Friday), 5:59pm EDT\n[Deliverable] Final Project",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assignments-and-grading",
    "href": "syllabus.html#assignments-and-grading",
    "title": "Syllabus",
    "section": "Assignments and Grading",
    "text": "Assignments and Grading\nThe main assignment in the course will be your final project, submitted at the end of the semester. However, there will also be a (virtual) in-class midterm exam and a series of assignments which exist to let you explore each of the modules of the course, in turn.\n\n\n\n\n\n\n\n\nAssignment\nDue Date\n% of Grade\n\n\n\n\nHW1: Causal Modeling with DAGs and PGMs: Direct and Indirect Effects\nMonday, June 9\n11.25%\n\n\nHW2: Using Our Modeling Languages\nTuesday, July 1\n11.25%\n\n\nHW3: Social Science: PGMs and the Asymptote of Causality\nTBD\n11.25%\n\n\nIn-Class Midterm\nWednesday, July 2\n25%\n\n\nHW4: Computational Causal Methods\nTBD\n11.25%\n\n\nFinal Project\nFriday, August 15\n30%\n\n\n\n\nHomework Lateness Policy\nAfter the due date, for each assignment besides the midterm, you will have a grace period of 24 hours to submit the assignment without a lateness penalty. After this 24-hour grace period, late penalties will be applied based on the following scale (unless you obtain an excused lateness from one of the instructional staff!):\n\n0 to 24 hours late: no penalty\n24 to 30 hours late: 2.5% penalty\n30 to 42 hours late: 5% penalty\n42 to 54 hours late: 10% penalty\n54 to 66 hours late: 20% penalty\nMore than 66 hours late: Assignment submissions no longer accepted (without instructor approval)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPearl (2000) represents a key work in this field of research, as it essentially brought together different pieces of causal models into one unified, rigorous framework.‚Ü©Ô∏é\nChallenge yourself to keep watching to the end of the 5th video here, if you can! Even if you feel frustrated/scared! Because, the examples (e.g., macro vs.¬†microeconomics) are what we mainly care about here, more so than e.g.¬†the mathematical definition of entropy (which we will go towards at a slower pace!)‚Ü©Ô∏é",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "w06/slides.html#from-hws-to-midterm",
    "href": "w06/slides.html#from-hws-to-midterm",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "From HWs to Midterm",
    "text": "From HWs to Midterm\n\nHWs: More focus on social science / more ‚Äúin the weeds‚Äù (checking details, manipulating prior parameters, debugging, etc.)\nMidterm: More focus on causality, bc, in 2 hours I can test you on concepts and things like ‚ÄúWhat are the backdoor paths here? How would you close them?‚Äù more easily than on loading datasets, cleaning, fitting models, etc."
  },
  {
    "objectID": "w06/slides.html#looking-forwards-post-midterm",
    "href": "w06/slides.html#looking-forwards-post-midterm",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Looking Forwards (Post-Midterm)",
    "text": "Looking Forwards (Post-Midterm)\n\nMore text analysis!\nMaking the connection between modeling and predicting"
  },
  {
    "objectID": "w06/slides.html#modeling-how-trees-become-forests",
    "href": "w06/slides.html#modeling-how-trees-become-forests",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Modeling How Trees Become Forests",
    "text": "Modeling How Trees Become Forests\n\nYour model, if it‚Äôs generative (which‚Ä¶ nearly all in 5650 are), relates observed data \\(\\mathbf{D}\\) to a set of underlying parameters \\(\\boldsymbol{\\theta}\\) that are hypothesized as ‚Äúgiving rise‚Äù to \\(\\mathbf{D}\\)\nWhen you specify how exactly this ‚Äúgiving rise‚Äù works, you‚Äôre specifying a DGP!\nPGMs: human-brain-friendly (bc graphical) language for writing DGPs; then move to PyAgrum \\(\\rightarrow\\) Stan \\(\\rightarrow\\) PyMC to ‚Äúencode‚Äù PGMs (in 0100101) so computer can‚Ä¶\n\n\n\n\n Super-charge your EDA/modeling\n Estimate \\(\\boldsymbol{\\theta}\\) from data\n\n\n\n\n\\(\\leadsto\\) Prior distributions\n\\(\\leadsto\\) Posterior distributions\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggExtra)\ngen_walk_plot &lt;- function(walk_data, a=0.0075) {\n  # print(end_df)\n  grid_color &lt;- rgb(0, 0, 0, 0.1)\n  # And plot!\n  walkplot &lt;- ggplot() +\n    geom_line(\n      data = walk_data$long_df,\n      aes(x = t, y = pos, group = pid),\n      linewidth = g_linewidth,\n      alpha = a,\n      #color = cb_palette[2]\n      #color = \"#cf8f00\"\n      color = \"black\"\n    ) +\n    geom_point(\n      data = walk_data$end_df,\n      aes(x = t, y = endpos),\n      alpha = 0\n    ) +\n    scale_x_continuous(\n      breaks = seq(\n        0,\n        walk_data$num_steps,\n        walk_data$num_steps / 4\n      )\n    ) +\n    scale_y_continuous(\n      breaks = seq(-20, 20, 10)\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      legend.position = \"none\",\n      # title = element_text(size = 16)\n    ) +\n    theme(\n      panel.grid.major.y = element_line(\n        color = grid_color,\n        linewidth = 1,\n        linetype = 1\n      )\n    ) +\n    labs(\n      title = paste0(\n        walk_data$num_people, \" Random Walks, \",\n        walk_data$num_steps, \" Steps\"\n      ),\n      x = \"Number of Steps\",\n      y = \"Position\"\n    )\n}\nwalk_data &lt;- readRDS(\"assets/walk_data.rds\")\n# 16 steps\n# wp1 &lt;- gen_walkplot(500, 16, 0.05)\n# ggMarginal(wp1, margins = \"y\", type = \"histogram\", yparams = list(binwidth = 1))\nwp &lt;- gen_walk_plot(walk_data) + ylim(-30,30)\nggMarginal(\n  wp, margins = \"y\",\n  type = \"histogram\",\n  yparams = list(binwidth = 1)\n)"
  },
  {
    "objectID": "w06/slides.html#prior-stage-distributions",
    "href": "w06/slides.html#prior-stage-distributions",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Prior ‚ÄúStage‚Äù Distributions",
    "text": "Prior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\nEnter Prior World (Before observing any data): Since we have priors over \\(\\boldsymbol\\theta\\) \\(\\leadsto\\) we can sample values of \\(\\boldsymbol\\theta\\), then generate synthetic data on the basis of these values\n\n\n\n\n\n Prior Distribution: \\(\\Pr(\\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat can I guess about values of my parameters from background knowledge of the world? e.g.:\n\nHuman heights can‚Äôt be negative\nData collected at a bar \\(\\Rightarrow\\) Age \\(\\geq\\) 18, \\(\\Pr(\\text{Age} = x)\\) decreases as \\(x\\) goes above 30\n\n\n\n\n\n\n\n\n Prior Predictive Distribution:  \\(\\Pr(\\mathbf{X}^{‚ùì} \\mid \\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat could the outcomes look like if I ran my guesses through the DGP?\n\n100 simulated heights, none are negative\n1K sim bar-goers; 80% have this haircut \\(\\rightarrow\\)"
  },
  {
    "objectID": "w06/slides.html#posterior-stage-distributions",
    "href": "w06/slides.html#posterior-stage-distributions",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Posterior ‚ÄúStage‚Äù Distributions",
    "text": "Posterior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\n\n\n\n\n Posterior Distribution: \\(\\Pr\\left(\\boldsymbol{\\theta} \\; \\middle| \\; \\mathbf{X} = \\ponode{\\mathbf{X}}\\right)\\)\n\nNow we observe data: \\(\\pnode{\\mathbf{X}} \\leadsto \\ponode{\\mathbf{X}}\\), which means we can use Bayes‚Äô Rule to infer distribution over \\(\\boldsymbol{\\theta}\\): what values are most likely to produce \\(\\ponode{\\mathbf{X}}\\)?\n\n60% of observed bar-goers have that haircut \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta}_{\\text{post}} \\approx \\frac{0.6 + 0.8}{2} = 0.7\\)\nCoin = \\(\\textsf{H}\\) 4/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 0.8}{2} = 0.65\\)\nCoin = \\(\\textsf{H}\\) 5/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 1.0}{2} = 0.75\\)\n\n\n\n\n\n\n\n\n Posterior Predictive Distribution: \\(\\Pr(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)\n\nNow that we‚Äôve fit \\(\\Pr(\\boldsymbol{\\theta})\\) to data, can generate as much new data as we want, e.g.¬†to evaluate how well model predicts outcomes for test data\n\n\n\nFrom PyMC documentation"
  },
  {
    "objectID": "w06/slides.html#why-do-we-need-subjective-priors",
    "href": "w06/slides.html#why-do-we-need-subjective-priors",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Why Do We Need ‚ÄúSubjective‚Äù Priors?",
    "text": "Why Do We Need ‚ÄúSubjective‚Äù Priors?\n\nUnder frequentism‚Ä¶ (tldr) literally no method for dealing with shades of uncertainty\nFrequentist assumption: For a given coin, \\(\\Pr(\\textsf{Heads})\\) is not a Random Variable! It‚Äôs some number, like 0.5 or 0.8. It‚Äôs the asymptote of \\(\\frac{\\#[\\textsf{Heads}]}{\\#[\\textsf{Heads}] + \\#[\\textsf{Tails}]}\\) as \\(n \\rightarrow \\infty\\)\nSo, if we flip coin 1 time, and get \\(\\textsf{Heads}\\), no basis for inferring \\(\\Pr(\\textsf{Coin is Fair})\\) vs.¬†\\(\\Pr(\\textsf{Coin is Biased})\\): Both are undefined. We‚Äôve‚Ä¶ ‚Äúdiscovered‚Äù that \\(\\Pr(\\textsf{Heads}) = 1\\)\nBy using Bayesian inference, we can bring prior knowledge into our studies, which we‚Äôll need to do especially for complex emergent systems: societies, economies, etc.!\nIn fact, as a guy named Laplace discovered in the nineteenth century, frequentism = Bayesian inference with ‚Äúflat priors‚Äù‚Ä¶"
  },
  {
    "objectID": "w06/slides.html#flat-vs.-informative-priors",
    "href": "w06/slides.html#flat-vs.-informative-priors",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Flat vs.¬†Informative Priors",
    "text": "Flat vs.¬†Informative Priors\n\nCode\nlibrary(tidyverse)\nflat_df &lt;- tibble(x=seq(0, 1, 0.1), y=0)\nflat_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=cb_palette[1],\n    linewidth=g_linewidth\n  ) +\n  ylim(0, 1) +\n  labs(\n    title=\"Flat Prior on Pr(Heads)\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Observed Data\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\nlibrary(latex2exp)\nw_label &lt;- TeX(\"Width = $1/n$\")\nh_label &lt;- TeX(\"Height = $n$\")\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth,\n    color=cb_palette[1], arrow=arrow()\n  ) +\n  geom_segment(\n    x=0, y=0, xend=1, linewidth=g_linewidth,\n    color=cb_palette[1]\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Posterior of Pr(Heads)\",\n    y = \"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20)) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.8, \n    label = w_label, hjust = 0, vjust = 1, size = 8\n  ) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.7, \n    label = h_label, hjust = 0, vjust = 1, size = 8\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nunif_df &lt;- tibble(x=seq(0, 1, 0.1), y=1)\nunif_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=\"#e69f00\", linewidth=g_linewidth\n  ) +\n  annotate('rect', xmin=0, xmax=1, ymin=0, ymax=1, fill='#e69f00', alpha=0.3) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(title=\"Uniform Prior on Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Observed Data\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\nx_vals &lt;- seq(0, 1, 0.01)\nmy_exp &lt;- function(x) exp(1-1/(x^2))\ny_vals &lt;- sapply(x_vals, my_exp)\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\nrib_df &lt;- tibble(x=x_vals, ymax=y_vals, ymin=0)\nggplot() +\n  # stat_function(fun=my_exp, linewidth=g_linewidth, color=cb_palette[1]) +\n  geom_line(\n    data=data_df,\n    aes(x=x, y=y),\n    linewidth=g_linewidth, color=cb_palette[1]\n  ) +\n  geom_ribbon(\n    data=rib_df,\n    aes(x=x, ymin=ymin, ymax=ymax),\n    fill=cb_palette[1], alpha=0.3\n  ) +\n  # geom_segment(\n  #   x=1, y=0, yend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1], arrow=arrow()\n  # ) +\n  # geom_segment(\n  #   x=0, y=0, xend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1]\n  # ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Posterior of Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]"
  },
  {
    "objectID": "w06/slides.html#references",
    "href": "w06/slides.html#references",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "w06/index.html",
    "href": "w06/index.html",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#from-hws-to-midterm",
    "href": "w06/index.html#from-hws-to-midterm",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "From HWs to Midterm",
    "text": "From HWs to Midterm\n\nHWs: More focus on social science / more ‚Äúin the weeds‚Äù (checking details, manipulating prior parameters, debugging, etc.)\nMidterm: More focus on causality, bc, in 2 hours I can test you on concepts and things like ‚ÄúWhat are the backdoor paths here? How would you close them?‚Äù more easily than on loading datasets, cleaning, fitting models, etc.",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#looking-forwards-post-midterm",
    "href": "w06/index.html#looking-forwards-post-midterm",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Looking Forwards (Post-Midterm)",
    "text": "Looking Forwards (Post-Midterm)\n\nMore text analysis!\nMaking the connection between modeling and predicting",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#modeling-how-trees-become-forests",
    "href": "w06/index.html#modeling-how-trees-become-forests",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Modeling How Trees Become Forests",
    "text": "Modeling How Trees Become Forests\n\nYour model, if it‚Äôs generative (which‚Ä¶ nearly all in 5650 are), relates observed data \\(\\mathbf{D}\\) to a set of underlying parameters \\(\\boldsymbol{\\theta}\\) that are hypothesized as ‚Äúgiving rise‚Äù to \\(\\mathbf{D}\\)\nWhen you specify how exactly this ‚Äúgiving rise‚Äù works, you‚Äôre specifying a DGP!\nPGMs: human-brain-friendly (bc graphical) language for writing DGPs; then move to PyAgrum \\(\\rightarrow\\) Stan \\(\\rightarrow\\) PyMC to ‚Äúencode‚Äù PGMs (in 0100101) so computer can‚Ä¶\n\n\n\n\n Super-charge your EDA/modeling\n Estimate \\(\\boldsymbol{\\theta}\\) from data\n\n\n\n\n\\(\\leadsto\\) Prior distributions\n\\(\\leadsto\\) Posterior distributions\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggExtra)\ngen_walk_plot &lt;- function(walk_data, a=0.0075) {\n  # print(end_df)\n  grid_color &lt;- rgb(0, 0, 0, 0.1)\n  # And plot!\n  walkplot &lt;- ggplot() +\n    geom_line(\n      data = walk_data$long_df,\n      aes(x = t, y = pos, group = pid),\n      linewidth = g_linewidth,\n      alpha = a,\n      #color = cb_palette[2]\n      #color = \"#cf8f00\"\n      color = \"black\"\n    ) +\n    geom_point(\n      data = walk_data$end_df,\n      aes(x = t, y = endpos),\n      alpha = 0\n    ) +\n    scale_x_continuous(\n      breaks = seq(\n        0,\n        walk_data$num_steps,\n        walk_data$num_steps / 4\n      )\n    ) +\n    scale_y_continuous(\n      breaks = seq(-20, 20, 10)\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      legend.position = \"none\",\n      # title = element_text(size = 16)\n    ) +\n    theme(\n      panel.grid.major.y = element_line(\n        color = grid_color,\n        linewidth = 1,\n        linetype = 1\n      )\n    ) +\n    labs(\n      title = paste0(\n        walk_data$num_people, \" Random Walks, \",\n        walk_data$num_steps, \" Steps\"\n      ),\n      x = \"Number of Steps\",\n      y = \"Position\"\n    )\n}\nwalk_data &lt;- readRDS(\"assets/walk_data.rds\")\n# 16 steps\n# wp1 &lt;- gen_walkplot(500, 16, 0.05)\n# ggMarginal(wp1, margins = \"y\", type = \"histogram\", yparams = list(binwidth = 1))\nwp &lt;- gen_walk_plot(walk_data) + ylim(-30,30)\nggMarginal(\n  wp, margins = \"y\",\n  type = \"histogram\",\n  yparams = list(binwidth = 1)\n)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#prior-stage-distributions",
    "href": "w06/index.html#prior-stage-distributions",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Prior ‚ÄúStage‚Äù Distributions",
    "text": "Prior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\nEnter Prior World (Before observing any data): Since we have priors over \\(\\boldsymbol\\theta\\) \\(\\leadsto\\) we can sample values of \\(\\boldsymbol\\theta\\), then generate synthetic data on the basis of these values\n\n\n\n\n\n Prior Distribution: \\(\\Pr(\\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat can I guess about values of my parameters from background knowledge of the world? e.g.:\n\nHuman heights can‚Äôt be negative\nData collected at a bar \\(\\Rightarrow\\) Age \\(\\geq\\) 18, \\(\\Pr(\\text{Age} = x)\\) decreases as \\(x\\) goes above 30\n\n\n\n\n\n\n\n\n Prior Predictive Distribution:  \\(\\Pr(\\mathbf{X}^{‚ùì} \\mid \\boldsymbol{\\theta}^{‚ùì})\\)\n\nWhat could the outcomes look like if I ran my guesses through the DGP?\n\n100 simulated heights, none are negative\n1K sim bar-goers; 80% have this haircut \\(\\rightarrow\\)",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#posterior-stage-distributions",
    "href": "w06/index.html#posterior-stage-distributions",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Posterior ‚ÄúStage‚Äù Distributions",
    "text": "Posterior ‚ÄúStage‚Äù Distributions\n\nRVs \\(\\boldsymbol\\theta\\) = params of your model, RVs \\(\\mathbf{X}\\) = data \\(\\leadsto\\) Generative model \\(\\pedge{\\boldsymbol\\theta}{\\mathbf{X}}\\)\n\n\n\n\n Posterior Distribution: \\(\\Pr\\left(\\boldsymbol{\\theta} \\; \\middle| \\; \\mathbf{X} = \\ponode{\\mathbf{X}}\\right)\\)\n\nNow we observe data: \\(\\pnode{\\mathbf{X}} \\leadsto \\ponode{\\mathbf{X}}\\), which means we can use Bayes‚Äô Rule to infer distribution over \\(\\boldsymbol{\\theta}\\): what values are most likely to produce \\(\\ponode{\\mathbf{X}}\\)?\n\n60% of observed bar-goers have that haircut \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta}_{\\text{post}} \\approx \\frac{0.6 + 0.8}{2} = 0.7\\)\nCoin = \\(\\textsf{H}\\) 4/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 0.8}{2} = 0.65\\)\nCoin = \\(\\textsf{H}\\) 5/5 times \\(\\Rightarrow\\) \\(\\boldsymbol{\\theta} = p = \\frac{0.5 + 1.0}{2} = 0.75\\)\n\n\n\n\n\n\n\n\n Posterior Predictive Distribution: \\(\\Pr(\\mathbf{X} \\mid \\boldsymbol{\\theta})\\)\n\nNow that we‚Äôve fit \\(\\Pr(\\boldsymbol{\\theta})\\) to data, can generate as much new data as we want, e.g.¬†to evaluate how well model predicts outcomes for test data\n\n\n\nFrom PyMC documentation",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#why-do-we-need-subjective-priors",
    "href": "w06/index.html#why-do-we-need-subjective-priors",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Why Do We Need ‚ÄúSubjective‚Äù Priors?",
    "text": "Why Do We Need ‚ÄúSubjective‚Äù Priors?\n\nUnder frequentism‚Ä¶ (tldr) literally no method for dealing with shades of uncertainty\nFrequentist assumption: For a given coin, \\(\\Pr(\\textsf{Heads})\\) is not a Random Variable! It‚Äôs some number, like 0.5 or 0.8. It‚Äôs the asymptote of \\(\\frac{\\#[\\textsf{Heads}]}{\\#[\\textsf{Heads}] + \\#[\\textsf{Tails}]}\\) as \\(n \\rightarrow \\infty\\)\nSo, if we flip coin 1 time, and get \\(\\textsf{Heads}\\), no basis for inferring \\(\\Pr(\\textsf{Coin is Fair})\\) vs.¬†\\(\\Pr(\\textsf{Coin is Biased})\\): Both are undefined. We‚Äôve‚Ä¶ ‚Äúdiscovered‚Äù that \\(\\Pr(\\textsf{Heads}) = 1\\)\nBy using Bayesian inference, we can bring prior knowledge into our studies, which we‚Äôll need to do especially for complex emergent systems: societies, economies, etc.!\nIn fact, as a guy named Laplace discovered in the nineteenth century, frequentism = Bayesian inference with ‚Äúflat priors‚Äù‚Ä¶",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#flat-vs.-informative-priors",
    "href": "w06/index.html#flat-vs.-informative-priors",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "Flat vs.¬†Informative Priors",
    "text": "Flat vs.¬†Informative Priors\n\nCode\nlibrary(tidyverse)\nflat_df &lt;- tibble(x=seq(0, 1, 0.1), y=0)\nflat_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=cb_palette[1],\n    linewidth=g_linewidth\n  ) +\n  ylim(0, 1) +\n  labs(\n    title=\"Flat Prior on Pr(Heads)\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Observed Data\",\n    y=\"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\nlibrary(latex2exp)\nw_label &lt;- TeX(\"Width = $1/n$\")\nh_label &lt;- TeX(\"Height = $n$\")\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth,\n    color=cb_palette[1], arrow=arrow()\n  ) +\n  geom_segment(\n    x=0, y=0, xend=1, linewidth=g_linewidth,\n    color=cb_palette[1]\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(\n    title=\"Posterior of Pr(Heads)\",\n    y = \"Density\"\n  ) +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20)) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.8, \n    label = w_label, hjust = 0, vjust = 1, size = 8\n  ) +\n  annotate(\n    geom = \"text\", x = 0.5, y = 0.7, \n    label = h_label, hjust = 0, vjust = 1, size = 8\n  )\n\n\n\n\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî lubridate 1.9.4     ‚úî tibble    3.3.0\n‚úî purrr     1.0.4     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]\n\n\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nunif_df &lt;- tibble(x=seq(0, 1, 0.1), y=1)\nunif_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_line(\n    color=\"#e69f00\", linewidth=g_linewidth\n  ) +\n  annotate('rect', xmin=0, xmax=1, ymin=0, ymax=1, fill='#e69f00', alpha=0.3) +\n  xlim(0, 1) + ylim(0, 1) +\n  labs(title=\"Uniform Prior on Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\ndata_df |&gt; ggplot(aes(x=x, y=y)) +\n  geom_point(size=5) +\n  geom_segment(\n    x=1, y=0, yend=1, linewidth=g_linewidth\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Observed Data\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\nlibrary(tidyverse)\ndata_df &lt;- tibble(x=1, y=1)\nx_vals &lt;- seq(0, 1, 0.01)\nmy_exp &lt;- function(x) exp(1-1/(x^2))\ny_vals &lt;- sapply(x_vals, my_exp)\ndata_df &lt;- tibble(x=x_vals, y=y_vals)\nrib_df &lt;- tibble(x=x_vals, ymax=y_vals, ymin=0)\nggplot() +\n  # stat_function(fun=my_exp, linewidth=g_linewidth, color=cb_palette[1]) +\n  geom_line(\n    data=data_df,\n    aes(x=x, y=y),\n    linewidth=g_linewidth, color=cb_palette[1]\n  ) +\n  geom_ribbon(\n    data=rib_df,\n    aes(x=x, ymin=ymin, ymax=ymax),\n    fill=cb_palette[1], alpha=0.3\n  ) +\n  # geom_segment(\n  #   x=1, y=0, yend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1], arrow=arrow()\n  # ) +\n  # geom_segment(\n  #   x=0, y=0, xend=1, linewidth=g_linewidth,\n  #   color=cb_palette[1]\n  # ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  labs(title=\"Posterior of Pr(Heads)\") +\n  theme_dsan(base_size=28) +\n  theme(title=element_text(size=20))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\otimes\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\leadsto\n\\]",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w06/index.html#references",
    "href": "w06/index.html#references",
    "title": "Week 6: Bayesian Workflow, Midterm Pre-Review",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Week 6: {{< var w06.date-md >}}"
    ]
  },
  {
    "objectID": "w08/slides.html#roadmap-for-part-2",
    "href": "w08/slides.html#roadmap-for-part-2",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Roadmap for Part 2",
    "text": "Roadmap for Part 2\n\n\\(\\downarrow\\) Focus on abstract concepts / terminology\n\\(\\uparrow\\) Focus on data analysis\n(Should hopefully start to feel more like other DSAN classes!)\n(\\(\\Rightarrow\\) Need you to take specific examples I pick and analogize them to your field(s) of interest)"
  },
  {
    "objectID": "w08/slides.html#working-example-growth-mindset",
    "href": "w08/slides.html#working-example-growth-mindset",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Working Example: Growth Mindset(!)",
    "text": "Working Example: Growth Mindset(!)\n\n\n\nFrom Athey and Wager (2019)\nTreatment \\(T\\), called intervention in the dataset: a seminar on growth mindset for high school students\nOutcome \\(Y\\), called achievement_score in the dataset: performance on state‚Äôs standardized test\nIn a perfect world, we could just compute\n\n\\[\n\\mathbb{E}[Y \\mid T = 1] - \\mathbb{E}[Y \\mid T = 0]\n\\]\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nstudent_df &lt;- read_csv(\"assets/learning_mindset.csv\")\n(mean_df &lt;- student_df |&gt; group_by(intervention) |&gt; summarize(mean_score=mean(achievement_score)))\n\n\n\n\n\n\nintervention\nmean_score\n\n\n\n\n0\n-0.1538030\n\n\n1\n0.3184686\n\n\n\n\n\n\nCode\nstudent_naive_lm &lt;- lm(achievement_score ~ intervention, data=student_df)\nstudent_naive_lm |&gt; broom::tidy() |&gt; select(term, estimate)\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n-0.1538030\n\n\nintervention\n0.4722717\n\n\n\n\n\n\nCode\nstudent_df |&gt;\n  ggplot(aes(x=intervention, y=achievement_score)) +\n  geom_boxplot(\n    aes(group=intervention),\n    width=0.5\n  ) +\n  geom_smooth(\n    method='lm',\n    formula='y ~ x',\n    se=TRUE\n  ) +\n  geom_point(\n    data=mean_df,\n    aes(x=intervention, y=mean_score),\n    size=3\n  ) +\n  theme_dsan(base_size=16)"
  },
  {
    "objectID": "w08/slides.html#the-problem-pesky-covariates",
    "href": "w08/slides.html#the-problem-pesky-covariates",
    "title": "Week 8: Propensity Score Weighting",
    "section": "The Problem: Pesky Covariates",
    "text": "The Problem: Pesky Covariates\n\nHere the ‚Äúblob‚Äù \\(\\mathbf{X}\\) forms a fork, as drawn‚Ä¶\nBut in reality the work of modeling is flying into the cloud and modeling the \\(\\mathbf{X}\\)-\\(T\\) and \\(\\mathbf{X}\\)-\\(Y\\) relationships (especially: figuring out which covariates \\(X_j \\in \\mathbf{X}\\) are colliders), so you can close the backdoors:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis can be really difficult, for a bunch of reasons‚Ä¶ What if there was an easier way?"
  },
  {
    "objectID": "w08/slides.html#if-we-had-control-over-everything-experiments-vs.-observational-data-analysis",
    "href": "w08/slides.html#if-we-had-control-over-everything-experiments-vs.-observational-data-analysis",
    "title": "Week 8: Propensity Score Weighting",
    "section": "If We Had Control Over Everything (Experiments vs.¬†Observational Data Analysis)",
    "text": "If We Had Control Over Everything (Experiments vs.¬†Observational Data Analysis)\n\nIf we could intervene in the DGP, we could assign treatment randomly, thus removing the impact of Covariates on \\(T\\)!\n\n\n\nAlas, we are data scientists, not (necessarily) experiment-conductors, plus there are often ethical reasons to not perform experiments!\n‚Ä¶There‚Äôs still another approach!"
  },
  {
    "objectID": "w08/slides.html#closing-backdoors-the-too-good-to-be-true-way",
    "href": "w08/slides.html#closing-backdoors-the-too-good-to-be-true-way",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Closing Backdoors the Too-Good-To-Be-True Way",
    "text": "Closing Backdoors the Too-Good-To-Be-True Way\n\nKey insight from causal thinking: Transformation of the problem from ‚Äúcontrol for all covariates‚Äù to ‚Äúclose all backdoor paths‚Äù‚Ä¶\nFor the goal of just closing these paths, we have an alternative1:\n\n\n\n\nRosenbaum and Rubin (1983): there exists a statistic \\(\\mathtt{e}(\\mathbf{X}) = \\Pr(T \\mid \\mathbf{X})\\), the propensity score, which ‚Äúcaptures‚Äù info in \\(\\textbf{X}\\) relevant to \\(T\\) such that\nConditioning on \\(\\mathtt{e}(\\mathbf{X})\\) closes \\(\\mathbf{X} \\Rightarrow \\mathtt{e}(\\mathbf{X}) \\rightarrow T\\) (\\(\\mathtt{e}(\\mathbf{X})\\) is a pipe)\n\n\n\n\n\n\n\n\n\nThis would close backdoor path \\(T \\leftarrow \\mathtt{e}(\\mathbf{X}) \\Leftarrow \\mathbf{X} \\rightarrow Y\\), leaving only direct effect \\(T \\rightarrow Y\\)! There‚Äôs one remaining complication‚Ä¶\n\nFor reasons we‚Äôll see later (double-robustness), you should ultimately try to achieve both goals!"
  },
  {
    "objectID": "w08/slides.html#closing-backdoors-via-propensity-score-estimation",
    "href": "w08/slides.html#closing-backdoors-via-propensity-score-estimation",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Closing Backdoors via Propensity Score Estimation",
    "text": "Closing Backdoors via Propensity Score Estimation\n\nSadly we don‚Äôt observe true probability of being treated for all possible values of \\(\\mathbf{X}\\)\nBut, we can derive an estimate \\(\\hat{\\mathtt{e}}(\\mathbf{X})\\) using our machine learning skills üòé\nWe now have that \\(\\hat{\\mathtt{e}}(\\mathbf{X})\\), as a proxy relative to the pipe \\(\\mathbf{X} \\Rightarrow \\mathtt{e}(\\mathbf{X}) \\rightarrow T\\), blocks the pipe to the extent that it captures the true probability \\(\\mathtt{e}(\\mathbf{X}) = \\Pr(T \\mid \\mathbf{X})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBackdoor Path: \\(T \\leftarrow \\mathtt{e}(\\mathbf{X}) \\Leftarrow \\mathbf{X} \\rightarrow Y\\)\nClosed in proportion to \\(\\left[ \\text{Cor}(\\hat{\\mathtt{e}}(\\mathbf{X}), \\mathtt{e}(\\mathbf{X})) \\right]^2 = ‚ùì\\)"
  },
  {
    "objectID": "w08/slides.html#sometimes-helpful-thought-experiment",
    "href": "w08/slides.html#sometimes-helpful-thought-experiment",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Sometimes-Helpful Thought Experiment",
    "text": "Sometimes-Helpful Thought Experiment\n\n\n\nBack in our basic confounding scenario:\nIf there was only one covariate (\\(\\mathbf{X} = X\\)), and it was a constant (\\(\\Pr(X = c) = 1\\)), then all the variation in \\(Y\\) would be due to variation in \\(T\\)\nLess extreme: if person \\(i\\) has covariates \\(\\mathbf{X}_i\\) and person \\(j\\) has covariates \\(\\mathbf{X}_j\\), but \\(\\mathbf{X}_i = \\mathbf{X}_j\\), then variation in their outcomes is due solely to \\(T\\)\n\n\n\n\n\n\n\n\nPart of the logic of propensity score is that, if person \\(i\\) has covariates \\(\\mathbf{X}_i\\) and person \\(j\\) has covariates \\(\\mathbf{X}_j\\), but \\(\\mathtt{e}(\\mathbf{X}_i) = \\mathtt{e}(\\mathbf{X}_j)\\), then \\(i\\) and \\(j\\) are perfectly matched\n\\(\\Rightarrow\\) (by fun math proof) variation in their outcomes is due solely to \\(T\\)"
  },
  {
    "objectID": "w08/slides.html#how-exactly-do-we-adjust-for-hatmathttemathbfx",
    "href": "w08/slides.html#how-exactly-do-we-adjust-for-hatmathttemathbfx",
    "title": "Week 8: Propensity Score Weighting",
    "section": "How Exactly Do We Adjust For \\(\\hat{\\mathtt{e}}(\\mathbf{X})\\)?",
    "text": "How Exactly Do We Adjust For \\(\\hat{\\mathtt{e}}(\\mathbf{X})\\)?\n\n\nSimulation example: smoking reduction\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(Rlab)\nset.seed(5650)\nn &lt;- 250\nmotiv_vals &lt;- runif(n, 0, 1)\nenroll_vals &lt;- ifelse(\n  motiv_vals &lt; 0.25,\n  0,\n  # We know motiv &gt; 0.25\n  ifelse(\n    motiv_vals &gt; 0.75,\n    1,\n    # We know 0.25 &lt; motiv &lt; 0.75\n    rbern(n, prob=(motiv_vals - 0.125)*1.5)\n  )\n)\nncigs_vals &lt;- rbinom(n, size=30, prob=0.6-0.2*enroll_vals)\nsmoke_df &lt;- tibble(\n  motiv=motiv_vals,\n  enroll=enroll_vals,\n  ncigs=ncigs_vals\n)\n(smoke_mean_df &lt;- smoke_df |&gt; group_by(enroll) |&gt; summarize(mean_ncigs=mean(ncigs)))\n\n\n\n\n\n\nenroll\nmean_ncigs\n\n\n\n\n0\n17.88060\n\n\n1\n12.26724\n\n\n\n\n\n\nCode\nnaive_smoke_lm &lt;- lm(ncigs ~ enroll, data=smoke_df)\nsummary(naive_smoke_lm) |&gt; broom::tidy() |&gt;\n  select(term, estimate)\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n17.880597\n\n\nenroll\n-5.613356\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsmoke_df |&gt; ggplot(aes(x=enroll, y=ncigs)) +\n  geom_boxplot(\n    aes(group=enroll),\n    width=0.5\n  ) +\n  geom_smooth(\n    method='lm',\n    formula='y ~ x',\n    se=TRUE\n  ) +\n  geom_point(\n    data=smoke_mean_df,\n    aes(x=enroll, y=mean_ncigs),\n    size=3\n  ) +\n  theme_dsan(base_size=24)"
  },
  {
    "objectID": "w08/slides.html#inverse-probability-of-treatment-weighting",
    "href": "w08/slides.html#inverse-probability-of-treatment-weighting",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Inverse Probability-of-Treatment Weighting",
    "text": "Inverse Probability-of-Treatment Weighting\n\n\n\n\nCode\neprop_model &lt;- glm(enroll ~ motiv, family='binomial', data=smoke_df)\neprop_preds &lt;- predict(eprop_model, type=\"response\")\nsmoke_df &lt;- smoke_df |&gt; mutate(pred=eprop_preds)\n# Use the preds to compute IPW\nsmoke_df &lt;- smoke_df |&gt; rowwise() |&gt; mutate(\n  ipw=ifelse(enroll, 1/pred, 1/(1-pred))\n) |&gt; arrange(pred)\n#smoke_df\nsmoke_df |&gt; mutate(enroll=factor(enroll)) |&gt;\n  ggplot(aes(x=motiv, y=ncigs, color=enroll)) +\n  geom_point() +\n  theme_dsan(base_size=24) +\n  labs(title=\"Before Weighting\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsmoke_df |&gt; mutate(enroll=factor(enroll)) |&gt;\n  ggplot(aes(\n    x=motiv, y=ncigs, color=enroll, size=ipw,\n    alpha=log(ipw-1)\n  )) +\n  geom_point() +\n  guides(alpha=\"none\") +\n  theme_dsan(base_size=24) +\n  labs(title=\"After Weighting\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nsmoke_df |&gt;\n  ggplot(aes(x=motiv)) +\n  # Predictions\n  geom_point(\n    aes(y=enroll, color=factor(enroll))\n  ) +\n  # Values\n  geom_point(\n    aes(y=pred, color=factor(enroll))\n  ) +\n  labs(color=\"enroll\") +\n  theme_dsan(base_size=24) +\n  labs(title=\"Propensity to Enroll\")\nipw_min &lt;- min(smoke_df$ipw)\nipw_max &lt;- max(smoke_df$ipw)\nsmoke_df &lt;- smoke_df |&gt; mutate(\n  ipw_scaled = (ipw - ipw_min) / (ipw_max - ipw_min)\n)\nsmoke_df |&gt;\n  ggplot(aes(x=motiv)) +\n  # Predictions\n  geom_point(\n    aes(y=enroll, color=factor(enroll))\n  ) +\n  # Values\n  geom_point(\n    aes(y=ipw_scaled, color=factor(enroll))\n  ) +\n  theme_dsan(base_size=24) +\n  labs(\n    title=\"Inverse Probability-of-Treatment Weights (IPTW)\",\n    color=\"enroll\"\n  )\n\n\n\n\n\n‚Üì\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Üí\n\n\n\n\n‚Üë"
  },
  {
    "objectID": "w08/slides.html#the-final-result",
    "href": "w08/slides.html#the-final-result",
    "title": "Week 8: Propensity Score Weighting",
    "section": "The Final Result!",
    "text": "The Final Result!\n\n\n\nCode\nlm_with_weights &lt;- lm(ncigs ~ enroll,\n  data=smoke_df, weights=smoke_df$ipw\n)\nsummary(lm_with_weights) |&gt; broom::tidy() |&gt;\n  select(term, estimate, std.error)\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n18.10133\n0.2466712\n\n\nenroll\n-5.66453\n0.3571696\n\n\n\n\n\n\n\n\n\nCode\nlibrary(WeightIt)\nW &lt;- weightit(\n  enroll ~ motiv, data = smoke_df, ps=\"pred\"\n)\nsmoke_weighted_lm &lt;- lm_weightit(\n  ncigs ~ enroll, data = smoke_df, weightit = W\n)\nsummary(smoke_weighted_lm, ci = FALSE)\n\n\n\n\nCall:\nlm_weightit(formula = ncigs ~ enroll, data = smoke_df, weightit = W)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  18.1013     0.2479   73.02   &lt;1e-06 ***\nenroll       -5.6645     0.5463  -10.37   &lt;1e-06 ***\nStandard error: HC0 robust\n\n\n\n\n\nCode\nW_default &lt;- weightit(enroll ~ motiv, data = smoke_df)\nsmoke_default_lm &lt;- lm_weightit(\n  ncigs ~ enroll, data = smoke_df,\n  weightit = W_default\n)\nsummary(smoke_default_lm, ci = FALSE)\n\n\n\n\nCall:\nlm_weightit(formula = ncigs ~ enroll, data = smoke_df, weightit = W_default)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  18.1013     0.2441   74.16   &lt;1e-06 ***\nenroll       -5.6645     0.5444  -10.41   &lt;1e-06 ***\nStandard error: HC0 robust (adjusted for estimation of weights)"
  },
  {
    "objectID": "w08/slides.html#references",
    "href": "w08/slides.html#references",
    "title": "Week 8: Propensity Score Weighting",
    "section": "References",
    "text": "References\n\n\nAthey, Susan, and Stefan Wager. 2019. ‚ÄúEstimating Treatment Effects with Causal Forests: An Application.‚Äù arXiv. https://doi.org/10.48550/arXiv.1902.07409.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1983. ‚ÄúThe Central Role of the Propensity Score in Observational Studies for Causal Effects.‚Äù Biometrika 70 (1): 41‚Äì55. https://doi.org/10.2307/2335942."
  },
  {
    "objectID": "w08/index.html",
    "href": "w08/index.html",
    "title": "Week 8: Propensity Score Weighting",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#roadmap-for-part-2",
    "href": "w08/index.html#roadmap-for-part-2",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Roadmap for Part 2",
    "text": "Roadmap for Part 2\n\n\\(\\downarrow\\) Focus on abstract concepts / terminology\n\\(\\uparrow\\) Focus on data analysis\n(Should hopefully start to feel more like other DSAN classes!)\n(\\(\\Rightarrow\\) Need you to take specific examples I pick and analogize them to your field(s) of interest)",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#working-example-growth-mindset",
    "href": "w08/index.html#working-example-growth-mindset",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Working Example: Growth Mindset(!)",
    "text": "Working Example: Growth Mindset(!)\n\n\n\nFrom Athey and Wager (2019)\nTreatment \\(T\\), called intervention in the dataset: a seminar on growth mindset for high school students\nOutcome \\(Y\\), called achievement_score in the dataset: performance on state‚Äôs standardized test\nIn a perfect world, we could just compute\n\n\\[\n\\mathbb{E}[Y \\mid T = 1] - \\mathbb{E}[Y \\mid T = 0]\n\\]\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nstudent_df &lt;- read_csv(\"assets/learning_mindset.csv\")\n\n\nRows: 10391 Columns: 13\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (13): schoolid, intervention, achievement_score, success_expect, ethnici...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n(mean_df &lt;- student_df |&gt; group_by(intervention) |&gt; summarize(mean_score=mean(achievement_score)))\n\n\n\n\n\n\nintervention\nmean_score\n\n\n\n\n0\n-0.1538030\n\n\n1\n0.3184686\n\n\n\n\n\n\nCode\nstudent_naive_lm &lt;- lm(achievement_score ~ intervention, data=student_df)\nstudent_naive_lm |&gt; broom::tidy() |&gt; select(term, estimate)\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n-0.1538030\n\n\nintervention\n0.4722717\n\n\n\n\n\n\nCode\nstudent_df |&gt;\n  ggplot(aes(x=intervention, y=achievement_score)) +\n  geom_boxplot(\n    aes(group=intervention),\n    width=0.5\n  ) +\n  geom_smooth(\n    method='lm',\n    formula='y ~ x',\n    se=TRUE\n  ) +\n  geom_point(\n    data=mean_df,\n    aes(x=intervention, y=mean_score),\n    size=3\n  ) +\n  theme_dsan(base_size=16)",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#the-problem-pesky-covariates",
    "href": "w08/index.html#the-problem-pesky-covariates",
    "title": "Week 8: Propensity Score Weighting",
    "section": "The Problem: Pesky Covariates",
    "text": "The Problem: Pesky Covariates\n\nHere the ‚Äúblob‚Äù \\(\\mathbf{X}\\) forms a fork, as drawn‚Ä¶\nBut in reality the work of modeling is flying into the cloud and modeling the \\(\\mathbf{X}\\)-\\(T\\) and \\(\\mathbf{X}\\)-\\(Y\\) relationships (especially: figuring out which covariates \\(X_j \\in \\mathbf{X}\\) are colliders), so you can close the backdoors:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis can be really difficult, for a bunch of reasons‚Ä¶ What if there was an easier way?",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#if-we-had-control-over-everything-experiments-vs.-observational-data-analysis",
    "href": "w08/index.html#if-we-had-control-over-everything-experiments-vs.-observational-data-analysis",
    "title": "Week 8: Propensity Score Weighting",
    "section": "If We Had Control Over Everything (Experiments vs.¬†Observational Data Analysis)",
    "text": "If We Had Control Over Everything (Experiments vs.¬†Observational Data Analysis)\n\nIf we could intervene in the DGP, we could assign treatment randomly, thus removing the impact of Covariates on \\(T\\)!\n\n\n\n\n\n\n\nAlas, we are data scientists, not (necessarily) experiment-conductors, plus there are often ethical reasons to not perform experiments!\n‚Ä¶There‚Äôs still another approach!",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#closing-backdoors-the-too-good-to-be-true-way",
    "href": "w08/index.html#closing-backdoors-the-too-good-to-be-true-way",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Closing Backdoors the Too-Good-To-Be-True Way",
    "text": "Closing Backdoors the Too-Good-To-Be-True Way\n\nKey insight from causal thinking: Transformation of the problem from ‚Äúcontrol for all covariates‚Äù to ‚Äúclose all backdoor paths‚Äù‚Ä¶\nFor the goal of just closing these paths, we have an alternative1:\n\n\n\n\nRosenbaum and Rubin (1983): there exists a statistic \\(\\mathtt{e}(\\mathbf{X}) = \\Pr(T \\mid \\mathbf{X})\\), the propensity score, which ‚Äúcaptures‚Äù info in \\(\\textbf{X}\\) relevant to \\(T\\) such that\nConditioning on \\(\\mathtt{e}(\\mathbf{X})\\) closes \\(\\mathbf{X} \\Rightarrow \\mathtt{e}(\\mathbf{X}) \\rightarrow T\\) (\\(\\mathtt{e}(\\mathbf{X})\\) is a pipe)\n\n\n\n\n\n\n\n\n\n\nThis would close backdoor path \\(T \\leftarrow \\mathtt{e}(\\mathbf{X}) \\Leftarrow \\mathbf{X} \\rightarrow Y\\), leaving only direct effect \\(T \\rightarrow Y\\)! There‚Äôs one remaining complication‚Ä¶",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#closing-backdoors-via-propensity-score-estimation",
    "href": "w08/index.html#closing-backdoors-via-propensity-score-estimation",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Closing Backdoors via Propensity Score Estimation",
    "text": "Closing Backdoors via Propensity Score Estimation\n\nSadly we don‚Äôt observe true probability of being treated for all possible values of \\(\\mathbf{X}\\)\nBut, we can derive an estimate \\(\\hat{\\mathtt{e}}(\\mathbf{X})\\) using our machine learning skills üòé\nWe now have that \\(\\hat{\\mathtt{e}}(\\mathbf{X})\\), as a proxy relative to the pipe \\(\\mathbf{X} \\Rightarrow \\mathtt{e}(\\mathbf{X}) \\rightarrow T\\), blocks the pipe to the extent that it captures the true probability \\(\\mathtt{e}(\\mathbf{X}) = \\Pr(T \\mid \\mathbf{X})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBackdoor Path: \\(T \\leftarrow \\mathtt{e}(\\mathbf{X}) \\Leftarrow \\mathbf{X} \\rightarrow Y\\)\nClosed in proportion to \\(\\left[ \\text{Cor}(\\hat{\\mathtt{e}}(\\mathbf{X}), \\mathtt{e}(\\mathbf{X})) \\right]^2 = ‚ùì\\)",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#sometimes-helpful-thought-experiment",
    "href": "w08/index.html#sometimes-helpful-thought-experiment",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Sometimes-Helpful Thought Experiment",
    "text": "Sometimes-Helpful Thought Experiment\n\n\n\nBack in our basic confounding scenario:\nIf there was only one covariate (\\(\\mathbf{X} = X\\)), and it was a constant (\\(\\Pr(X = c) = 1\\)), then all the variation in \\(Y\\) would be due to variation in \\(T\\)\nLess extreme: if person \\(i\\) has covariates \\(\\mathbf{X}_i\\) and person \\(j\\) has covariates \\(\\mathbf{X}_j\\), but \\(\\mathbf{X}_i = \\mathbf{X}_j\\), then variation in their outcomes is due solely to \\(T\\)\n\n\n\n\n\n\n\n\nPart of the logic of propensity score is that, if person \\(i\\) has covariates \\(\\mathbf{X}_i\\) and person \\(j\\) has covariates \\(\\mathbf{X}_j\\), but \\(\\mathtt{e}(\\mathbf{X}_i) = \\mathtt{e}(\\mathbf{X}_j)\\), then \\(i\\) and \\(j\\) are perfectly matched\n\\(\\Rightarrow\\) (by fun math proof) variation in their outcomes is due solely to \\(T\\)",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#how-exactly-do-we-adjust-for-hatmathttemathbfx",
    "href": "w08/index.html#how-exactly-do-we-adjust-for-hatmathttemathbfx",
    "title": "Week 8: Propensity Score Weighting",
    "section": "How Exactly Do We Adjust For \\(\\hat{\\mathtt{e}}(\\mathbf{X})\\)?",
    "text": "How Exactly Do We Adjust For \\(\\hat{\\mathtt{e}}(\\mathbf{X})\\)?\n\n\nSimulation example: smoking reduction\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(Rlab)\nset.seed(5650)\nn &lt;- 250\nmotiv_vals &lt;- runif(n, 0, 1)\nenroll_vals &lt;- ifelse(\n  motiv_vals &lt; 0.25,\n  0,\n  # We know motiv &gt; 0.25\n  ifelse(\n    motiv_vals &gt; 0.75,\n    1,\n    # We know 0.25 &lt; motiv &lt; 0.75\n    rbern(n, prob=(motiv_vals - 0.125)*1.5)\n  )\n)\n\n\nWarning in rbinom(n, size = 1, prob = prob): NAs produced\n\n\nCode\nncigs_vals &lt;- rbinom(n, size=30, prob=0.6-0.2*enroll_vals)\nsmoke_df &lt;- tibble(\n  motiv=motiv_vals,\n  enroll=enroll_vals,\n  ncigs=ncigs_vals\n)\n(smoke_mean_df &lt;- smoke_df |&gt; group_by(enroll) |&gt; summarize(mean_ncigs=mean(ncigs)))\n\n\n\n\n\n\nenroll\nmean_ncigs\n\n\n\n\n0\n17.88060\n\n\n1\n12.26724\n\n\n\n\n\n\nCode\nnaive_smoke_lm &lt;- lm(ncigs ~ enroll, data=smoke_df)\nsummary(naive_smoke_lm) |&gt; broom::tidy() |&gt;\n  select(term, estimate)\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n17.880597\n\n\nenroll\n-5.613356\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsmoke_df |&gt; ggplot(aes(x=enroll, y=ncigs)) +\n  geom_boxplot(\n    aes(group=enroll),\n    width=0.5\n  ) +\n  geom_smooth(\n    method='lm',\n    formula='y ~ x',\n    se=TRUE\n  ) +\n  geom_point(\n    data=smoke_mean_df,\n    aes(x=enroll, y=mean_ncigs),\n    size=3\n  ) +\n  theme_dsan(base_size=24)",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#inverse-probability-of-treatment-weighting",
    "href": "w08/index.html#inverse-probability-of-treatment-weighting",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Inverse Probability-of-Treatment Weighting",
    "text": "Inverse Probability-of-Treatment Weighting\n\n\n\n\nCode\neprop_model &lt;- glm(enroll ~ motiv, family='binomial', data=smoke_df)\neprop_preds &lt;- predict(eprop_model, type=\"response\")\nsmoke_df &lt;- smoke_df |&gt; mutate(pred=eprop_preds)\n# Use the preds to compute IPW\nsmoke_df &lt;- smoke_df |&gt; rowwise() |&gt; mutate(\n  ipw=ifelse(enroll, 1/pred, 1/(1-pred))\n) |&gt; arrange(pred)\n#smoke_df\nsmoke_df |&gt; mutate(enroll=factor(enroll)) |&gt;\n  ggplot(aes(x=motiv, y=ncigs, color=enroll)) +\n  geom_point() +\n  theme_dsan(base_size=24) +\n  labs(title=\"Before Weighting\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsmoke_df |&gt; mutate(enroll=factor(enroll)) |&gt;\n  ggplot(aes(\n    x=motiv, y=ncigs, color=enroll, size=ipw,\n    alpha=log(ipw-1)\n  )) +\n  geom_point() +\n  guides(alpha=\"none\") +\n  theme_dsan(base_size=24) +\n  labs(title=\"After Weighting\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsmoke_df |&gt;\n  ggplot(aes(x=motiv)) +\n  # Predictions\n  geom_point(\n    aes(y=enroll, color=factor(enroll))\n  ) +\n  # Values\n  geom_point(\n    aes(y=pred, color=factor(enroll))\n  ) +\n  labs(color=\"enroll\") +\n  theme_dsan(base_size=24) +\n  labs(title=\"Propensity to Enroll\")\nipw_min &lt;- min(smoke_df$ipw)\nipw_max &lt;- max(smoke_df$ipw)\nsmoke_df &lt;- smoke_df |&gt; mutate(\n  ipw_scaled = (ipw - ipw_min) / (ipw_max - ipw_min)\n)\nsmoke_df |&gt;\n  ggplot(aes(x=motiv)) +\n  # Predictions\n  geom_point(\n    aes(y=enroll, color=factor(enroll))\n  ) +\n  # Values\n  geom_point(\n    aes(y=ipw_scaled, color=factor(enroll))\n  ) +\n  theme_dsan(base_size=24) +\n  labs(\n    title=\"Inverse Probability-of-Treatment Weights (IPTW)\",\n    color=\"enroll\"\n  )\n\n\n\n\n\n‚Üì\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Üí\n\n\n\n\n‚Üë",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#the-final-result",
    "href": "w08/index.html#the-final-result",
    "title": "Week 8: Propensity Score Weighting",
    "section": "The Final Result!",
    "text": "The Final Result!\n\n\nCode\nlm_with_weights &lt;- lm(ncigs ~ enroll,\n  data=smoke_df, weights=smoke_df$ipw\n)\nsummary(lm_with_weights) |&gt; broom::tidy() |&gt;\n  select(term, estimate, std.error)\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n18.10133\n0.2466712\n\n\nenroll\n-5.66453\n0.3571696\n\n\n\n\n\n\n\n\nCode\nlibrary(WeightIt)\nW &lt;- weightit(\n  enroll ~ motiv, data = smoke_df, ps=\"pred\"\n)\nsmoke_weighted_lm &lt;- lm_weightit(\n  ncigs ~ enroll, data = smoke_df, weightit = W\n)\nsummary(smoke_weighted_lm, ci = FALSE)\n\n\n\nCall:\nlm_weightit(formula = ncigs ~ enroll, data = smoke_df, weightit = W)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  18.1013     0.2479   73.02   &lt;1e-06 ***\nenroll       -5.6645     0.5463  -10.37   &lt;1e-06 ***\nStandard error: HC0 robust\n\n\n\n\nCode\nW_default &lt;- weightit(enroll ~ motiv, data = smoke_df)\nsmoke_default_lm &lt;- lm_weightit(\n  ncigs ~ enroll, data = smoke_df,\n  weightit = W_default\n)\nsummary(smoke_default_lm, ci = FALSE)\n\n\n\nCall:\nlm_weightit(formula = ncigs ~ enroll, data = smoke_df, weightit = W_default)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  18.1013     0.2441   74.16   &lt;1e-06 ***\nenroll       -5.6645     0.5444  -10.41   &lt;1e-06 ***\nStandard error: HC0 robust (adjusted for estimation of weights)",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#references",
    "href": "w08/index.html#references",
    "title": "Week 8: Propensity Score Weighting",
    "section": "References",
    "text": "References\n\n\nAthey, Susan, and Stefan Wager. 2019. ‚ÄúEstimating Treatment Effects with Causal Forests: An Application.‚Äù arXiv. https://doi.org/10.48550/arXiv.1902.07409.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1983. ‚ÄúThe Central Role of the Propensity Score in Observational Studies for Causal Effects.‚Äù Biometrika 70 (1): 41‚Äì55. https://doi.org/10.2307/2335942.",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w08/index.html#footnotes",
    "href": "w08/index.html#footnotes",
    "title": "Week 8: Propensity Score Weighting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor reasons we‚Äôll see later (double-robustness), you should ultimately try to achieve both goals!‚Ü©Ô∏é",
    "crumbs": [
      "Week 8: {{< var w08.date-md >}}"
    ]
  },
  {
    "objectID": "w07/slides.html#practice-question-1-bayesian-spellchecking",
    "href": "w07/slides.html#practice-question-1-bayesian-spellchecking",
    "title": "Week 7: Midterm Introduction",
    "section": "Practice Question 1: Bayesian Spellchecking",
    "text": "Practice Question 1: Bayesian Spellchecking\n\nA user types Radom‚Ä¶\nDo they mean Random, Radon, or Radom?"
  },
  {
    "objectID": "w07/slides.html#practice-question-2-modeling-political-opinions-with-pgms",
    "href": "w07/slides.html#practice-question-2-modeling-political-opinions-with-pgms",
    "title": "Week 7: Midterm Introduction",
    "section": "Practice Question 2: Modeling Political Opinions with PGMs",
    "text": "Practice Question 2: Modeling Political Opinions with PGMs\n\nHandling sarcasm!"
  },
  {
    "objectID": "w07/slides.html#part-1-background-french-revolution",
    "href": "w07/slides.html#part-1-background-french-revolution",
    "title": "Week 7: Midterm Introduction",
    "section": "Part 1 Background: French Revolution",
    "text": "Part 1 Background: French Revolution"
  },
  {
    "objectID": "w07/slides.html#part-4-background-first-intifada",
    "href": "w07/slides.html#part-4-background-first-intifada",
    "title": "Week 7: Midterm Introduction",
    "section": "Part 4 Background: First Intifada",
    "text": "Part 4 Background: First Intifada"
  },
  {
    "objectID": "w07/slides.html#references",
    "href": "w07/slides.html#references",
    "title": "Week 7: Midterm Introduction",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "w07/index.html",
    "href": "w07/index.html",
    "title": "Week 7: Midterm Introduction",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#practice-question-1-bayesian-spellchecking",
    "href": "w07/index.html#practice-question-1-bayesian-spellchecking",
    "title": "Week 7: Midterm Introduction",
    "section": "Practice Question 1: Bayesian Spellchecking",
    "text": "Practice Question 1: Bayesian Spellchecking\n\nA user types Radom‚Ä¶\nDo they mean Random, Radon, or Radom?",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#practice-question-2-modeling-political-opinions-with-pgms",
    "href": "w07/index.html#practice-question-2-modeling-political-opinions-with-pgms",
    "title": "Week 7: Midterm Introduction",
    "section": "Practice Question 2: Modeling Political Opinions with PGMs",
    "text": "Practice Question 2: Modeling Political Opinions with PGMs\n\nHandling sarcasm!",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#part-1-background-french-revolution",
    "href": "w07/index.html#part-1-background-french-revolution",
    "title": "Week 7: Midterm Introduction",
    "section": "Part 1 Background: French Revolution",
    "text": "Part 1 Background: French Revolution",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#part-4-background-first-intifada",
    "href": "w07/index.html#part-4-background-first-intifada",
    "title": "Week 7: Midterm Introduction",
    "section": "Part 4 Background: First Intifada",
    "text": "Part 4 Background: First Intifada",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w07/index.html#references",
    "href": "w07/index.html#references",
    "title": "Week 7: Midterm Introduction",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Week 7: {{< var w07.date-md >}}"
    ]
  },
  {
    "objectID": "w10/slides.html#independent-vs.-dependent-variables",
    "href": "w10/slides.html#independent-vs.-dependent-variables",
    "title": "Week 10: Text-as-Data",
    "section": "Independent vs.¬†Dependent Variables",
    "text": "Independent vs.¬†Dependent Variables\n\n\n\n\n\n\n\nStarting point: puzzle in social world!\n\nWhy are fertility rates dropping in these countries?\nWhat explains the move from ‚Äúquietism‚Äù to ‚ÄúPolitical Islam‚Äù?\nWhat behaviors produce positive health outcomes in old age?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndependent Variable / Treatment \\(D\\)\n\n\nWhat happens when‚Ä¶\n\n[Someone gets a degree/internship]\n[There are casualties in a conflict]\n\nStart with independent var \\(\\Rightarrow\\) ‚ÄúWhat are the effects of this cause?‚Äù\n\n\n\nDependent Variable / Outcome \\(Y\\)\n\n\n‚ÄúI wonder what explains this?‚Äù\n\n[Differences in earnings]\n[Probability of news story]\n\nStart with dependent var \\(\\Rightarrow\\) ‚ÄúWhat are the causes of this effect?‚Äù"
  },
  {
    "objectID": "w10/slides.html#birthday-as-instrument",
    "href": "w10/slides.html#birthday-as-instrument",
    "title": "Week 10: Text-as-Data",
    "section": "Birthday as Instrument",
    "text": "Birthday as Instrument\n\nMini-Lab Time!"
  },
  {
    "objectID": "w10/slides.html#discovered-topics-depend-on-the-data",
    "href": "w10/slides.html#discovered-topics-depend-on-the-data",
    "title": "Week 10: Text-as-Data",
    "section": "‚ÄúDiscovered‚Äù Topics Depend on the Data üòü",
    "text": "‚ÄúDiscovered‚Äù Topics Depend on the Data üòü\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Y_i \\mid \\textsf{do}(D_i \\leftarrow 1)\\)\n\\(Y_i \\mid \\textsf{do}(D_i \\leftarrow 0)\\)\n\n\n\n\nPerson 1\nCandidate‚Äôs Morals\nTaxes\n\n\nPerson 2\nCandidate‚Äôs Morals\nTaxes\n\n\nPerson 3\nPolarization\nImmigration\n\n\nPerson 4\nPolarization\nImmigration\n\n\n\n\n\nTable¬†2: From Egami et al. (2022)\n\n\n\n\n\n\n\n\n\n\n\n\nActual Assignment\nOutcome \\(Y_i\\)\n\n\n\n\nPerson 1\n\\(D_1 = 1\\)\nMorals\n\n\nPerson 2\n\\(D_2 = 1\\)\nMorals\n\n\nPerson 3\n\\(D_3 = 0\\)\nImmigration\n\n\nPerson 4\n\\(D_4 = 0\\)\nImmigration\n\n\n\n\n\nTable¬†3: Realized assignments and outcomes in World 1\n\n\n\n\n\n\n\n\n\n\n\nActual Assignment\nOutcome \\(Y_i\\)\n\n\n\n\nPerson 1\n\\(D_1 = 1\\)\nMorals\n\n\nPerson 2\n\\(D_2 = 0\\)\nTaxes\n\n\nPerson 3\n\\(D_3 = 1\\)\nPolarization\n\n\nPerson 4\n\\(D_4 = 0\\)\nImmigration\n\n\n\n\n\nTable¬†4: Realized assignments and outcomes in World 2"
  },
  {
    "objectID": "w10/slides.html#the-solution-sample-splitting",
    "href": "w10/slides.html#the-solution-sample-splitting",
    "title": "Week 10: Text-as-Data",
    "section": "The Solution? Sample Splitting!",
    "text": "The Solution? Sample Splitting!\n\nMachine learning noticed this long ago: the goal is a model that generalizes, not memorizes!"
  },
  {
    "objectID": "w10/slides.html#topic-models",
    "href": "w10/slides.html#topic-models",
    "title": "Week 10: Text-as-Data",
    "section": "Topic Models",
    "text": "Topic Models\n\nIntuition is just: let‚Äôs model latent topics ‚Äúunderlying‚Äù observed words\n\n\n\n\nSection\nKeywords\n\n\n\n\nU.S. News\nstate, court, federal, republican\n\n\nWorld News\ngovernment, country, officials, minister\n\n\nArts\nmusic, show, art, dance\n\n\nSports\ngame, league, team, coach\n\n\nReal Estate\nhome, bedrooms, bathrooms, building\n\n\n\n\nAlready, by just classifying articles based on these keyword counts:\n\n\n\n\n\nArts\nReal Estate\nSports\nU.S. News\nWorld News\n\n\n\n\nCorrect\n3020\n690\n4860\n1330\n1730\n\n\nIncorrect\n750\n60\n370\n1100\n590\n\n\nAccuracy\n0.801\n0.920\n0.929\n0.547\n0.746"
  },
  {
    "objectID": "w10/slides.html#topic-models-as-pgms",
    "href": "w10/slides.html#topic-models-as-pgms",
    "title": "Week 10: Text-as-Data",
    "section": "Topic Models as PGMs",
    "text": "Topic Models as PGMs\n\nFrom Blei (2012)\n‚Ä¶Unlocks a world of social modeling through text!"
  },
  {
    "objectID": "w10/slides.html#cross-sectional-analysis",
    "href": "w10/slides.html#cross-sectional-analysis",
    "title": "Week 10: Text-as-Data",
    "section": "Cross-Sectional Analysis",
    "text": "Cross-Sectional Analysis\nBlaydes, Grimmer, and McQueen (2018)"
  },
  {
    "objectID": "w10/slides.html#influence-over-time",
    "href": "w10/slides.html#influence-over-time",
    "title": "Week 10: Text-as-Data",
    "section": "Influence Over Time",
    "text": "Influence Over Time\n\nFrom Barron et al. (2018)"
  },
  {
    "objectID": "w10/slides.html#textual-influence-over-time",
    "href": "w10/slides.html#textual-influence-over-time",
    "title": "Week 10: Text-as-Data",
    "section": "Textual Influence Over Time",
    "text": "Textual Influence Over Time"
  },
  {
    "objectID": "w10/slides.html#references",
    "href": "w10/slides.html#references",
    "title": "Week 10: Text-as-Data",
    "section": "References",
    "text": "References\n\n\nBarron, Alexander T. J., Jenny Huang, Rebecca L. Spang, and Simon DeDeo. 2018. ‚ÄúIndividuals, Institutions, and Innovation in the Debates of the French Revolution.‚Äù Proceedings of the National Academy of Sciences 115 (18): 4607‚Äì12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBlaydes, Lisa, Justin Grimmer, and Alison McQueen. 2018. ‚ÄúMirrors for Princes and Sultans: Advice on the Art of Governance in the Medieval Christian and Islamic Worlds.‚Äù The Journal of Politics 80 (4): 1150‚Äì67. https://doi.org/10.1086/699246.\n\n\nBlei, David M. 2012. ‚ÄúProbabilistic Topic Models.‚Äù Commun. ACM 55 (4): 77‚Äì84. https://doi.org/10.1145/2133806.2133826.\n\n\nEgami, Naoki, Christian J. Fong, Justin Grimmer, Margaret E. Roberts, and Brandon M. Stewart. 2022. ‚ÄúHow to Make Causal Inferences Using Texts.‚Äù Science Advances 8 (42): eabg2652. https://doi.org/10.1126/sciadv.abg2652."
  },
  {
    "objectID": "w10/index.html",
    "href": "w10/index.html",
    "title": "Week 10: Text-as-Data",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#independent-vs.-dependent-variables",
    "href": "w10/index.html#independent-vs.-dependent-variables",
    "title": "Week 10: Text-as-Data",
    "section": "Independent vs.¬†Dependent Variables",
    "text": "Independent vs.¬†Dependent Variables\n\n\n\n\n\n\n\nStarting point: puzzle in social world!\n\nWhy are fertility rates dropping in these countries?\nWhat explains the move from ‚Äúquietism‚Äù to ‚ÄúPolitical Islam‚Äù?\nWhat behaviors produce positive health outcomes in old age?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndependent Variable / Treatment \\(D\\)\n\n\nWhat happens when‚Ä¶\n\n[Someone gets a degree/internship]\n[There are casualties in a conflict]\n\nStart with independent var \\(\\Rightarrow\\) ‚ÄúWhat are the effects of this cause?‚Äù\n\n\n\nDependent Variable / Outcome \\(Y\\)\n\n\n‚ÄúI wonder what explains this?‚Äù\n\n[Differences in earnings]\n[Probability of news story]\n\nStart with dependent var \\(\\Rightarrow\\) ‚ÄúWhat are the causes of this effect?‚Äù",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#birthday-as-instrument",
    "href": "w10/index.html#birthday-as-instrument",
    "title": "Week 10: Text-as-Data",
    "section": "Birthday as Instrument",
    "text": "Birthday as Instrument\n\nMini-Lab Time!",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#discovered-topics-depend-on-the-data",
    "href": "w10/index.html#discovered-topics-depend-on-the-data",
    "title": "Week 10: Text-as-Data",
    "section": "‚ÄúDiscovered‚Äù Topics Depend on the Data üòü",
    "text": "‚ÄúDiscovered‚Äù Topics Depend on the Data üòü\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Y_i \\mid \\textsf{do}(D_i \\leftarrow 1)\\)\n\\(Y_i \\mid \\textsf{do}(D_i \\leftarrow 0)\\)\n\n\n\n\nPerson 1\nCandidate‚Äôs Morals\nTaxes\n\n\nPerson 2\nCandidate‚Äôs Morals\nTaxes\n\n\nPerson 3\nPolarization\nImmigration\n\n\nPerson 4\nPolarization\nImmigration\n\n\n\n\n\nTable¬†2: From Egami et al. (2022)\n\n\n\n\n\n\n\n\n\n\n\n\nActual Assignment\nOutcome \\(Y_i\\)\n\n\n\n\nPerson 1\n\\(D_1 = 1\\)\nMorals\n\n\nPerson 2\n\\(D_2 = 1\\)\nMorals\n\n\nPerson 3\n\\(D_3 = 0\\)\nImmigration\n\n\nPerson 4\n\\(D_4 = 0\\)\nImmigration\n\n\n\n\n\nTable¬†3: Realized assignments and outcomes in World 1\n\n\n\n\n\n\n\n\n\n\n\nActual Assignment\nOutcome \\(Y_i\\)\n\n\n\n\nPerson 1\n\\(D_1 = 1\\)\nMorals\n\n\nPerson 2\n\\(D_2 = 0\\)\nTaxes\n\n\nPerson 3\n\\(D_3 = 1\\)\nPolarization\n\n\nPerson 4\n\\(D_4 = 0\\)\nImmigration\n\n\n\n\n\nTable¬†4: Realized assignments and outcomes in World 2",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#the-solution-sample-splitting",
    "href": "w10/index.html#the-solution-sample-splitting",
    "title": "Week 10: Text-as-Data",
    "section": "The Solution? Sample Splitting!",
    "text": "The Solution? Sample Splitting!\n\nMachine learning noticed this long ago: the goal is a model that generalizes, not memorizes!",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#topic-models",
    "href": "w10/index.html#topic-models",
    "title": "Week 10: Text-as-Data",
    "section": "Topic Models",
    "text": "Topic Models\n\nIntuition is just: let‚Äôs model latent topics ‚Äúunderlying‚Äù observed words\n\n\n\n\nSection\nKeywords\n\n\n\n\nU.S. News\nstate, court, federal, republican\n\n\nWorld News\ngovernment, country, officials, minister\n\n\nArts\nmusic, show, art, dance\n\n\nSports\ngame, league, team, coach\n\n\nReal Estate\nhome, bedrooms, bathrooms, building\n\n\n\n\nAlready, by just classifying articles based on these keyword counts:\n\n\n\n\n\nArts\nReal Estate\nSports\nU.S. News\nWorld News\n\n\n\n\nCorrect\n3020\n690\n4860\n1330\n1730\n\n\nIncorrect\n750\n60\n370\n1100\n590\n\n\nAccuracy\n0.801\n0.920\n0.929\n0.547\n0.746",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#topic-models-as-pgms",
    "href": "w10/index.html#topic-models-as-pgms",
    "title": "Week 10: Text-as-Data",
    "section": "Topic Models as PGMs",
    "text": "Topic Models as PGMs\n\n\n\nFrom Blei (2012)\n\n\n\n‚Ä¶Unlocks a world of social modeling through text!",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#cross-sectional-analysis",
    "href": "w10/index.html#cross-sectional-analysis",
    "title": "Week 10: Text-as-Data",
    "section": "Cross-Sectional Analysis",
    "text": "Cross-Sectional Analysis\nBlaydes, Grimmer, and McQueen (2018)",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#influence-over-time",
    "href": "w10/index.html#influence-over-time",
    "title": "Week 10: Text-as-Data",
    "section": "Influence Over Time",
    "text": "Influence Over Time\n\n\n\nFrom Barron et al. (2018)",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#textual-influence-over-time",
    "href": "w10/index.html#textual-influence-over-time",
    "title": "Week 10: Text-as-Data",
    "section": "Textual Influence Over Time",
    "text": "Textual Influence Over Time",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "w10/index.html#references",
    "href": "w10/index.html#references",
    "title": "Week 10: Text-as-Data",
    "section": "References",
    "text": "References\n\n\nBarron, Alexander T. J., Jenny Huang, Rebecca L. Spang, and Simon DeDeo. 2018. ‚ÄúIndividuals, Institutions, and Innovation in the Debates of the French Revolution.‚Äù Proceedings of the National Academy of Sciences 115 (18): 4607‚Äì12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBlaydes, Lisa, Justin Grimmer, and Alison McQueen. 2018. ‚ÄúMirrors for Princes and Sultans: Advice on the Art of Governance in the Medieval Christian and Islamic Worlds.‚Äù The Journal of Politics 80 (4): 1150‚Äì67. https://doi.org/10.1086/699246.\n\n\nBlei, David M. 2012. ‚ÄúProbabilistic Topic Models.‚Äù Commun. ACM 55 (4): 77‚Äì84. https://doi.org/10.1145/2133806.2133826.\n\n\nEgami, Naoki, Christian J. Fong, Justin Grimmer, Margaret E. Roberts, and Brandon M. Stewart. 2022. ‚ÄúHow to Make Causal Inferences Using Texts.‚Äù Science Advances 8 (42): eabg2652. https://doi.org/10.1126/sciadv.abg2652.",
    "crumbs": [
      "Week 10: {{< var w10.date-md >}}"
    ]
  },
  {
    "objectID": "final.html",
    "href": "final.html",
    "title": "Final Project Specifications",
    "section": "",
    "text": "Changelog\n\n\n\n\n[Sunday, July 12, 11pm] Updated examples box for each option to reflect that HW3A and HW3B will walk you through examples of these two approaches, respectively.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final.html#overview",
    "href": "final.html#overview",
    "title": "Final Project Specifications",
    "section": "Overview",
    "text": "Overview\nOur goal is to make the final project as open-ended as possible, to give you the space to explore any particular topic that may have piqued your interest throughout the semester! At the same time, we hope to provide you with guidance and mentorship so that you don‚Äôt feel lost as to how to start, how to proceed, and/or what to submit for the final deliverable!1\nFor each lecture (especially from Week 8 onwards), our hope is that there are some next step(s) that come to mind, that you feel like you can follow to move from learning the concepts covered in class to applying them towards some social phenomenon of interest to you! Though exceptions are always welcome (see previous paragraph), here we outline two general paths you can take, with examples under each heading that you can use as inspiration:\n\n\n\n\n\n\nOption 1: Modeling Social Phenomena with PGMs\n\n\n\nMoving from vaguely-thinking-about to formally modeling\n\nExample 1.1: HW3A walks you through how PGMs can be used to ‚Äúlink‚Äù survey-based and experimental studies of the same social phenomenon (employer discrimination on the basis of criminal record and race)\nExample 1.2: Writeup on Modeling Cahiers de Dol√©ances (Coming soon!)\n\n\n\n\n\n\n\n\n\n\nOption 2: Pushing Towards the Asymptote of Causality\n\n\n\nTaking an existing associational analysis and tackling confounders\n\nExample 2.1: HW3B walks you through how propensity score weighting can be used to enhance an existing associational analysis, by taking a ‚Äúfirst step‚Äù towards removing confounders\nExample 2.2: Writeup on improving estimates of labor market monopsony (Coming soon!)",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final.html#concrete-requirements",
    "href": "final.html#concrete-requirements",
    "title": "Final Project Specifications",
    "section": "Concrete Requirements",
    "text": "Concrete Requirements\nFor the two options described above, the following info boxes describe the structure of the deliverable(s) you‚Äôre responsible for:\n\n\n\n\n\n\nOption 1: Modeling Social Phenomena with PGMs\n\n\n\n\n\nMoving from vaguely-thinking-about to formally modeling\n(Coming soon!)\n\n\n\n\n\n\n\n\n\nOption 2: Associational \\(\\leadsto\\) Causal Analysis\n\n\n\n\n\nFiguring out how to address some existing drawback of the algorithm/data structure\nThe structure of your deliverable should be as follows:\n\nTo begin, you should summarize existing literature that you‚Äôre building upon: this could be, e.g., a summary of a past project you‚Äôve done, or of a published paper or series of published papers that you think fail to address causal concerns as-is\nThen, you should explain exactly what the ‚Äúmissing pieces‚Äù are, in terms of bridging the gap between associational and causal analysis: what covariates have not been properly accounted for, for example?\nYou should then explicitly state a course topic which can be used to address this shortcoming, and justify its use: for example, can a propensity score-based analysis be used to balance the treatment and control groups?\n\nFor example, recall the point from Week 8 that propensity scores can only be used when there is sufficient common support between the treatment and control groups in a dataset! In that week‚Äôs simulated smoking-cessation example, the DGP for the relationship between motivation and enrollment gave rise to a scenario where we did not have common support for the lowest and highest 25% of people with respect to motivation score (since nobody from the lowest 25% enrolled, while everybody from the highest 25% enrolled). We did, however, have common support for the middle 50%, since this middle 50% had a mixture of enrollers and non-enrollers. For your project, then, if you choose to pursue a propensity score-based analysis, you‚Äôll need to be explicit about the specific subset(s) of your population for which the propensity-weighted estimate(s) are valid.\n\n\n(Remainder coming soon!)",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final.html#individual-vs.-group-projects",
    "href": "final.html#individual-vs.-group-projects",
    "title": "Final Project Specifications",
    "section": "Individual vs.¬†Group Projects",
    "text": "Individual vs.¬†Group Projects\nIt is totally up to you whether you‚Äôd like to do the project individually or in a group with other students. However, if you are pursuing the project as a group, please choose one member of the group to serve as the ‚Äúproject lead‚Äù, and include this detail in an email to your mentor.\nThe mentor for the group project will then be whoever was assigned as the individual mentor for the project leader (this choice doesn‚Äôt have to be related to the actual work on the project, it is just for us to be able to allocate mentees fairly between the course staff!).\nExpectations for group projects will scale based on the number of members in the group: for example, a group with two members will be expected to carry out a more substantive project, such that it requires approximately two times the amount of work that would be expected for individual projects2",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final.html#timeline",
    "href": "final.html#timeline",
    "title": "Final Project Specifications",
    "section": "Timeline",
    "text": "Timeline\n\nProposal (Abstract on Notion):\n\nSubmitted to instructors by Tuesday, July 15th, 6:30pm EDT\nApproved by an instructor by Tuesday, July 22nd, 6:30pm EDT\n\nFinal Draft:\n\nSubmitted to instructors for review by Friday, August 1st, 5:59pm EDT\nApproved by an instructor by Monday, August 4th, 11:59pm EDT\n\nFinal Submission:\n\nSubmitted via Canvas by Friday, August 8th, 5:59pm EDT",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final.html#submission-format",
    "href": "final.html#submission-format",
    "title": "Final Project Specifications",
    "section": "Submission Format",
    "text": "Submission Format\nThe course‚Äôs Canvas page now has a ‚ÄúFinal Project‚Äù assignment, where you will upload your final submission for grading. The following is a rough sketch of what we‚Äôre looking for in terms of the structure of your submission:\n\nHTML format, as a rendered Quarto manuscript, would be optimal, but can be PDF if preferred‚Äîfor example, if you choose Option 4 (involving mathematical proofs), you might instead want to use LaTeX rendered to PDF.\nA requirement in terms of number of pages is difficult, but a reasonable range for the PDF format would be 3-10 pages double-spaced. Therefore, for a Quarto document or Jupyter notebook, the length can be the equivalent of this (for example, you can print-preview the Quarto doc to see how many pages it would produce if printed)\nIt should have an abstract, a 250-500 word paragraph at the very top of the manuscript, summarizing what you did (this can be copied from your Notion abstract, or updated as needed!)\nCitations should be set up so that they‚Äôre handled automatically. By Quarto‚Äôs citation manager for example, or by Bibtex/Biber if you‚Äôre using LaTeX.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final.html#references",
    "href": "final.html#references",
    "title": "Final Project Specifications",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "final.html#footnotes",
    "href": "final.html#footnotes",
    "title": "Final Project Specifications",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you can‚Äôt tell, my whole educational philosophy here is just the Montessori system‚Äîthis approach was originally developed for younger (primary school) children, but lots and lots of recent educational research indicates that it‚Äôs an actually an extremely effective way to learn, and to motivate self-learning, for people of any age üòé‚Ü©Ô∏é\nThis detail is not something we‚Äôre trying to explicitly measure or be harsh about, but is included here since otherwise (if the expectations for individual and group projects were the exact same) the work would scale the other way: that each person in a two-person project would be doing half the amount of work that a person doing an individual project is doing‚Ä¶ Hopefully that makes sense from a fairness perspective!.‚Ü©Ô∏é",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "writeups/bayesian-workflow/Bayesian_Workflow.html",
    "href": "writeups/bayesian-workflow/Bayesian_Workflow.html",
    "title": "[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure",
    "section": "",
    "text": "%%html\n&lt;style&gt;\n@import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100..900;1,100..900&display=swap');\n.jp-RenderedHTMLCommon {\n    font-family: \"Roboto\", sans-serif;\n}\n&lt;/style&gt;\nimport os\nos.environ[ 'MPLCONFIGDIR' ] = './tmp/'\n%config IPCompleter.use_jedi = False\n\nimport pandas as pd\nimport numpy as np\nrng = np.random.default_rng(seed=5660)\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\n# sns.set_style('whitegrid')\ncb_palette = ['#e69f00','#56b4e9','#009e73']\nsns.set_palette(cb_palette)\nimport patchworklib as pw;\n\nimport pymc as pm\nimport arviz as az\n\n&lt;Figure size 100x100 with 0 Axes&gt;"
  },
  {
    "objectID": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-1-elden-coin-the-immersive-coin-flipping-rpg-thats-taking-the-world-by-storm",
    "href": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-1-elden-coin-the-immersive-coin-flipping-rpg-thats-taking-the-world-by-storm",
    "title": "[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure",
    "section": "[Part 1] Elden Coin: The Immersive Coin-Flipping RPG That‚Äôs Taking the World By Storm!",
    "text": "[Part 1] Elden Coin: The Immersive Coin-Flipping RPG That‚Äôs Taking the World By Storm!\n\n[Part 1.1] The Rules of the Game\n\nA friend \\(B\\) challenges you (\\(A\\)) to a match of Elden Coin (\\(B\\) provides their Elden Coin, as well as the table to play on, which is a key part of the game), a tabletop coin-flipping game\nA match is a series of 50 rounds\nAt the start of each round, both you and \\(B\\) put one Euro (‚Ç¨1) into the pot on the side of the table, then write a big ‚Äú0‚Äù in chalk on the other side\nThen, the round consists of you and \\(B\\) taking turns flipping the Elden Coin in the center of the table until you‚Äôve reached 13 total flips\nEvery time a flip comes up Heads, you add one to the number written in chalk (by erasing and rewriting it)\nAt the end of the round:\n\nIf the number of heads written down is above 6, you (\\(A\\)) win the entire pot (‚Ç¨2), meaning that you gain an additional Euro above the amount you entered the round with.\nOtherwise, \\(B\\) wins the entire pot, meaning that you lose one Euro relative to the amount you entered the round with.\n\n\nThe following cell defines a Game class, that we can use to store the different parameters we‚Äôll need when simulating a match.\n\nclass Game:\n    flips_per_round = 13\n    num_heads_range = (0, flips_per_round)\n    num_heads_support = list(range(0, flips_per_round + 1))\n    win_cutoff = 6\n    rounds_per_match = 50\n\nThen, on top of just the rules of the game itself, here we define a separate set of globals ‚Äúoutside of‚Äù the game, specifying the number of simulations of the game we‚Äôd like to run in our simulation cells (so that, for example, you can set it to lower values if running on a slower computer or if you just want the cells to run more quickly!)\n\nclass MyGlobals:\n    # If we're trying to get a distribution over what might happen in *one round*\n    # of the game (with outcomes Win or Lose), we'll use this many simulations\n    rounds_to_sim = 10_000\n    # Otherwise, for obtaining a distribution over what might happen in a *match*\n    # (with the outcome being the total amount of money won or lost), we'll use\n    # this many\n    matches_to_sim = 1_000\n\n\n\n[Part 1.2] Simulating a Round\nIf we truly wanted to work at the most fine-grained level, we could simulate individual coin flips, one-by-one\n\ndef sim_round_flips(p):\n    round_flips = rng.choice([0,1],p=[1-p, p], size=Game.flips_per_round)\n    return round_flips\nprint(sim_round_flips(0.5))\nprint(sim_round_flips(0.85))\n\n[0 0 1 0 0 0 0 0 1 0 1 0 1]\n[1 0 1 1 1 1 1 1 1 1 0 0 1]\n\n\nThen, to check the outcome at the level of number-of-heads, we could just use sum():\n\nsum(sim_round_flips(0.5))\n\n6\n\n\nBut, if we‚Äôre going to just sum up the individual coin flips anyways, we can achieve a pretty massive speed improvement (not noticeable for only one or a few rounds, but massive once we move to the level of simulating thousands of rounds) by using rng.binomial(), that is, by operating at the coarser-grained level of generating [number of heads out of 9 flips] rather than individual 0/1 heads/tails values:\n\ndef sim_round(p):\n    num_heads = int(rng.binomial(n=Game.flips_per_round, p=p, size=1))\n    return num_heads\nsim_round(0.5)\n\n6\n\n\nSo, let‚Äôs do some simulations (we‚Äôll use MyGlobals.rounds_to_sim, set above, as our number of simulations \\(N\\)), by running this function a bunch of times, to see how often the number of heads is above 6 when the coin is fair‚Ä¶\n\ndef get_counts(df):\n    support_df = pd.DataFrame({'num_heads': Game.num_heads_support})\n    count_df = df['num_heads'].value_counts().reset_index().sort_values(by='num_heads')\n    merged_df = support_df.merge(count_df, on='num_heads', how='left')\n    merged_df['win'] = merged_df['num_heads'] &gt; Game.win_cutoff\n    merged_df['count'] = merged_df['count'].replace(np.nan, 0).astype(int)\n    merged_df['Pr(num_heads)'] = merged_df['count'] / merged_df['count'].sum()\n    return merged_df\n\nfair_sim_result = [sim_round(0.5) for _ in range(MyGlobals.rounds_to_sim)]\nfair_sim_df = pd.DataFrame({'num_heads': fair_sim_result, 'params': 'p = 0.5'})\nfair_sim_df['win'] = fair_sim_df['num_heads'] &gt; Game.win_cutoff\nfair_count_df = get_counts(fair_sim_df)\nfair_count_df.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\nnum_heads\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\ncount\n0\n22\n116\n376\n874\n1593\n1990\n2065\n1577\n890\n382\n101\n13\n1\n\n\nwin\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\nPr(num_heads)\n0.0\n0.0022\n0.0116\n0.0376\n0.0874\n0.1593\n0.199\n0.2065\n0.1577\n0.089\n0.0382\n0.0101\n0.0013\n0.0001\n\n\n\n\n\n\n\nAnd we can plot the distribution of these simulated outcomes, assigning different colors to the outcomes where we win and where we lose:\n\nax = pw.Brick(figsize=(3.5,2.25))\ng = sns.barplot(\n    x=\"num_heads\", y=\"Pr(num_heads)\", hue=\"win\", data=fair_count_df,\n    alpha=0.75, ax=ax\n);\nax.axvline(x=6.5, ls='dashed', color='black', alpha=0.9);\nax.set_title(f\"N = {len(fair_sim_result)} Simulated Rounds, p = 0.5\")\nax.savefig()\n\n\n\n\n\n\n\n\nTo make it even more straightforward to see the fairness of the game (given a fair coin!), we can just plot win and loss proportions directly:\n\nfair_win_df = fair_count_df[['win','count']].groupby(\"win\").sum().reset_index()\nfair_win_df\n\n\n\n\n\n\n\n\nwin\ncount\n\n\n\n\n0\nFalse\n4971\n\n\n1\nTrue\n5029\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5,2.25))\nwin_plot = sns.barplot(\n    x=\"win\", y=\"count\", hue=\"win\",\n    data=fair_win_df,\n    alpha=0.75, ax=ax, legend=False\n);\nwin_plot.set_title(\"Wins vs. Losses\")\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n[Part 1.3] Checking the Math\nSo far, individual rounds of this game look pretty fair. But, can we be mathematically sure they are?\nLet‚Äôs say we didn‚Äôt even trust this \\(N = 10000\\) simulation. Something that I think is good to have in your toolkit, for checking the math on this stuff without needing to take out a piece of paper, is a symbolic mathematics system!\nSymPy, in this case, can be used to derive exact closed-form solutions (when possible) to probability theory problems, so that you don‚Äôt even have to depend on running simulations for simple enough problems! You can find the documentation for the SymPy stats module here, but the following cells should give you the gist as we check the math of the game‚Äôs fairness.\nAs a quick demonstration of why SymPy is useful, let‚Äôs say we‚Äôre trying to remember the formula for the sum of the first \\(n\\) natural numbers‚Ä¶ Is Python able to help us here on its own?\n\ndef sum_up_to(n):\n    return sum(range(1, n+1))\n\n\nsum_df = pd.DataFrame({'n': list(range(1, 7))})\nsum_df['sum_1_to_n'] = sum_df['n'].apply(sum_up_to)\nsum_df.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nn\n1\n2\n3\n4\n5\n6\n\n\nsum_1_to_n\n1\n3\n6\n10\n15\n21\n\n\n\n\n\n\n\nOne option would be to stare at this and try to figure out the pattern until we remember‚Ä¶ Which is fine but, my brain is too tired to do that, which is why I wanted the computer to do it for me in the first place! Instead, let‚Äôs let SymPy try it:\n\nfrom sympy import expand, oo, plot, simplify, symbols, Eq, Rational, Sum\n\n\ni = symbols('i', integer=True)\nn = symbols('n', integer=True)\nmy_sum = Sum(i, (i, 1, n))\nmy_sum\n\n\\(\\displaystyle \\sum_{i=1}^{n} i\\)\n\n\nThis is the thing we‚Äôre trying to remember the formula for‚Ä¶ so let‚Äôs as SymPy to .doit()!\n\nmy_result = my_sum.doit()\nmy_result\n\n\\(\\displaystyle \\frac{n^{2}}{2} + \\frac{n}{2}\\)\n\n\n‚Ä¶Already very cool, but we can get it in an even more familiar form by using simplify() as well:\n\nsimplify(my_result)\n\n\\(\\displaystyle \\frac{n \\left(n + 1\\right)}{2}\\)\n\n\nAnd we can ask it to derive exact, closed-form solutions for a bunch of other things too! For now, we can try sums of squares‚Ä¶\n\nsimplify(Sum(i**2, (i, 1, n)).doit())\n\n\\(\\displaystyle \\frac{n \\left(2 n^{2} + 3 n + 1\\right)}{6}\\)\n\n\nOr the sum of a geometric series \\(\\sum_{i=1}^{\\infty}ar^i\\)‚Ä¶\n\na = symbols('a', nonnegative=True)\nr = symbols('r', nonnegative=True)\nmy_geo_sum = Sum(a * (r ** i), (i, 0, oo))\nmy_geo_result = my_geo_sum.doit()\nmy_geo_result\n\n\\(\\displaystyle a \\left(\\begin{cases} \\frac{1}{1 - r} & \\text{for}\\: r &lt; 1 \\\\\\sum_{i=0}^{\\infty} r^{i} & \\text{otherwise} \\end{cases}\\right)\\)\n\n\nAnd now that we have the formula, we could plug in a specific value for \\(r\\) as well (here \\(r = \\frac{1}{5}\\)):\n\nmy_geo_result.subs(r, Rational(1,5))\n\n\\(\\displaystyle \\frac{5 a}{4}\\)\n\n\nNow we can import from the sympy.stats module, and compute exact values for the probabilities of each possible round result!\n\nfrom sympy.stats import P, E, density, Bernoulli, DiscreteUniform\n\nDefining a sequence of i.i.d. RVs in SymPy is a bit tough in general (meaning, for example, it‚Äôd be tough to get it to prove the Central Limit Theorem), but, for simple cases like this where we‚Äôre summing up a finite number of coin flips, the following works!\nWe‚Äôll use Z to represent the sum of the 13 coin flips, or in other words, the number of heads in a round of Elden Coin, now modeled exactly (without rounding or approximations), rather than simulated:\n\ncoins = [Bernoulli(f'X{i}', p=Rational(1,2)) for i in range(0, Game.flips_per_round)]\nZ = sum(coins)\nZ\n\n\\(\\displaystyle X_{0} + X_{1} + X_{10} + X_{11} + X_{12} + X_{2} + X_{3} + X_{4} + X_{5} + X_{6} + X_{7} + X_{8} + X_{9}\\)\n\n\nWe can use the E() function imported above to get the exact expected value of the sum of 13 coin flips:\n\nE(Z)\n\n\\(\\displaystyle \\frac{13}{2}\\)\n\n\nAnd then the P() function to compute probabilities. For example, we can use Python‚Äôs built-in inequality operator &gt; in this case to find the thing we‚Äôre looking for: the game is fair if \\(\\Pr(Z &gt; 6)\\) is exactly \\(1/2\\):\n\nP(Z &gt; Game.win_cutoff)\n\n\\(\\displaystyle \\frac{1}{2}\\)\n\n\nAs a warning if you decide to use SymPy for other stuff, though: it does not work well with Python‚Äôs equality operator ==! For example, if you want to find the probability of a dice roll coming up 5, you might think of the following:\n\nD = DiscreteUniform('D', list(range(1, 7)))\nP(D == 5)\n\n\\(\\displaystyle 0\\)\n\n\nAwful‚Ä¶ üòµ Instead, we have to import SymPy‚Äôs symbolic version of ==, namely, the equality function Eq(). If we use this instead of ==, we‚Äôll (thankfully) get the expected result!\n\nP(Eq(D, 5))\n\n\\(\\displaystyle \\frac{1}{6}\\)\n\n\nOk! Now that you have a feel for SymPy, as a final helpful use for it here let‚Äôs derive the exact probability values for the different outcomes of a round of Elden Coin. For that, we can use SymPy‚Äôs density() function on a Random Variable, in this case on \\(Z\\):\n\npmf_Z = density(Z)\npmf_Z\n\n{0: 1/8192,\n 1: 13/8192,\n 2: 39/4096,\n 3: 143/4096,\n 4: 715/8192,\n 5: 1287/8192,\n 6: 429/2048,\n 7: 429/2048,\n 8: 1287/8192,\n 9: 715/8192,\n 10: 143/4096,\n 11: 39/4096,\n 12: 13/8192,\n 13: 1/8192}\n\n\nAnd then, for easier viewing/plotting, we can form a Pandas DataFrame from these values:\n\nZ_df = pd.DataFrame({'X':pmf_Z.keys(), 'Pr(X)':pmf_Z.values()})\nZ_df.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\nX\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nPr(X)\n1/8192\n13/8192\n39/4096\n143/4096\n715/8192\n1287/8192\n429/2048\n429/2048\n1287/8192\n715/8192\n143/4096\n39/4096\n13/8192\n1/8192\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5,2.25))\nax.stem(\"X\", \"Pr(X)\", data=Z_df, basefmt='grey');\nax.set_title(\"Exact Values of Distribution\");\nax.set_xlabel(\"Num Heads\");\nax.set_ylabel(\"Probability\")\n# ax.grid(False);\n#ax.set_facecolor('white');\nax.savefig()\n\n\n\n\n\n\n\n\nSadly though, if you‚Äôre like me and you prefer seaborn to ‚Äúbase‚Äù matplotlib, seaborn doesn‚Äôt work well with the exact-numeric-value format of SymPy for fractions. So, for seaborn plotting you‚Äôll have to convert to (non-exact) Python float values. But still, you‚Äôll be plotting the closest possible float approximation to the true values!\n\napprox_df = Z_df.copy()\napprox_df['X'] = approx_df['X'].astype(int)\napprox_df['Pr(X)'] = approx_df['Pr(X)'].astype(float)\napprox_df.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\nX\n0.000000\n1.000000\n2.000000\n3.000000\n4.00000\n5.000000\n6.000000\n7.000000\n8.000000\n9.00000\n10.000000\n11.000000\n12.000000\n13.000000\n\n\nPr(X)\n0.000122\n0.001587\n0.009521\n0.034912\n0.08728\n0.157104\n0.209473\n0.209473\n0.157104\n0.08728\n0.034912\n0.009521\n0.001587\n0.000122\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5,2.25))\nsns.barplot(\n    x=\"X\", y=\"Pr(X)\", data=approx_df,\n    ax=ax, alpha=0.75\n);\nax.set_title(\"(Approximately-Exact) Values of Distribution\")\n# ax.grid(False);\n#ax.set_facecolor('white');\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n[Part 1.4] Simulating Many Rounds\nNow that we‚Äôve convinced ourselves, both via simulation and exact-math, that a single round is fair, let‚Äôs move to simulating matches, comprised of 50 rounds! This is what we really care about, since it will tell us the range our winnings/losings might take on at the end of a full match.\nThe functions in the following cell carry out simulations at more coarse-grained levels than an individual round:\nsim_match(p) simulates a 50-round match and produces a Pandas DataFrame with the results:\n\nt reprsents the round number (We add a special \\(t = 0\\) ‚Äúround‚Äù to mark the fact that we start with $0 at the beginning of the game, which will make the plots below more easy to read)\nnum_heads represents the number of heads at round t\nmoney_change is either \\(-1\\) or \\(1\\): \\(-1\\) if num_heads was less than 6, and \\(1\\) otherwise, representing the net change in your money at the end of the round\nmoney_at_round_end represents the cumulative sum of money_change from round to round, so that at any round this contains the current overall winnings/losings relative to the 0 ‚Ç¨ starting point at \\(t = 0\\)\n\nsim_outcome(p) allows us to get a sense for the ranges of values that can come out of a game, by simulating MyGlobals.matches_to_sim matches, and returning two Pandas DataFrames giving us different pieces of information:\n\nIn final_df, each row represents a single round within one of the simulated matches. We can use this to track the trajectory of money over the course of a match, and then, since we‚Äôre simulating many matches, we‚Äôll in fact get a distribution over trajectories, which will help us visualize our expectations about how much money we may win or lose in a given match.\nIn end_df, each row represents the final round of a simulated match. As we‚Äôll see in the plots below, this allows us to visualize the distribution over final winnings/losings we can expect to see for a given Elden Coin.\n\nFinally, get_result_label(money_at_end) is just a helper function for sim_outcome(p): it allows us to ‚Äúmap‚Äù the money_at_round_end values from sim_match() to a three-level Random Variable called result, which will have value:\n\n\"Gained Money\" if we ended the match with more money than we started with,\n\"Lost Money\" if we ended the match with less money than we started with, and\n\"Broke Even\" if we ended the match with exactly the same amount of money that we started with.\n\n\ndef sim_match(p):\n    round_outcomes = rng.binomial(n=Game.flips_per_round, p=p, size=Game.rounds_per_match)\n    match_df = pd.DataFrame({'t': range(1,Game.rounds_per_match+1), 'num_heads': round_outcomes})\n    match_df['money_change'] = -1 + 2 * (match_df['num_heads'] &gt; Game.win_cutoff)\n    match_df['money_at_round_end'] = match_df['money_change'].cumsum()\n    t0_df = pd.DataFrame({'t':[0],'money_at_round_end':[0],'money_change':[0]})\n    match_df = pd.concat([t0_df, match_df])\n    return(match_df)\n\nresult_order = ['Lost Money', 'Broke Even', 'Gained Money']\ndef get_result_label(money_at_end):\n    if money_at_end &gt; 0:\n        return \"Gained Money\"\n    if money_at_end &lt; 0:\n        return \"Lost Money\"\n    return \"Broke Even\"\n\ndef sim_outcome(p):  \n    all_match_dfs = []\n    for m in range(MyGlobals.matches_to_sim):\n        match_df = sim_match(p)\n        match_df['match_num'] = m\n        all_match_dfs.append(match_df)\n    # All the data\n    final_df = pd.concat(all_match_dfs)\n    final_df.insert(0, 'match_num', final_df.pop('match_num'))\n    final_df['params'] = f\"p = {p}\"\n    # Just the last round money_at_end\n    end_df = final_df[final_df['t'] == Game.rounds_per_match].copy()\n    end_df = end_df[['match_num','t','money_at_round_end']]\n    end_df['result'] = end_df['money_at_round_end'].apply(get_result_label)\n    end_df['result'] = pd.Categorical(\n        end_df['result'],\n        ordered=True,\n        categories=result_order\n    )\n    end_df['params'] = f\"p = {p}\"\n    return final_df, end_df\nfair_final_df, fair_end_df = sim_outcome(0.5)\nprint(\"fair_final_df:\")\ndisplay(fair_final_df.head())\nprint(\"fair_end_df:\")\ndisplay(fair_end_df.head())\n\nfair_final_df:\n\n\n\n\n\n\n\n\n\nmatch_num\nt\nmoney_at_round_end\nmoney_change\nnum_heads\nparams\n\n\n\n\n0\n0\n0\n0\n0\nNaN\np = 0.5\n\n\n0\n0\n1\n1\n1\n8.0\np = 0.5\n\n\n1\n0\n2\n0\n-1\n6.0\np = 0.5\n\n\n2\n0\n3\n1\n1\n9.0\np = 0.5\n\n\n3\n0\n4\n2\n1\n8.0\np = 0.5\n\n\n\n\n\n\n\nfair_end_df:\n\n\n\n\n\n\n\n\n\nmatch_num\nt\nmoney_at_round_end\nresult\nparams\n\n\n\n\n49\n0\n50\n2\nGained Money\np = 0.5\n\n\n49\n1\n50\n-10\nLost Money\np = 0.5\n\n\n49\n2\n50\n-4\nLost Money\np = 0.5\n\n\n49\n3\n50\n16\nGained Money\np = 0.5\n\n\n49\n4\n50\n-2\nLost Money\np = 0.5\n\n\n\n\n\n\n\nWith fair_final_df and fair_end_df now ready for use, we can plot the range of trajectories (from \\(t = 0\\) to \\(t = 50\\)) that a match might exhibit, as well as the distribution of final winnings/losings that we may obtain at the end of a match under the specified Elden Coin value of \\(p\\).\nFor now we‚Äôll plot what matches with a fair coin might look like, but in the next section we‚Äôll produce the same kind of plot for a biased coin!\nThe most straightforward way to visualize the trajectories is probably just to plot every ‚Äúpath‚Äù through the game space, with a low alpha value so that the more-frequent paths appear as a darker orange:\n\ndef plot_trajectories(final_df, end_df, suptitle, custom_ylim=None):\n    if custom_ylim is None:\n        grid = sns.JointGrid(height=4.5);\n    else:\n        grid = sns.JointGrid(height=4.5, ylim=custom_ylim);\n    fair_trajectory_plot = sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", data=final_df, ax=grid.ax_joint,\n        units='match_num', estimator=None, alpha=0.02,\n    );\n    # Breaking-even line\n    grid.refline(\n        y=0, color='black', lw=1, ls='solid', alpha=0.5, label=\"Breaking Even\"\n    );\n    # Add the median line in dashed black\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", # hue=\"source\",\n        data=final_df, ax=grid.ax_joint,\n        label=\"Median\",\n        estimator='median',\n        errorbar=None,\n        # errorbar=('pi',90),\n        color='black',\n        lw=1, ls='dashed',\n        err_kws=dict(alpha=0.15),\n    );\n    # And mean line in dotted black\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", # hue=\"source\",\n        data=final_df, ax=grid.ax_joint,\n        label=\"Mean\",\n        estimator='mean',\n        errorbar=None,\n        # errorbar=('pi',90),\n        color='black',\n        lw=1, ls='dotted',\n        err_kws=dict(alpha=0.15),\n    );\n    grid.ax_marg_x.remove();\n    grid.ax_marg_y.axhline(\n        y=end_df['money_at_round_end'].mean(),\n        color='#e69f00', lw=2\n    );\n    # Marginal axis\n    winnings_plot = sns.histplot(\n        y=\"money_at_round_end\", data=end_df,\n        ax=grid.ax_marg_y,\n        discrete=True,\n    );\n    # Dashed line on margin for median\n    grid.ax_marg_y.axhline(\n        y=end_df['money_at_round_end'].median(), color='black', ls='dashed', alpha=0.9\n    );\n    # Dotted line on margin for mean\n    grid.ax_marg_y.axhline(\n        y=end_df['money_at_round_end'].mean(), color='black', ls='dotted', alpha=0.9\n    );\n    grid.ax_joint.set_title(\"Round Level\");\n    grid.ax_marg_y.set_title(\"Match Level\");\n    grid.fig.set_figwidth(10);\n    grid.fig.suptitle(suptitle);\n    grid.ax_joint.set_ylabel(\"Money at Round End\");\n    grid.ax_joint.legend(loc=\"upper left\");\n    return grid\nplot_trajectories(\n    fair_final_df, fair_end_df,\n    suptitle=\"Trajectories and Winnings for 1000 Matches, p = 0.5\"\n);\n\n\n\n\n\n\n\n\nThough this plot is helpful for seeing the ‚Äúemergent‚Äù distribution of winnings relative to the underlying coin flips, it also makes it difficult to see the median or mean pathways/winnings ‚Äì these lines will be difficult to see in this case either way, because they‚Äôre close to 0 throughout, but being able to see them a bit better will be helpful when we move to biased coins.\nSo, as an alternative that we‚Äôll use from here on out, we can just plot a series of lighter ‚Äúbands‚Äù around the median pathway, containing (from darkest to lightest) the middle 50, 90, and 99 percentiles of money_at_round_end across all simulations (note that the band containing the middle 99 percentiles may be cut off, since it may reach extreme negative and positive values when we simulate many matches!)\n\ndef plot_trajectory_bands(final_df, end_df, suptitle, custom_ylim=None):\n    if custom_ylim is not None:\n        fair_grid_var = sns.JointGrid(height=5, ylim=(-22,22));\n    else:\n        fair_grid_var = sns.JointGrid(height=5);\n    # Don't need marginal plot on x-axis\n    fair_grid_var.ax_marg_x.remove();\n    # This ensures that the legend doesn't include params 3 different times\n    ftemp_df = final_df.copy()\n    ftemp_df['params'] = \"_\" + ftemp_df['params']\n    # Band containing the middle 100 percentiles of all trajectories\n    full_range_plot = sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", hue=\"params\",\n        data=ftemp_df, ax=fair_grid_var.ax_joint,\n        # color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',99),\n        lw=1,\n        err_kws=dict(alpha=0.125),\n    );\n    # Band containing middle 90 percentiles\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", hue=\"params\",\n        data=ftemp_df, ax=fair_grid_var.ax_joint,\n        # color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',90),\n        lw=1,\n        err_kws=dict(alpha=0.25),\n    );\n    # Band containing middle 50 percentiles\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", hue=\"params\",\n        data=ftemp_df, ax=fair_grid_var.ax_joint,\n        # color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',50),\n        lw=1,\n        err_kws=dict(alpha=0.5),\n    );\n    # Median trajectory as dashed black line\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", label=\"Median of Simulated Rounds\",\n        data=ftemp_df, ax=fair_grid_var.ax_joint,\n        color='black', alpha=0.9,\n        estimator='median',\n        errorbar=None,\n        lw=1, ls='dashed',\n    );\n    # Mean trajectory as dotted black line\n    sns.lineplot(\n        x=\"t\", y=\"money_at_round_end\", label=\"Mean of Simulated Rounds\",\n        data=ftemp_df, ax=fair_grid_var.ax_joint,\n        lw=1, ls='dotted', color='black', alpha=0.9,\n        estimator='mean',\n        errorbar=None,\n    );\n    # Refline links joint and marginal plots\n    fair_grid_var.refline(\n        y=0, color='black', lw=1, ls='solid', label='Breaking Even', alpha=0.5\n    );\n    fair_grid_var.ax_joint.legend(loc=\"lower left\");\n\n    ### Marginal plot\n    margin_kde = sns.histplot(\n        y=\"money_at_round_end\", data=end_df,\n        ax=fair_grid_var.ax_marg_y,\n        discrete=True,\n        # fill=True,\n        legend=False,\n    );\n    margin_kde = sns.kdeplot(\n        y=\"money_at_round_end\", data=end_df,\n        ax=fair_grid_var.ax_marg_y,\n        fill=True,\n        legend=False,\n    );\n    # Dashed line on margin for median\n    fair_grid_var.ax_marg_y.axhline(\n        y=end_df['money_at_round_end'].median(), color='black', ls='dashed', alpha=0.9\n    );\n    # Dotted line on margin for mean\n    fair_grid_var.ax_marg_y.axhline(\n        y=end_df['money_at_round_end'].mean(), color='black', ls='dotted', alpha=0.9\n    );\n    fair_grid_var.fig.suptitle(suptitle);\n    fair_grid_var.ax_joint.set_ylabel(\"Money at Round End\");\n    fair_grid_var.ax_joint.set_title(\"Round Level\");\n    fair_grid_var.ax_marg_y.set_title(\"Match Level\");\n    fair_grid_var.fig.set_figwidth(9.5);\n    return fair_grid_var\nplot_trajectory_bands(\n    fair_final_df, fair_end_df, custom_ylim=(-25, 25),\n    suptitle=\"Distribution of Trajectories and Winnings for 1000 Matches, p = 0.5\"\n);\n\n\n\n\n\n\n\n\nAnd, like we did with Win vs.¬†Loss for a single round, here we can more simply plot the three outcomes we‚Äôve defined at the match level:\n\nfair_result_df = fair_end_df['result'].value_counts(sort=False, normalize=True).to_frame().reset_index()\nfair_result_df\n\n\n\n\n\n\n\n\nresult\nproportion\n\n\n\n\n0\nLost Money\n0.440\n\n\n1\nBroke Even\n0.117\n\n\n2\nGained Money\n0.443\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.barplot(\n    x=\"result\", y=\"proportion\", data=fair_result_df,\n    alpha=0.8, ax=ax\n);\nax.set_title(\"Outcomes for 1000 Matches, p = 0.5\");\nax.savefig()"
  },
  {
    "objectID": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-2-simulating-the-unlockable-sheisty-coin",
    "href": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-2-simulating-the-unlockable-sheisty-coin",
    "title": "[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure",
    "section": "[Part 2] Simulating the Unlockable Sheisty Coin",
    "text": "[Part 2] Simulating the Unlockable Sheisty Coin\nThe only problem is‚Ä¶ in the new Season 2 of Elden Coin, the company who makes the game (ThumbSoft) has added a new powerup shop, which means that now your friend \\(B\\) may have used their Elden Coin earnings to buy a powerup which secretly biases their Elden Coin in their favor, turning it into a new evolved form, the Shiesty Coin. When someone buys this powerup, it adds a randomly-generated amount of bias to their coin, so that its new probability of Heads is some value (uniformly) between \\(0.4\\) and \\(0.5\\).\nSo, this means there are two new ‚Äúlayers‚Äù of uncertainty in a sense:\n\nYou don‚Äôt know whether or not your friend has a Sheisty Coin at all, and\nYou don‚Äôt know how much bias was added to their coin if they did purchase the upgrade.\n\nThe eventual goal will be to use PyMC to model this uncertainty explicitly, allowing you to make optimal gameplay choices!\nFor now, let‚Äôs simulate what rounds may look like, then what match-level trajectories and winnings may look like, under a specific biased-coin scenario.\n\n[Part 2.1] Simulating Biased Round Outcomes\nThe following cells generate and then plot a distribution of simulated round outcomes for a Sheisty Coin biased exactly in the middle of the possible range, with \\(p = 0.45\\):\n\np_biased = 0.45\n\n\nbiased_sim_result = [sim_round(p_biased) for _ in range(MyGlobals.rounds_to_sim)]\nbiased_sim_df = pd.DataFrame({'num_heads': biased_sim_result, 'params': 'p = 0.45'})\nbiased_sim_df['win'] = biased_sim_df['num_heads'] &gt; Game.win_cutoff\nbiased_count_df = get_counts(biased_sim_df)\nbiased_count_df.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\nnum_heads\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\ncount\n8\n44\n242\n665\n1353\n1983\n2177\n1699\n1150\n479\n160\n38\n2\n0\n\n\nwin\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\nPr(num_heads)\n0.0008\n0.0044\n0.0242\n0.0665\n0.1353\n0.1983\n0.2177\n0.1699\n0.115\n0.0479\n0.016\n0.0038\n0.0002\n0.0\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5,2.25))\ng = sns.barplot(\n    x=\"num_heads\", y=\"Pr(num_heads)\", hue=\"win\", data=biased_count_df,\n    alpha=0.75, ax=ax\n);\nax.axvline(x=6.5, ls='dashed', color='black', alpha=0.9);\nax.set_title(f\"N = {len(biased_sim_result)} Simulated Rounds, p = 0.5\")\nax.savefig()\n\n\n\n\n\n\n\n\nAnd we can also plot the kernel densities of the two num_heads distributions, to compare the biased and fair outcomes visually:\n\ncombined_sim_df = pd.concat([fair_sim_df, biased_sim_df])\ncombined_sim_df\n\n# Histogram (confusing imo)\nax1 = pw.Brick(figsize=(4.5,2.5))\nsns.histplot(\n    x=\"num_heads\", hue=\"params\", data=combined_sim_df, ax=ax1,\n    discrete=True\n    # fill=True, bw_adjust=2\n);\nax1.axvline(x=6.5, ls='dashed', color='black', alpha=0.9);\n# KDE Plot (more helpful imo!)\nax2 = pw.Brick(figsize=(4.5, 2.5))\nsns.kdeplot(\n    x=\"num_heads\", hue=\"params\", data=combined_sim_df, ax=ax2,\n    fill=True, bw_adjust=2\n);\nax12 = ax1 | ax2\nax12.set_suptitle(f\"N = {len(biased_sim_result)} Simulated Rounds\")\nax12.savefig()\n\n\n\n\n\n\n\n\n\n\n[Part 2.2] Simulating Biased Match Outcomes\nThe above plot shows the bias relative to a single round of the game, but let‚Äôs also generate a plot to see what this looks like when we run 50 matches. We‚Äôll plot both the \\(p = 0.5\\) and \\(p = 0.45\\) trajectories so you can get a comparative sense:\n\nbiased_final_df, biased_end_df = sim_outcome(p_biased)\nprint(\"biased_final_df:\")\ndisplay(biased_final_df.head())\nprint(\"biased_end_df:\")\ndisplay(biased_end_df.head())\n\nbiased_final_df:\n\n\n\n\n\n\n\n\n\nmatch_num\nt\nmoney_at_round_end\nmoney_change\nnum_heads\nparams\n\n\n\n\n0\n0\n0\n0\n0\nNaN\np = 0.45\n\n\n0\n0\n1\n-1\n-1\n5.0\np = 0.45\n\n\n1\n0\n2\n0\n1\n11.0\np = 0.45\n\n\n2\n0\n3\n-1\n-1\n5.0\np = 0.45\n\n\n3\n0\n4\n0\n1\n7.0\np = 0.45\n\n\n\n\n\n\n\nbiased_end_df:\n\n\n\n\n\n\n\n\n\nmatch_num\nt\nmoney_at_round_end\nresult\nparams\n\n\n\n\n49\n0\n50\n-10\nLost Money\np = 0.45\n\n\n49\n1\n50\n-22\nLost Money\np = 0.45\n\n\n49\n2\n50\n-8\nLost Money\np = 0.45\n\n\n49\n3\n50\n-12\nLost Money\np = 0.45\n\n\n49\n4\n50\n-10\nLost Money\np = 0.45\n\n\n\n\n\n\n\n\nplot_trajectories(\n    biased_final_df, biased_end_df,\n    suptitle=\"Trajectories and Winnings for 1000 Matches, p = 0.45\"\n);\n\n\n\n\n\n\n\n\nAnd then, if we plot just the mean amount of money at the end of each round, averaged across simulations, we can compare the two trajectories on the same plot:\n\ncombined_final_df = pd.concat([fair_final_df, biased_final_df])\ncombined_end_df = pd.concat([fair_end_df, biased_end_df])\n\nbiased_grid = sns.JointGrid(height=5, ylim=(-30,10));\nbiased_grid.refline(y=0, color='black', lw=1, ls='solid', label='Breaking Even');\n# Mean lines\nsns.lineplot(\n    x=\"t\", y=\"money_at_round_end\", hue=\"params\",\n    data=combined_final_df, ax=biased_grid.ax_joint,\n    estimator='median',\n    #errorbar=None,\n    errorbar=('pi',50),\n    lw=1,\n    err_kws=dict(alpha=0.15),\n);\nbiased_grid.ax_marg_x.remove();\nbiased_grid.refline(\n    y=fair_end_df['money_at_round_end'].mean(),\n    color=cb_palette[0], lw=2,\n    label='Expected Payoff (p = 0.5)'\n);\nbiased_grid.refline(\n    y=biased_end_df['money_at_round_end'].mean(),\n    color=cb_palette[1], lw=2,\n    label='Expected Payoff (p = 0.45)'\n);\nbiased_grid.ax_joint.legend(loc=\"lower left\");\n# plt.legend(['Breaking Even','Your Expected Payoff'], loc=\"left\")\n\nsns.kdeplot(\n    y=\"money_at_round_end\", hue=\"params\", data=combined_end_df,\n    ax=biased_grid.ax_marg_y, fill=True, legend=False,\n    # stat='density',\n    # discrete=True,\n    # shrink=0.5,\n);\nbiased_grid.ax_joint.set_ylabel(\"Money at Round End\");\nbiased_grid.fig.set_figwidth(10)\n\n\n\n\n\n\n\n\nAnd from this plot we can see how, the median trajectories begin to noticeably depart from about \\(t = 5\\) onwards, and then even the intervals containing the middle 50% of end-of-round money amounts begin to separate from about \\(t = 26\\) onwards."
  },
  {
    "objectID": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-3-moving-to-pymc-modeling-the-season-2-dynamic",
    "href": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-3-moving-to-pymc-modeling-the-season-2-dynamic",
    "title": "[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure",
    "section": "[Part 3] Moving to PyMC: Modeling the Season 2 Dynamic",
    "text": "[Part 3] Moving to PyMC: Modeling the Season 2 Dynamic\nSo far, it seems like playing your friend is a capital-B capital-D Bad Deal for you! Which raises the question‚Ä¶\n\n[Part 3.1] Why Would You Play At All?\nThe short answer is that‚Ä¶ this is not so far removed from the structure of e.g.¬†a lottery, or just gambling in general: you get the thrill of the small chance of winning big, and meanwhile, if \\(B\\) was for example a US state, they would get the revenue from their state lottery.\nIn Elden Coin specifically, though, the idea is that you can similarly win big if you beat someone who has a Sheisty Coin, because of a game mechanic that the developers released alongside the new powerup shop:\n\nIf you beat someone without the coin, you get the ‚Äústandard‚Äù, expected money amount given by the rules of the game. However,\nIf you beat someone with the Sheisty Coin, your winnings double!\n\nWith this final addition to the game rules, it‚Äôs getting a bit complicated to think through everything in our heads ‚Äì and that‚Äôs exactly the reason why PyMC is so useful! It‚Äôs a tool for making ‚Äúoptimal‚Äù inferences in complex settings with tons of different sources of uncertainty occurring at different levels of analysis![1]\n\n\n\nHere, and throughout the class, the ‚Äúoptimality‚Äù of Bayesian inference is rooted pretty straightforwardly in De Finetti‚Äôs Theorem and its corollaries: If a person \\(i\\) decides to update probabilities in the face of evidence using any criteria besides Bayes‚Äô Rule, they are provably susceptible to what‚Äôs called a Dutch Book scheme, meaning that someone who is using Bayes‚Äô Rule can construct a betting scheme that seems good to \\(i\\) (meaning, when \\(i\\) calculates their expected outcomes using their non-Bayesian probability-updating scheme), but will actually result in \\(i\\) losing money in expectation.\n\n\n\n\n[Part 3.2] Modeling \\(\\Pr(\\textsf{Heads} \\mid \\textsf{Sheisty})\\)\nAt this point, though we could keep making our sim_outcome() function more and more complicated to adapt to these new dynamics‚Ä¶ we have PyMC right there, a library which was created for carrying out optimal (Bayesian) inferences in probabilistic systems!\nAnd specifically, since the whole point of this writeup is to introduce a standard Bayesian Workflow‚Ä¶ this is where that starts! My hope is that you‚Äôll see the benefit of having a full-on inference machine that can quickly produce the four distributions discussed in Week 6:\n\nThe Prior Distribution, which in this case incorporates:\n\n\nThe probability \\(s \\in [0, 1]\\) of \\(B\\) having a Sheisty Coin and\nThe corresponding coin bias \\(p \\in [0.4, 0.5]\\)\n\n\nThe Prior Predictive Distribution, which gives us:\n\n\nA distribution over num_heads \\(\\in \\{0, 1, \\ldots, 13\\}\\), as well as distributions over two quantities that can be derived from this:\nA distribution over the binary value win \\(\\in \\{0, 1\\}\\), and\nA distribution over the money_change value \\(\\in \\{-1, 1\\}\\).\n\n(Note how, since we‚Äôre building num_rounds into our model, PyMC will automatically generate 50 separate round samples for each draw from this prior predictive distribution, so that we can quickly just sum up these 50 values to obtain a distribution over mach outcomes as well)\n\nThe Posterior Distribution, which gives us new distributions over the parameters \\(s\\) and \\(p\\), representing the optimal updates of the Prior Distribution after observing a set of data. And lastly,\nThe Posterior Predictive Distribution, which we can use in the same way we used the Prior Predictive distribution, but in this case it will generate distributions over num_heads, win, and money_change in light of the updated values of \\(s\\) and \\(p\\).\n\nSo, let‚Äôs write out a model in PyMC that takes into account all of the possible sources of uncertainty that we want to model in this case:\n\nclass S2Game:\n    flips_per_round = 13\n    num_heads_range = (0, flips_per_round)\n    num_heads_support = list(range(0, flips_per_round + 1))\n    win_cutoff = 6\n    rounds_per_match = 50\n    bonus_multiplier = 5\n\n\ncoords = {\n    't': list(range(1, Game.rounds_per_match + 1))\n}\nwith pm.Model(coords=coords) as s2_model:\n    p_sheisty = pm.Uniform(\"p_sheisty\", 0, 1)\n    is_sheisty = pm.Bernoulli(\"is_sheisty\", p=p_sheisty)\n    fair_p_heads = 0.5\n    sheisty_p_heads = pm.Uniform(\"sheisty_p_heads\", 0.4, 0.5)\n    p_heads = pm.Deterministic(\"p_heads\", pm.math.switch(is_sheisty, sheisty_p_heads, fair_p_heads))\n    # The flips\n    num_heads = pm.Binomial(\"num_heads\", n=Game.flips_per_round, p=p_heads, dims='t')\n    # The round result\n    money_change = pm.Deterministic(\n        \"money_change\",\n        pm.math.switch(pm.math.gt(num_heads, Game.win_cutoff), 1, -1),\n        dims='t'\n    )\n    # The cumulative result\n    money_at_round_end = pm.Deterministic(\n        \"money_at_round_end\",\n        pm.math.cumsum(money_change, 0),\n        dims='t'\n    )\n    # And the doubling-at-end if we win\n    money_w_bonus = pm.Deterministic(\n        \"money_w_bonus\",\n        pm.math.switch(\n            pm.math.gt(money_at_round_end, 0),\n            S2Game.bonus_multiplier * money_at_round_end,\n            money_at_round_end\n        ),\n        dims='t'\n    )\npm.model_to_graphviz(s2_model)\n\n\n\n\n\n\n\n\n\n\n[Part 3.3] The Prior Distribution\n\nwith s2_model:\n    s2_prior_idata = pm.sample_prior_predictive(draws=MyGlobals.matches_to_sim, random_seed=5650)\n\nSampling: [is_sheisty, num_heads, p_sheisty, sheisty_p_heads]\n\n\n\ns2_prior_df = s2_prior_idata.prior.to_dataframe().reset_index().drop(columns='chain')\ns2_prior_df.head()\n\n\n\n\n\n\n\n\ndraw\nt\nnum_heads\nis_sheisty\np_heads\nmoney_w_bonus\nmoney_change\np_sheisty\nsheisty_p_heads\nmoney_at_round_end\n\n\n\n\n0\n0\n1\n6\n1\n0.432234\n-1\n-1\n0.888773\n0.432234\n-1\n\n\n1\n0\n2\n6\n1\n0.432234\n-2\n-1\n0.888773\n0.432234\n-2\n\n\n2\n0\n3\n7\n1\n0.432234\n-1\n1\n0.888773\n0.432234\n-1\n\n\n3\n0\n4\n7\n1\n0.432234\n0\n1\n0.888773\n0.432234\n0\n\n\n4\n0\n5\n6\n1\n0.432234\n-1\n-1\n0.888773\n0.432234\n-1\n\n\n\n\n\n\n\n\ns2_prior_end_df = s2_prior_df[s2_prior_df['t'] == Game.rounds_per_match].copy()\ns2_prior_end_df.head()\n\n\n\n\n\n\n\n\ndraw\nt\nnum_heads\nis_sheisty\np_heads\nmoney_w_bonus\nmoney_change\np_sheisty\nsheisty_p_heads\nmoney_at_round_end\n\n\n\n\n49\n0\n50\n5\n1\n0.432234\n-24\n-1\n0.888773\n0.432234\n-24\n\n\n99\n1\n50\n8\n0\n0.500000\n10\n1\n0.027332\n0.433957\n2\n\n\n149\n2\n50\n5\n0\n0.500000\n-10\n-1\n0.081197\n0.407078\n-10\n\n\n199\n3\n50\n5\n0\n0.500000\n-6\n-1\n0.864813\n0.400025\n-6\n\n\n249\n4\n50\n7\n1\n0.412902\n-22\n1\n0.871763\n0.412902\n-22\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"p_sheisty\", data=s2_prior_end_df, ax=ax\n);\nax.set_title(\"Prior Distribution of p_sheisty\")\nax.savefig()\n\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"p_heads\", data=s2_prior_end_df, ax=ax\n);\nax.set_title(\"Prior Distribution of p_heads\")\nax.savefig()\n\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"is_sheisty\", data=s2_prior_end_df, ax=ax\n);\nax.set_title(\"Prior Distribution of is_sheisty\");\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n[Part 3.3] The Prior Predictive Distribution\nNow, the vocabulary in this part is going to be a bit confusing at first:\n\nWith the way we have defined the model parameters ‚Äì p_sheisty and p_heads as parameters and then num_heads (as a vector) and money final (as a scalar) as outcomes ‚Äì we do have a prior predictive distribution her, since we can obtain distributions over the outcomes before actually observing any data. However‚Ä¶\nWith the way PyMC defines paramters, we don‚Äôt ‚Äúunlock‚Äù the prior-predictive or posterior-predictive distributions until we actually provide observed data to the model.\n\nSo, what that means is, don‚Äôt be fooled by the PyMC terminology (which is for computational convenience ‚Äì see e.g.¬†this post on their forum about it)! We‚Äôre going to use the .prior attribute of our inference data, but we‚Äôre using it to carry out a prior-predictive check in this part![1]\n\n\n\nI‚Äôm going to release a shorter writeup soon where we‚Äôll supply the data to our pm.Model() right away, so that we‚Äôll immediately have access to .prior, .prior_predictive, .posterior, and .posterior_predictive!\n\n\nFirst things first, we could infer just from a table of the means and medians of the two models that the bonus-money has had an interesting effect on our expected earnings:\n\ns2_prior_money_df = s2_prior_end_df[['draw','money_at_round_end','money_w_bonus']].copy().melt(id_vars=\"draw\")\ns2_prior_money_df.groupby('variable')['value'].agg(['mean','median'])\n\n\n\n\n\n\n\n\nmean\nmedian\n\n\nvariable\n\n\n\n\n\n\nmoney_at_round_end\n-6.536\n-6.0\n\n\nmoney_w_bonus\n0.792\n-6.0\n\n\n\n\n\n\n\nEven though the medians are exactly the same, the means of the two variables now differ in an important way, with one above 0 and the other well below 0. To get a visual for why this happens, let‚Äôs plot two posterior-predictive distributions here, on the same axes, to see how the doubling-of-positive-winnings dynamic affects the distribution of outcomes (as well as the ultimate expected value!)\n\nprior_mean_money_nb = s2_prior_end_df['money_at_round_end'].mean()\nprior_mean_money = s2_prior_end_df['money_w_bonus'].mean()\nax = pw.Brick(figsize=(6, 3.5));\nsns.kdeplot(\n    x=\"value\", hue=\"variable\", data=s2_prior_money_df, ax=ax,\n    fill=True\n);\nax.axvline(x=0, color='black', ls='dashed', lw=1, alpha=0.9);\nax.axvline(x=prior_mean_money_nb, color=cb_palette[1], ls='dashed', lw=1, alpha=0.9);\nax.axvline(x=prior_mean_money, color=cb_palette[0], ls='dashed', lw=1, alpha=0.9);\nax.set_title(\"Distribution of Net Earnings Before and After Bonus\");\nax.savefig()\n\n\n\n\n\n\n\n\nSo, that‚Äôs the prior-predictive distribution with respect to the final end-of-match payout, where we can already see (from the dashed vertical lines) that the bonus makes playing the game ever-so-slightly worth it, with an expected value now slightly above ‚Ç¨0. This plot also explains why only the mean was affected: the bonus only affected the final money amounts by scaling up outcomes which were already above the 50% mark, leaving the median (the observation with exactly 50% of observations above it) unchanged.\nAnd, since our model has two levels (round-level and match-level), we can also use our prior-predictive distribution to visualize the distribution of trajectories:\n\ndef plot_predictive_trajectories(df, suptitle, custom_bw_adjust=2):\n    # Add a special t=0 row to each match, so the plotted trajectories start at\n    # (t,y) = (0,0)\n    t0_df = pd.DataFrame({'t': 0, 'money_w_bonus': 0, 'draw': range(df['draw'].max())})\n    plot_df = pd.concat([t0_df, df])\n    end_df = df[df['t'] == S2Game.rounds_per_match].copy()\n    mean_money = end_df['money_w_bonus'].mean()\n    med_money = end_df['money_w_bonus'].median()\n    \n    grid = sns.JointGrid(height=5.25) #, ylim=(-30,15));\n    grid.ax_marg_x.remove();\n    # Mean lines\n    full_range_plot = sns.lineplot(\n        x=\"t\", y=\"money_w_bonus\",\n        data=df, ax=grid.ax_joint,\n        color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',100),\n        lw=1,\n        err_kws=dict(alpha=0.125),\n    );\n    sns.lineplot(\n        x=\"t\", y=\"money_w_bonus\",\n        data=df, ax=grid.ax_joint,\n        color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',75),\n        lw=1,\n        err_kws=dict(alpha=0.25),\n    );\n    sns.lineplot(\n        x=\"t\", y=\"money_w_bonus\",\n        data=df, ax=grid.ax_joint,\n        color=cb_palette[0],\n        estimator='median',\n        #errorbar=None,\n        errorbar=('pi',50),\n        lw=1,\n        err_kws=dict(alpha=0.5),\n    );\n    # Median trajectory outline in black\n    sns.lineplot(\n        x=\"t\", y=\"money_w_bonus\", label=\"Median of Simulated Rounds\",\n        data=df, ax=grid.ax_joint,\n        color='black', alpha=0.9,\n        estimator='median',\n        errorbar=None,\n        lw=1, ls='dashed',\n    );\n    # Mean trajectory\n    sns.lineplot(\n        x=\"t\", y=\"money_w_bonus\", label=\"Mean of Simulated Rounds\",\n        data=df, ax=grid.ax_joint,\n        color='black', alpha=0.9,\n        estimator='mean',\n        errorbar=None,\n        lw=1, ls='dotted',\n    );\n    grid.refline(\n        y=0, color='black', lw=1, ls='solid', label='Breaking Even', alpha=0.9\n    );\n    # prior_grid.refline(\n    #     y=biased_end_df['money_at_round_end'].mean(),\n    #     color=cb_palette[1], lw=2,\n    #     label='Expected Payoff'\n    # );\n    grid.ax_joint.legend(loc=\"lower left\");\n    # plt.legend(['Breaking Even','Your Expected Payoff'], loc=\"left\")\n    \n    sns.kdeplot(\n        y=\"money_w_bonus\", data=df,\n        ax=grid.ax_marg_y, fill=True, legend=False,\n        bw_adjust=custom_bw_adjust,\n        # stat='density',\n        # discrete=True,\n        # shrink=0.5,\n    );\n    # Line on margin for mean\n    grid.ax_marg_y.axhline(\n        y=mean_money, color='black', ls='dotted', alpha=0.9\n    );\n    grid.ax_marg_y.axhline(\n        y=med_money, color='black', ls='dashed', alpha=0.9\n    );\n    grid.ax_joint.set_ylabel(\"Money at Round End\");\n    grid.ax_joint.set_title(\"Round Level\");\n    grid.ax_marg_y.set_title(\"Match Level\");\n    grid.fig.suptitle(suptitle);\n    grid.fig.set_figwidth(9.5);\n    return grid\nplot_predictive_trajectories(\n    s2_prior_df,\n    suptitle=\"Prior Predictive Distributions\"\n);\n\n\n\n\n\n\n\n\nSo now we can see, from this and the previous plot, that given our uncertainty about p_sheisty and p_heads, it is just barely worthwhile to play! Even though the median of the game with and without the beating-sheisty bonus is the same, the mean with the bonus is just above ‚Ç¨0.\nIn other words, given our state of knowledge about the game at this point, it is worthwhile to play (at least, on the assumption that we play if \\(\\mathbb{E}[Y] &gt; 0\\).\nHowever, each time you play a match with \\(B\\), you will obtain some information about (a) whether or not they have the Sheisty Coin, and then (b) if they do have it, what their bias level may be. So, in the next part we‚Äôll simulate playing one round and then updating based on the results."
  },
  {
    "objectID": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-4-updating-on-observed-data",
    "href": "writeups/bayesian-workflow/Bayesian_Workflow.html#part-4-updating-on-observed-data",
    "title": "[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure",
    "section": "[Part 4] Updating On Observed Data",
    "text": "[Part 4] Updating On Observed Data\nNow, let‚Äôs say you play one match with your friend, and you‚Äôre trying to determine whether it‚Äôs worthwhile to keep playing with them‚Ä¶ How exactly would you figure this out? By what specific amounts should you update your distribution over p_sheisty and p_heads?\nYou could sit down and solve a gigantic 5100-style problem‚Ä¶ or just plug the data into your Bayesian inference machine, namely, PyMC üòä\n\n[Part 4.1] Your (Single) Observed Match\nHere we‚Äôll finally reveal the secret: though you, as player \\(A\\), don‚Äôt know it (this is the true DGP we‚Äôre simulating, finally, which is not observable to the actual agents playing the game!), player \\(B\\) does indeed have the Sheisty Coin, and the random bias they‚Äôve obtained puts their p_heads at 0.41.\nSo, let‚Äôs simulate one match with this coin, keeping in mind that you-as-player-\\(A\\) don‚Äôt know that this data was generated via \\(p = 0.48\\) ‚Äì you only know that the match ended up with the following trajectory and outcome:\n\ntrue_is_sheisty = 1\ntrue_p_heads = 0.48\n\n\ndef sim_s2_match(p, rng_seed=5650):\n    sim_rng = np.random.default_rng(seed=rng_seed)\n    round_outcomes = sim_rng.binomial(n=Game.flips_per_round, p=p, size=Game.rounds_per_match)\n    match_df = pd.DataFrame({'t': range(1,Game.rounds_per_match+1), 'num_heads': round_outcomes})\n    match_df['money_change'] = -1 + 2 * (match_df['num_heads'] &gt; Game.win_cutoff)\n    match_df['money_at_round_end'] = match_df['money_change'].cumsum()\n    # New in Season 2: Quintupling your money when it's above 0!\n    match_df['money_w_bonus'] = match_df['money_at_round_end'].apply(lambda x: S2Game.bonus_multiplier * x if x &gt; 0 else x) \n    t0_df = pd.DataFrame({'t':[0],'money_at_round_end':[0],'money_change':[0]})\n    match_df = pd.concat([t0_df, match_df])\n    return(match_df)\nobs_df = sim_s2_match(true_p_heads)\nobs_df.tail()\n\n\n\n\n\n\n\n\nt\nmoney_at_round_end\nmoney_change\nnum_heads\nmoney_w_bonus\n\n\n\n\n45\n46\n-14\n1\n8.0\n-14.0\n\n\n46\n47\n-15\n-1\n4.0\n-15.0\n\n\n47\n48\n-14\n1\n8.0\n-14.0\n\n\n48\n49\n-13\n1\n7.0\n-13.0\n\n\n49\n50\n-14\n-1\n5.0\n-14.0\n\n\n\n\n\n\n\n\nobs_grid = sns.JointGrid(height=5) # , ylim=(-30,15));\nobs_grid.refline(\n    y=0, color='black', alpha=0.9, lw=1, ls='solid', label='Breaking Even'\n);\n# Mean lines\nsns.lineplot(\n    x=\"t\", y=\"money_at_round_end\", label=\"Observed Trajectory\",\n    data=obs_df, ax=obs_grid.ax_joint,\n    estimator='median',\n    #errorbar=None,\n    errorbar=('pi',50),\n    lw=1,\n    err_kws=dict(alpha=0.15),\n);\nobs_grid.ax_marg_x.remove();\nobs_grid.refline(\n    y=obs_df.iloc[-1]['money_at_round_end'],\n    color=cb_palette[0], lw=1,\n    label='Obseved Payoff'\n);\nobs_grid.ax_joint.legend(loc=\"lower left\");\n# plt.legend(['Breaking Even','Your Expected Payoff'], loc=\"left\")\n\n# sns.kdeplot(\n#     y=\"money_at_round_end\", hue=\"params\", data=combined_end_df,\n#     ax=biased_grid.ax_marg_y, fill=True, legend=False,\n#     # stat='density',\n#     # discrete=True,\n#     # shrink=0.5,\n# );\nobs_grid.ax_joint.set_ylabel(\"Money at Round End\");\nobs_grid.fig.set_figwidth(9)\n\n\n\n\n\n\n\n\nSo, by the end of this match, you ended up with ‚Ç¨14 less than you started with. If you only knew this final amount, you would essentially have one ‚Äúbit‚Äù of information. However, you have more than this! You also know the trajectory from \\(t = 0\\) to \\(t = 50\\), which gives you much more information you can use to update your model parameters.\nLet‚Äôs use PyMC as it was fully intended, by pairing our model with this observed data, and then see how this changes whether or not it‚Äôs still worth it to play against \\(B\\)!\n\n\n[Part 4.2] Adding the Observations to Our Model and Updating\nSince our model doesn‚Äôt actually incorporate the \\(t = 0\\) row (we added that just to make the plots a bit more readable), we drop these rows and then set obs_df without these rows as the observed data for our model.\n\nobs_t1_df = obs_df[obs_df['t'] &gt; 0].copy().set_index('t')\n\n\ncoords = {\n    't': list(range(1, Game.rounds_per_match + 1))\n}\nwith pm.Model(coords=coords) as s2_model_obs:\n    # Unobservable prior parameters\n    p_sheisty = pm.Uniform(\"p_sheisty\", 0, 1)\n    is_sheisty = pm.Bernoulli(\"is_sheisty\", p=p_sheisty)\n    fair_p_heads = 0.5\n    sheisty_p_heads = pm.Uniform(\"sheisty_p_heads\", 0.4, 0.5)\n    p_heads = pm.Deterministic(\n        \"p_heads\",\n        pm.math.switch(\n            is_sheisty,\n            sheisty_p_heads,\n            fair_p_heads\n        )\n    )\n    # Observable parameters\n    # The flips\n    num_heads = pm.Binomial(\n        \"num_heads\", n=Game.flips_per_round, p=p_heads,\n        dims='t', observed=obs_t1_df['num_heads']\n    )\n    # The round result\n    money_change = pm.Deterministic(\n        \"money_change\",\n        pm.math.switch(pm.math.gt(num_heads, Game.win_cutoff), 1, -1),\n        dims='t'\n    )\n    # The cumulative result\n    money_at_round_end = pm.Deterministic(\n        \"money_at_round_end\",\n        pm.math.cumsum(money_change, 0),\n        dims='t'\n    )\n    # And the doubling-at-end if we win\n    money_w_bonus = pm.Deterministic(\n        \"money_w_bonus\",\n        pm.math.switch(\n            pm.math.gt(money_at_round_end, 0),\n            S2Game.bonus_multiplier * money_at_round_end,\n            money_at_round_end\n        ),\n        dims='t'\n    )\npm.model_to_graphviz(s2_model_obs)\n\n\n\n\n\n\n\n\n\n\n[Part 4.3] Sample from Posterior Distribution\n\nwith s2_model_obs:\n    s2_post_idata = pm.sample(random_seed=5650)\n\nMultiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;NUTS: [p_sheisty, sheisty_p_heads]\n&gt;BinaryGibbsMetropolis: [is_sheisty]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\n\n\n\ns2_post_df = s2_post_idata.posterior.to_dataframe().reset_index()\ns2_post_df\n\n\n\n\n\n\n\n\nchain\ndraw\nt\nis_sheisty\np_sheisty\nsheisty_p_heads\np_heads\nmoney_change\nmoney_at_round_end\nmoney_w_bonus\n\n\n\n\n0\n0\n0\n1\n1\n0.634665\n0.425275\n0.425275\n-1\n-1\n-1\n\n\n1\n0\n0\n2\n1\n0.634665\n0.425275\n0.425275\n1\n0\n0\n\n\n2\n0\n0\n3\n1\n0.634665\n0.425275\n0.425275\n-1\n-1\n-1\n\n\n3\n0\n0\n4\n1\n0.634665\n0.425275\n0.425275\n-1\n-2\n-2\n\n\n4\n0\n0\n5\n1\n0.634665\n0.425275\n0.425275\n1\n-1\n-1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n199995\n3\n999\n46\n1\n0.902354\n0.431539\n0.431539\n1\n-14\n-14\n\n\n199996\n3\n999\n47\n1\n0.902354\n0.431539\n0.431539\n-1\n-15\n-15\n\n\n199997\n3\n999\n48\n1\n0.902354\n0.431539\n0.431539\n1\n-14\n-14\n\n\n199998\n3\n999\n49\n1\n0.902354\n0.431539\n0.431539\n1\n-13\n-13\n\n\n199999\n3\n999\n50\n1\n0.902354\n0.431539\n0.431539\n-1\n-14\n-14\n\n\n\n\n200000 rows √ó 10 columns\n\n\n\n\ns2_post_end_df = s2_post_df[s2_post_df['t'] == S2Game.rounds_per_match].copy()\ns2_post_end_df\n\n\n\n\n\n\n\n\nchain\ndraw\nt\nis_sheisty\np_sheisty\nsheisty_p_heads\np_heads\nmoney_change\nmoney_at_round_end\nmoney_w_bonus\n\n\n\n\n49\n0\n0\n50\n1\n0.634665\n0.425275\n0.425275\n-1\n-14\n-14\n\n\n99\n0\n1\n50\n1\n0.941787\n0.466247\n0.466247\n-1\n-14\n-14\n\n\n149\n0\n2\n50\n1\n0.201438\n0.432916\n0.432916\n-1\n-14\n-14\n\n\n199\n0\n3\n50\n1\n0.910678\n0.445946\n0.445946\n-1\n-14\n-14\n\n\n249\n0\n4\n50\n1\n0.500935\n0.446615\n0.446615\n-1\n-14\n-14\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n199799\n3\n995\n50\n1\n0.204122\n0.475606\n0.475606\n-1\n-14\n-14\n\n\n199849\n3\n996\n50\n1\n0.915687\n0.422559\n0.422559\n-1\n-14\n-14\n\n\n199899\n3\n997\n50\n1\n0.321373\n0.466857\n0.466857\n-1\n-14\n-14\n\n\n199949\n3\n998\n50\n1\n0.486654\n0.461348\n0.461348\n-1\n-14\n-14\n\n\n199999\n3\n999\n50\n1\n0.902354\n0.431539\n0.431539\n-1\n-14\n-14\n\n\n\n\n4000 rows √ó 10 columns\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"p_sheisty\", data=s2_post_end_df, ax=ax\n);\nax.set_title(\"Posterior Distribution of p_sheisty\");\nax.savefig()\n\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"is_sheisty\", data=s2_post_end_df, ax=ax\n);\nax.set_title(\"Posterior Distribution of is_sheisty\");\nax.savefig()\n\n\n\n\n\n\n\n\n\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"p_heads\", data=s2_post_end_df, ax=ax\n);\nax.set_title(\"Posterior Distribution of p_heads\");\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n[Part 4.4] Sample from Posterior Predictive Distribution\n\nwith s2_model_obs:\n    s2_post_pred_idata = pm.sample_posterior_predictive(s2_post_idata, random_seed=5650)\n\nSampling: [num_heads]\n\n\n\n\n\n\n\n\n\ns2_post_pred_df = s2_post_pred_idata.posterior_predictive.to_dataframe().reset_index()\ns2_post_pred_df['source'] = \"Posterior Predictive\"\ns2_post_pred_df\n\n\n\n\n\n\n\n\nchain\ndraw\nt\nnum_heads\nmoney_change\nmoney_at_round_end\nmoney_w_bonus\nsource\n\n\n\n\n0\n0\n0\n1\n5\n-1\n-1\n-1\nPosterior Predictive\n\n\n1\n0\n0\n2\n5\n-1\n-2\n-2\nPosterior Predictive\n\n\n2\n0\n0\n3\n3\n-1\n-3\n-3\nPosterior Predictive\n\n\n3\n0\n0\n4\n0\n-1\n-4\n-4\nPosterior Predictive\n\n\n4\n0\n0\n5\n4\n-1\n-5\n-5\nPosterior Predictive\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n199995\n3\n999\n46\n7\n1\n-16\n-16\nPosterior Predictive\n\n\n199996\n3\n999\n47\n5\n-1\n-17\n-17\nPosterior Predictive\n\n\n199997\n3\n999\n48\n5\n-1\n-18\n-18\nPosterior Predictive\n\n\n199998\n3\n999\n49\n4\n-1\n-19\n-19\nPosterior Predictive\n\n\n199999\n3\n999\n50\n7\n1\n-18\n-18\nPosterior Predictive\n\n\n\n\n200000 rows √ó 8 columns\n\n\n\n\nplot_predictive_trajectories(\n    s2_post_pred_df,\n    suptitle=\"Posterior Predictive Distributions\"\n);\n\n\n\n\n\n\n\n\nSo‚Ä¶ by incorporating what we learned from the one round about the possibility that \\(B\\) has the Sheisty Coin, we‚Äôve learn that it is no longer worthwhile to keep playing against \\(B\\)!\nWe can see, up in the portion above the ‚ÄúBreaking Even‚Äù line, the tantalizing 1% of matches that end in huge fortunes for us. Taking this risk-reward tradeoff into account would take us into the even more‚Ä¶ complex-but-important realm of Bayesian Decision Theory!\nAs a motivation to take the additional deep-dive into this field, imo it is especially, incredibly important for those interested in public policy. The Minnesota Radon data mentioned in class, for example, was part of a much broader case-study of how Bayesian data analysis can be used to provide policy recommendations, and following the Bayesian Workflow beyond the contents of this notebook and into Bayesian Decision Theory will allow you to construct plots like this, providing explicit Bayes-Theorem-optimized recommendations to policymakers:\n\n\n\n\nThe Bayesian Decision Analysis plot from Gelman et al.¬†(2014), Bayesian Data Analysis (Third Edition)"
  },
  {
    "objectID": "writeups/index.html",
    "href": "writeups/index.html",
    "title": "Extra Writeups",
    "section": "",
    "text": "Order By\n       Default\n         \n          Last Updated - Oldest\n        \n         \n          Last Updated - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nLast Updated\n\n\nCategory\n\n\n\n\n\n\nBirthdays as Instruments for Catholic School Effects\n\n\nSunday, July 20, 2025\n\n\nLabs\n\n\n\n\nSkeptical, Weakly-Informative, and Strongly-Informative Priors\n\n\nSaturday, June 28, 2025\n\n\nLabs\n\n\n\n\n[DSAN5650 Lab] Bayesian Workflow: A Modeling Adventure\n\n\nSaturday, June 28, 2025\n\n\nLabs\n\n\n\n\nDSAN 5650 HW2 Guide\n\n\nSunday, June 22, 2025\n\n\nExtra Writeups\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Writeups"
    ]
  },
  {
    "objectID": "w02/slides.html#courtney-green",
    "href": "w02/slides.html#courtney-green",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Courtney Green",
    "text": "Courtney Green\nBackground: BA in Public Policy & Leadership, now interested in using data to examine structural disparities.\nInterests & what I can help with: How sociodemographic factors (e.g., race, gender, income, immigration status, education level) shape policy outcomes and institutional practices in areas like criminal justice, housing, healthcare, education, and environment.\n\nAsk me about: Counterfactual balancing, handling messy or privacy-limited datasets, and framing causal questions around inequality.\nReach out if you‚Äôre thinking about a final project that touches social systems, systemic inequity, or fairness!"
  },
  {
    "objectID": "w02/slides.html#what-got-me-interested-in-causal-inference",
    "href": "w02/slides.html#what-got-me-interested-in-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Got Me Interested in Causal Inference",
    "text": "What Got Me Interested in Causal Inference\n5000 Project: Over-Policing and Wrongful Convictions in Illinois\n\nMotivation: Explore what demographic factors predict wrongful convictions using real exoneration data.\nChallenge: No data on ‚Äúnon-exonerated innocents,‚Äù so I (with prof‚Äôs help) created a counterfactual dataset by sampling from the incarcerated population\nMethod: Trained models (logistic regression, random forest, KNN) on a balanced dataset of 548 exonerated and 548 non-exonerated individuals\nFinding: Race and geography (esp.¬†Cook Count/Chicago) were the strongest predictors of exoneration\nTakeaway: Questions about systemic bias can‚Äôt be answered with just descriptive stats, you need counterfactual reasoning and causal inference\n\n(Optional) View the counterfactual setup"
  },
  {
    "objectID": "w02/slides.html#wendy-hu",
    "href": "w02/slides.html#wendy-hu",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Wendy Hu",
    "text": "Wendy Hu\n\nBackground: BS in Social Work, passionate about advancing social equity through computational tools\nInterest: I bring a background in social welfare, public health, and NGO data, with experience analyzing behavioral and institutional outcomes. My work spans Indigenous health, gender equity, and trauma-informed service design‚Äîalways with a focus on equity, system change, and policy relevance.\nAsk me about: Framing social justice questions in data terms, working with community or clinical datasets and interpreting model results in context.\nIf you‚Äôre designing a project around health, inequality, nonprofit impact, or any socially embedded issue‚ÄîI‚Äôd love to help bridge rigor and relevance."
  },
  {
    "objectID": "w02/slides.html#jeffs-hw1-updateapology",
    "href": "w02/slides.html#jeffs-hw1-updateapology",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Jeff‚Äôs HW1 Update/Apology",
    "text": "Jeff‚Äôs HW1 Update/Apology\n\nBasically‚Ä¶ I goofed by trying to shove a homework into the first two weeks of class üò≠\nLast week was ‚Äúsetting the table‚Äù, so there are a few (multiple-choice) questions on that\nBut, really only makes sense to have HW starting from end of today, once we‚Äôve introduced PGMs, the language we need to actually do Causal Computational Social Science!\n\n\n Due date pushed to Friday, June 6, 5:59pm"
  },
  {
    "objectID": "w02/slides.html#hw1-detail-plz-dont-hate-me",
    "href": "w02/slides.html#hw1-detail-plz-dont-hate-me",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)",
    "text": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)\n(The main reason this is taking me so long)\n\nTwo key libraries for ‚Äúmain‚Äù course content (HW2 onwards):\n\nIn R: rethinking, ‚Äúlite‚Äù version of Stan\nIn Python: PyMC\n\nBoth are overkill for current unit: lots of work required to ‚Äúturn off‚Äù fancy features and implement basic PGMs\n\\(\\leadsto\\) Only way I know to achieve two goals:\n\n Learning simple PGMs first, as tools for thinking, before adding in additional full-on coding baggage of Stan/PyMC\n Having an entire textbook on these base PGMs that you can use as reference\n\nIs to use [small, restricted] subset of JavaScript, called WebPPL, because‚Ä¶\n\n\nhttp://probmods.org/"
  },
  {
    "objectID": "w02/slides.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "href": "w02/slides.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality",
    "text": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality\n\n\n You‚Äôll no longer be able to read ‚Äúscientific‚Äù writing without striking this expression (involuntarily):\n\n ‚ÄúScientific‚Äù talks will begin to sound like the following:"
  },
  {
    "objectID": "w02/slides.html#blasting-off-into-causality",
    "href": "w02/slides.html#blasting-off-into-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Blasting Off Into Causality!",
    "text": "Blasting Off Into Causality!"
  },
  {
    "objectID": "w02/slides.html#data-generating-processes-dgps",
    "href": "w02/slides.html#data-generating-processes-dgps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Data-Generating Processes (DGPs)",
    "text": "Data-Generating Processes (DGPs)\n\n\n\n\n\n\n\nYou saw this in DSAN 5100!\n¬´\\(X_1, \\ldots, X_n\\) drawn i.i.d. Normal, mean \\(\\mu\\) variance \\(\\sigma^2\\)¬ª characterizes DGP of \\((X_1, \\ldots, X_n)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n5650: Dive into DGPs, rather than treating as black box/footnote to Law of Large Numbers, so we can move [asymptotically!]‚Ä¶\nFrom associational statements:\n\n¬´\\(\\underbrace{\\text{An increase}}_{\\small\\text{noun}}\\) in \\(X\\) by 1 is associated with increase in \\(Y\\) by \\(\\beta\\)¬ª\n\nTo causal ones: ¬´\\(\\underbrace{\\text{Increasing}}_{\\small\\text{verb}}\\) \\(X\\) by 1 causes \\(Y\\) to increase by \\(\\beta\\)¬ª"
  },
  {
    "objectID": "w02/slides.html#dgps-and-the-emergence-of-order",
    "href": "w02/slides.html#dgps-and-the-emergence-of-order",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "DGPs and the Emergence of Order",
    "text": "DGPs and the Emergence of Order\n\n\n\n\n\n\n\nWho can guess the state of this process after 10 steps, with 1 person?\n10 people? 50? 100? (If they find themselves on the same spot, they stand on each other‚Äôs heads)\n100 steps? 1000?"
  },
  {
    "objectID": "w02/slides.html#the-result-16-steps",
    "href": "w02/slides.html#the-result-16-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 16 Steps",
    "text": "The Result: 16 Steps"
  },
  {
    "objectID": "w02/slides.html#the-result-64-steps",
    "href": "w02/slides.html#the-result-64-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 64 Steps",
    "text": "The Result: 64 Steps"
  },
  {
    "objectID": "w02/slides.html#mathematicalscientific-modeling",
    "href": "w02/slides.html#mathematicalscientific-modeling",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "‚ÄúMathematical/Scientific Modeling‚Äù",
    "text": "‚ÄúMathematical/Scientific Modeling‚Äù\n\nThing we observe (poking out of water): data\nHidden but possibly discoverable via deeper dive (ecosystem under surface): DGP"
  },
  {
    "objectID": "w02/slides.html#so-whats-the-problem",
    "href": "w02/slides.html#so-whats-the-problem",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often\n\nSee Appendix Slide"
  },
  {
    "objectID": "w02/slides.html#the-shallow-problem-of-causal-inference",
    "href": "w02/slides.html#the-shallow-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Shallow Problem of Causal Inference",
    "text": "The Shallow Problem of Causal Inference\n\n\n\n\n\n\n\n\ncor(ski_df$value, law_df$value)\n[1] 0.9921178\n\n(Data from Vigen, Spurious Correlations)\n\nThis, however, is only a mini-boss. Beyond it lies the truly invincible FINAL BOSS‚Ä¶ üôÄ"
  },
  {
    "objectID": "w02/slides.html#the-fundamental-problem-of-causal-inference",
    "href": "w02/slides.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠"
  },
  {
    "objectID": "w02/slides.html#what-is-to-be-done",
    "href": "w02/slides.html#what-is-to-be-done",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?"
  },
  {
    "objectID": "w02/slides.html#probability",
    "href": "w02/slides.html#probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring."
  },
  {
    "objectID": "w02/slides.html#beyond-conditional-probability",
    "href": "w02/slides.html#beyond-conditional-probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects"
  },
  {
    "objectID": "w02/slides.html#preview-do-calculus",
    "href": "w02/slides.html#preview-do-calculus",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events"
  },
  {
    "objectID": "w02/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w02/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!"
  },
  {
    "objectID": "w02/slides.html#ulysses-and-the-computational-sirens",
    "href": "w02/slides.html#ulysses-and-the-computational-sirens",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Ulysses and the [Computational] Sirens",
    "text": "Ulysses and the [Computational] Sirens"
  },
  {
    "objectID": "w02/slides.html#bayesian-inference-but-with-pictures",
    "href": "w02/slides.html#bayesian-inference-but-with-pictures",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Bayesian Inference but with Pictures",
    "text": "Bayesian Inference but with Pictures\nA Probabilistic Graphical Model (PGM) provides us with:\n\nA formal-mathematical‚Ä¶\nBut also easily visualizable (by construction)‚Ä¶\nRepresentation of a data-generating process (DGP)!\n\nExample: Let‚Äôs model how weather \\(W\\) affects evening plans \\(Y\\): the choice between going to a party or staying in to watch movies\n\n\n\n\n The Partier‚Äôs Dilemma\n\n\n\nA person \\(i\\) wakes up with some initial affinity for partying: \\(\\Pr(Y_i = \\textsf{Go})\\)\n\\(i\\) then goes to their window and observes the weather \\(W_i\\) outside:\n\nIf the weather is sunny, \\(i\\)‚Äôs affinity increases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Sun}) &gt; \\Pr(Y = \\textsf{Go})\\)\nOtherwise, if it is rainy, \\(i\\)‚Äôs affinity decreases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Rain}) &lt; \\Pr(Y = \\textsf{Go})\\)"
  },
  {
    "objectID": "w02/slides.html#two-main-building-blocks",
    "href": "w02/slides.html#two-main-building-blocks",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Two Main ‚ÄúBuilding Blocks‚Äù",
    "text": "Two Main ‚ÄúBuilding Blocks‚Äù\n\nNodes like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}\\) denote Random Variables\n\n\\[\n\\boxed{\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}} \\simeq \\boxed{ \\begin{array}{c|cc}x & \\textsf{Tails} & \\textsf{Heads} \\\\\\hline \\Pr(X = x) & 0.5 & 0.5\\end{array}}\n\\]\n\nEdges like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) denote relationships between RVs\n\nWhat an edge ‚Äúmeans‚Äù can get [ontologically] tricky!\nRetain sanity by just remembering: an edge \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) is included in our PGM if we ‚Äúcare about‚Äù modeling the conditional probability table (CPT) of \\(Y\\) w.r.t. \\(X\\)\n\n\n\\[\n\\require{enclose}\\boxed{ \\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em} } \\simeq \\boxed{\n  \\begin{array}{c|cc}\n  x & \\Pr(Y = \\textsf{Lose} \\mid X = x) & \\Pr(Y = \\textsf{Win} \\mid X = x) \\\\\\hline\n  \\textsf{Tails} & 0.8 & 0.2 \\\\\n  \\textsf{Heads} & 0.5 & 0.5\n  \\end{array}\n}\n\\]"
  },
  {
    "objectID": "w02/slides.html#pgm-for-the-partiers-dilemma",
    "href": "w02/slides.html#pgm-for-the-partiers-dilemma",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "PGM for the Partier‚Äôs Dilemma",
    "text": "PGM for the Partier‚Äôs Dilemma\n\nA node \\(\\pnode{W}\\) denoting RV \\(W\\), which can take on values in \\(\\mathcal{R}_W = \\{\\textsf{Sun}, \\textsf{Rain}\\}\\),\nA node \\(\\pnode{Y}\\) denoting RV \\(Y\\), which can take on values in \\(\\mathcal{R}_Y = \\{\\textsf{Go}, \\textsf{Stay}\\}\\), and\nAn edge \\(\\pedge{W}{Y}\\) representing the following relationship between \\(W\\) and \\(Y\\):\n\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) = 0.8\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Sun}) = 0.2\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) = 0.1\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Rain}) = 0.9\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Our PGM of the Partier‚Äôs Dilemma\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\Pr(Y = \\textsf{Stay} \\mid W)\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W)\\)\n\n\n\n\n\\(W = \\textsf{Sun}\\)\n0.2\n0.8\n\n\n\\(W = \\textsf{Rain}\\)\n0.9\n0.1\n\n\n\n\n\nFigure¬†2: The Conditional Probability Table (CPT) for the edge \\(\\pedge{W}{Y}\\) in Figure¬†1"
  },
  {
    "objectID": "w02/slides.html#observed-vs.-latent-nodes",
    "href": "w02/slides.html#observed-vs.-latent-nodes",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed vs.¬†Latent Nodes",
    "text": "Observed vs.¬†Latent Nodes\n\nPGMs help us make valid (Bayesian) inferences about the world in the face of incomplete information!\nKey remaining tool: separation of nodes into two categories:\n\nObserved nodes (shaded)\nLatent nodes (unshaded)\n\n\\(\\Rightarrow\\) Can use our PGM as a weather-inference machine!\nIf we observe \\(i\\) at a party, what can we infer about the weather outside?"
  },
  {
    "objectID": "w02/slides.html#observed-partier-latent-weather",
    "href": "w02/slides.html#observed-partier-latent-weather",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed Partier, Latent Weather",
    "text": "Observed Partier, Latent Weather\n\nWe can draw this situation as a PGM with shaded and unshaded nodes, distinguishing what we know from what we‚Äôd like to infer:\n\n\n\n\n\n\n\n\n\n\n‚ùì\n¬†\n‚úÖ\n\n\n\n\nAnd we can now use Bayes‚Äô Rule to compute how observed information (\\(i\\) at party \\(\\Rightarrow [Y = \\textsf{Go}]\\)) ‚Äúflows‚Äù back into \\(W\\)"
  },
  {
    "objectID": "w02/slides.html#computation-via-bayes-rule",
    "href": "w02/slides.html#computation-via-bayes-rule",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Computation via Bayes‚Äô Rule",
    "text": "Computation via Bayes‚Äô Rule\n\nBayes‚Äô Rule, \\(\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}\\), tells us how to use info about \\(\\Pr(B \\mid A)\\) to obtain info about \\(\\Pr(A \\mid B)\\)!\nWe use it to obtain a distribution for \\(W\\) updated to incorporate new info \\([Y = \\textsf{Go}]\\):\n\n\\[\n\\begin{align*}\n&\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go})\n= \\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go})} \\\\\n=\\, &\\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun}) + \\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) \\Pr(W = \\textsf{Rain})}\n\\end{align*}\n\\]\n\nPlug in info from CPT to obtain our new (conditional) probability of interest:\n\n\\[\n\\begin{align*}\n\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go}) &= \\frac{(0.8)(0.5)}{(0.8)(0.5) + (0.1)(0.5)} = \\frac{0.4}{0.4 + 0.05} \\approx 0.89\n\\end{align*}\n\\]\n\nWe‚Äôve learned something interesting! Observing \\(i\\) at the party \\(\\leadsto\\) probability of sun jumps from \\(0.5\\) (‚Äúprior‚Äù estimate of \\(W\\), best guess without any other relevant info) to \\(0.89\\) (‚Äúposterior‚Äù estimate of \\(W\\), best guess after incorporating relevant info)."
  },
  {
    "objectID": "w02/slides.html#references",
    "href": "w02/slides.html#references",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nKoller, Daphne, and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press. https://www.dropbox.com/scl/fi/6qmnr9feizngaq4005isj/Probabilistic-Graphical-Models.pdf?rlkey=4803nmsffh4xtsdfiji8jppb1&dl=1.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1."
  },
  {
    "objectID": "w02/slides.html#appendix-zero-probabilities",
    "href": "w02/slides.html#appendix-zero-probabilities",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Appendix: Zero Probabilities",
    "text": "Appendix: Zero Probabilities\nFrom Koller and Friedman (2009), pp.¬†66-67:\n\nZero probabilities: A common mistake is to assign a probability of zero to an event that is extremely unlikely, but not impossible. The problem is that one can never condition away a zero probability, no matter how much evidence we get. When an event is unlikely but not impossible, giving it probability zero is guaranteed to lead to irrecoverable errors. For example, in one of the early versions of the the Pathfinder system (box 3.D), 10 percent of the misdiagnoses were due to zero probability estimates given by the expert to events that were unlikely but not impossible.\n\n‚Üê Back to slide"
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#courtney-green",
    "href": "w02/index.html#courtney-green",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Courtney Green",
    "text": "Courtney Green\nBackground: BA in Public Policy & Leadership, now interested in using data to examine structural disparities.\nInterests & what I can help with: How sociodemographic factors (e.g., race, gender, income, immigration status, education level) shape policy outcomes and institutional practices in areas like criminal justice, housing, healthcare, education, and environment.\n\nAsk me about: Counterfactual balancing, handling messy or privacy-limited datasets, and framing causal questions around inequality.\nReach out if you‚Äôre thinking about a final project that touches social systems, systemic inequity, or fairness!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#what-got-me-interested-in-causal-inference",
    "href": "w02/index.html#what-got-me-interested-in-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Got Me Interested in Causal Inference",
    "text": "What Got Me Interested in Causal Inference\n5000 Project: Over-Policing and Wrongful Convictions in Illinois\n\nMotivation: Explore what demographic factors predict wrongful convictions using real exoneration data.\nChallenge: No data on ‚Äúnon-exonerated innocents,‚Äù so I (with prof‚Äôs help) created a counterfactual dataset by sampling from the incarcerated population\nMethod: Trained models (logistic regression, random forest, KNN) on a balanced dataset of 548 exonerated and 548 non-exonerated individuals\nFinding: Race and geography (esp.¬†Cook Count/Chicago) were the strongest predictors of exoneration\nTakeaway: Questions about systemic bias can‚Äôt be answered with just descriptive stats, you need counterfactual reasoning and causal inference\n\n(Optional) View the counterfactual setup",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#wendy-hu",
    "href": "w02/index.html#wendy-hu",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Wendy Hu",
    "text": "Wendy Hu\n\nBackground: BS in Social Work, passionate about advancing social equity through computational tools\nInterest: I bring a background in social welfare, public health, and NGO data, with experience analyzing behavioral and institutional outcomes. My work spans Indigenous health, gender equity, and trauma-informed service design‚Äîalways with a focus on equity, system change, and policy relevance.\nAsk me about: Framing social justice questions in data terms, working with community or clinical datasets and interpreting model results in context.\nIf you‚Äôre designing a project around health, inequality, nonprofit impact, or any socially embedded issue‚ÄîI‚Äôd love to help bridge rigor and relevance.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#jeffs-hw1-updateapology",
    "href": "w02/index.html#jeffs-hw1-updateapology",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Jeff‚Äôs HW1 Update/Apology",
    "text": "Jeff‚Äôs HW1 Update/Apology\n\nBasically‚Ä¶ I goofed by trying to shove a homework into the first two weeks of class üò≠\nLast week was ‚Äúsetting the table‚Äù, so there are a few (multiple-choice) questions on that\nBut, really only makes sense to have HW starting from end of today, once we‚Äôve introduced PGMs, the language we need to actually do Causal Computational Social Science!\n\n\n Due date pushed to Friday, June 6, 5:59pm",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#hw1-detail-plz-dont-hate-me",
    "href": "w02/index.html#hw1-detail-plz-dont-hate-me",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)",
    "text": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)\n(The main reason this is taking me so long)\n\nTwo key libraries for ‚Äúmain‚Äù course content (HW2 onwards):\n\nIn R: rethinking, ‚Äúlite‚Äù version of Stan\nIn Python: PyMC\n\nBoth are overkill for current unit: lots of work required to ‚Äúturn off‚Äù fancy features and implement basic PGMs\n\\(\\leadsto\\) Only way I know to achieve two goals:\n\n Learning simple PGMs first, as tools for thinking, before adding in additional full-on coding baggage of Stan/PyMC\n Having an entire textbook on these base PGMs that you can use as reference\n\nIs to use [small, restricted] subset of JavaScript, called WebPPL, because‚Ä¶\n\n\nhttp://probmods.org/",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "href": "w02/index.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality",
    "text": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality\n\n\n You‚Äôll no longer be able to read ‚Äúscientific‚Äù writing without striking this expression (involuntarily):\n\n ‚ÄúScientific‚Äù talks will begin to sound like the following:",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#blasting-off-into-causality",
    "href": "w02/index.html#blasting-off-into-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Blasting Off Into Causality!",
    "text": "Blasting Off Into Causality!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#data-generating-processes-dgps",
    "href": "w02/index.html#data-generating-processes-dgps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Data-Generating Processes (DGPs)",
    "text": "Data-Generating Processes (DGPs)\n\n\n\n\n\n\n\nYou saw this in DSAN 5100!\n¬´\\(X_1, \\ldots, X_n\\) drawn i.i.d. Normal, mean \\(\\mu\\) variance \\(\\sigma^2\\)¬ª characterizes DGP of \\((X_1, \\ldots, X_n)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n5650: Dive into DGPs, rather than treating as black box/footnote to Law of Large Numbers, so we can move [asymptotically!]‚Ä¶\nFrom associational statements:\n\n¬´\\(\\underbrace{\\text{An increase}}_{\\small\\text{noun}}\\) in \\(X\\) by 1 is associated with increase in \\(Y\\) by \\(\\beta\\)¬ª\n\nTo causal ones: ¬´\\(\\underbrace{\\text{Increasing}}_{\\small\\text{verb}}\\) \\(X\\) by 1 causes \\(Y\\) to increase by \\(\\beta\\)¬ª",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#dgps-and-the-emergence-of-order",
    "href": "w02/index.html#dgps-and-the-emergence-of-order",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "DGPs and the Emergence of Order",
    "text": "DGPs and the Emergence of Order\n\n\n\n\n\n\n\nWho can guess the state of this process after 10 steps, with 1 person?\n10 people? 50? 100? (If they find themselves on the same spot, they stand on each other‚Äôs heads)\n100 steps? 1000?",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-result-16-steps",
    "href": "w02/index.html#the-result-16-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 16 Steps",
    "text": "The Result: 16 Steps",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-result-64-steps",
    "href": "w02/index.html#the-result-64-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 64 Steps",
    "text": "The Result: 64 Steps",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#mathematicalscientific-modeling",
    "href": "w02/index.html#mathematicalscientific-modeling",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "‚ÄúMathematical/Scientific Modeling‚Äù",
    "text": "‚ÄúMathematical/Scientific Modeling‚Äù\n\nThing we observe (poking out of water): data\nHidden but possibly discoverable via deeper dive (ecosystem under surface): DGP",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#so-whats-the-problem",
    "href": "w02/index.html#so-whats-the-problem",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-shallow-problem-of-causal-inference",
    "href": "w02/index.html#the-shallow-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Shallow Problem of Causal Inference",
    "text": "The Shallow Problem of Causal Inference\n\n\n\n\n\n\n\n\ncor(ski_df$value, law_df$value)\n[1] 0.9921178\n\n(Data from Vigen, Spurious Correlations)\n\nThis, however, is only a mini-boss. Beyond it lies the truly invincible FINAL BOSS‚Ä¶ üôÄ",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-fundamental-problem-of-causal-inference",
    "href": "w02/index.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#what-is-to-be-done",
    "href": "w02/index.html#what-is-to-be-done",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#probability",
    "href": "w02/index.html#probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#beyond-conditional-probability",
    "href": "w02/index.html#beyond-conditional-probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#preview-do-calculus",
    "href": "w02/index.html#preview-do-calculus",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w02/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#ulysses-and-the-computational-sirens",
    "href": "w02/index.html#ulysses-and-the-computational-sirens",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Ulysses and the [Computational] Sirens",
    "text": "Ulysses and the [Computational] Sirens",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#bayesian-inference-but-with-pictures",
    "href": "w02/index.html#bayesian-inference-but-with-pictures",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Bayesian Inference but with Pictures",
    "text": "Bayesian Inference but with Pictures\nA Probabilistic Graphical Model (PGM) provides us with:\n\nA formal-mathematical‚Ä¶\nBut also easily visualizable (by construction)‚Ä¶\nRepresentation of a data-generating process (DGP)!\n\nExample: Let‚Äôs model how weather \\(W\\) affects evening plans \\(Y\\): the choice between going to a party or staying in to watch movies\n\n\n\n\n\n\n The Partier‚Äôs Dilemma\n\n\n\n\nA person \\(i\\) wakes up with some initial affinity for partying: \\(\\Pr(Y_i = \\textsf{Go})\\)\n\\(i\\) then goes to their window and observes the weather \\(W_i\\) outside:\n\nIf the weather is sunny, \\(i\\)‚Äôs affinity increases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Sun}) &gt; \\Pr(Y = \\textsf{Go})\\)\nOtherwise, if it is rainy, \\(i\\)‚Äôs affinity decreases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Rain}) &lt; \\Pr(Y = \\textsf{Go})\\)",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#two-main-building-blocks",
    "href": "w02/index.html#two-main-building-blocks",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Two Main ‚ÄúBuilding Blocks‚Äù",
    "text": "Two Main ‚ÄúBuilding Blocks‚Äù\n\nNodes like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}\\) denote Random Variables\n\n\\[\n\\boxed{\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}} \\simeq \\boxed{ \\begin{array}{c|cc}x & \\textsf{Tails} & \\textsf{Heads} \\\\\\hline \\Pr(X = x) & 0.5 & 0.5\\end{array}}\n\\]\n\nEdges like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) denote relationships between RVs\n\nWhat an edge ‚Äúmeans‚Äù can get [ontologically] tricky!\nRetain sanity by just remembering: an edge \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) is included in our PGM if we ‚Äúcare about‚Äù modeling the conditional probability table (CPT) of \\(Y\\) w.r.t. \\(X\\)\n\n\n\\[\n\\require{enclose}\\boxed{ \\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em} } \\simeq \\boxed{\n  \\begin{array}{c|cc}\n  x & \\Pr(Y = \\textsf{Lose} \\mid X = x) & \\Pr(Y = \\textsf{Win} \\mid X = x) \\\\\\hline\n  \\textsf{Tails} & 0.8 & 0.2 \\\\\n  \\textsf{Heads} & 0.5 & 0.5\n  \\end{array}\n}\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#pgm-for-the-partiers-dilemma",
    "href": "w02/index.html#pgm-for-the-partiers-dilemma",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "PGM for the Partier‚Äôs Dilemma",
    "text": "PGM for the Partier‚Äôs Dilemma\n\nA node \\(\\pnode{W}\\) denoting RV \\(W\\), which can take on values in \\(\\mathcal{R}_W = \\{\\textsf{Sun}, \\textsf{Rain}\\}\\),\nA node \\(\\pnode{Y}\\) denoting RV \\(Y\\), which can take on values in \\(\\mathcal{R}_Y = \\{\\textsf{Go}, \\textsf{Stay}\\}\\), and\nAn edge \\(\\pedge{W}{Y}\\) representing the following relationship between \\(W\\) and \\(Y\\):\n\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) = 0.8\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Sun}) = 0.2\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) = 0.1\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Rain}) = 0.9\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Our PGM of the Partier‚Äôs Dilemma\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\Pr(Y = \\textsf{Stay} \\mid W)\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W)\\)\n\n\n\n\n\\(W = \\textsf{Sun}\\)\n0.2\n0.8\n\n\n\\(W = \\textsf{Rain}\\)\n0.9\n0.1\n\n\n\n\n\nFigure¬†2: The Conditional Probability Table (CPT) for the edge \\(\\pedge{W}{Y}\\) in Figure¬†1",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#observed-vs.-latent-nodes",
    "href": "w02/index.html#observed-vs.-latent-nodes",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed vs.¬†Latent Nodes",
    "text": "Observed vs.¬†Latent Nodes\n\nPGMs help us make valid (Bayesian) inferences about the world in the face of incomplete information!\nKey remaining tool: separation of nodes into two categories:\n\nObserved nodes (shaded)\nLatent nodes (unshaded)\n\n\\(\\Rightarrow\\) Can use our PGM as a weather-inference machine!\nIf we observe \\(i\\) at a party, what can we infer about the weather outside?",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#observed-partier-latent-weather",
    "href": "w02/index.html#observed-partier-latent-weather",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed Partier, Latent Weather",
    "text": "Observed Partier, Latent Weather\n\nWe can draw this situation as a PGM with shaded and unshaded nodes, distinguishing what we know from what we‚Äôd like to infer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùì\n¬†\n‚úÖ\n\n\n\n\nAnd we can now use Bayes‚Äô Rule to compute how observed information (\\(i\\) at party \\(\\Rightarrow [Y = \\textsf{Go}]\\)) ‚Äúflows‚Äù back into \\(W\\)",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#computation-via-bayes-rule",
    "href": "w02/index.html#computation-via-bayes-rule",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Computation via Bayes‚Äô Rule",
    "text": "Computation via Bayes‚Äô Rule\n\nBayes‚Äô Rule, \\(\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}\\), tells us how to use info about \\(\\Pr(B \\mid A)\\) to obtain info about \\(\\Pr(A \\mid B)\\)!\nWe use it to obtain a distribution for \\(W\\) updated to incorporate new info \\([Y = \\textsf{Go}]\\):\n\n\\[\n\\begin{align*}\n&\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go})\n= \\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go})} \\\\\n=\\, &\\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun}) + \\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) \\Pr(W = \\textsf{Rain})}\n\\end{align*}\n\\]\n\nPlug in info from CPT to obtain our new (conditional) probability of interest:\n\n\\[\n\\begin{align*}\n\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go}) &= \\frac{(0.8)(0.5)}{(0.8)(0.5) + (0.1)(0.5)} = \\frac{0.4}{0.4 + 0.05} \\approx 0.89\n\\end{align*}\n\\]\n\nWe‚Äôve learned something interesting! Observing \\(i\\) at the party \\(\\leadsto\\) probability of sun jumps from \\(0.5\\) (‚Äúprior‚Äù estimate of \\(W\\), best guess without any other relevant info) to \\(0.89\\) (‚Äúposterior‚Äù estimate of \\(W\\), best guess after incorporating relevant info).",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#references",
    "href": "w02/index.html#references",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nKoller, Daphne, and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press. https://www.dropbox.com/scl/fi/6qmnr9feizngaq4005isj/Probabilistic-Graphical-Models.pdf?rlkey=4803nmsffh4xtsdfiji8jppb1&dl=1.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#appendix-zero-probabilities",
    "href": "w02/index.html#appendix-zero-probabilities",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Appendix: Zero Probabilities",
    "text": "Appendix: Zero Probabilities\nFrom Koller and Friedman (2009), pp.¬†66-67:\n\nZero probabilities: A common mistake is to assign a probability of zero to an event that is extremely unlikely, but not impossible. The problem is that one can never condition away a zero probability, no matter how much evidence we get. When an event is unlikely but not impossible, giving it probability zero is guaranteed to lead to irrecoverable errors. For example, in one of the early versions of the the Pathfinder system (box 3.D), 10 percent of the misdiagnoses were due to zero probability estimates given by the expert to events that were unlikely but not impossible.\n\n‚Üê Back to slide",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#footnotes",
    "href": "w02/index.html#footnotes",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Appendix Slide‚Ü©Ô∏é",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignment Point Distributions",
    "section": "",
    "text": "Use the tabs below to view the point distributions for different assignments.\nThe distributions are imported from Google Sheets mainly for transparency: so that you can see exactly how totals are computed as a sum of the individual points allocated for each test!\n\nHW1\n\n\n\n\n\n\n\n\n\npart\nqid\npoints\nq_total\npart_total\n\n\n\n\n1\nQ1.1a\n4\n\n\n\n\n\nQ1.1b\n2\n6\n\n\n\n\nQ1.2\n8\n8\n\n\n\n\nQ1.3a\n3\n\n\n\n\n\nQ1.3b\n3\n\n\n\n\n\nQ1.3c\n3\n9\n\n\n\n\nQ1.4a\n3\n\n\n\n\n\nQ1.4b\n3\n6\n29\n\n\n2\nQ2.1\n4\n\n\n\n\n\nQ2.2\n4\n\n\n\n\n\nQ2.3\n4\n\n\n\n\n\nQ2.4\n4\n\n16\n\n\n3\nQ3.1\n4\n\n\n\n\n\nQ3.2\n4\n\n\n\n\n\nQ3.3\n4\n\n12\n\n\n4\nQ4.1\n3\n3\n\n\n\n\nQ4.2a\n4\n\n\n\n\n\nQ4.2b\n2\n6\n\n\n\n\nQ4.3a\n2\n\n\n\n\n\nQ4.3b\n2\n4\n\n\n\n\nQ4.4a\n2\n\n\n\n\n\nQ4.4b\n2\n4\n\n\n\n\nQ4.5a\n2\n\n\n\n\n\nQ4.5b\n2\n4\n\n\n\n\nQ4.6a\n2\n\n\n\n\n\nQ4.6b\n2\n4\n25\n\n\n5\nQ5.1\n4\n4\n\n\n\n\nQ5.2\n2\n2\n\n\n\n\nQ5.3\n2\n\n\n\n\n\nQ5.4a\n4\n\n\n\n\n\nQ5.4b\n3\n\n\n\n\n\nQ5.4c\n3\n10\n18\n\n\n\nTotal\n100",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "w03/slides.html#so-whats-the-problem",
    "href": "w03/slides.html#so-whats-the-problem",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often\n\nSee Appendix Slide"
  },
  {
    "objectID": "w03/slides.html#the-fundamental-problem-of-causal-inference",
    "href": "w03/slides.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠"
  },
  {
    "objectID": "w03/slides.html#what-is-to-be-done",
    "href": "w03/slides.html#what-is-to-be-done",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?"
  },
  {
    "objectID": "w03/slides.html#probability",
    "href": "w03/slides.html#probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring."
  },
  {
    "objectID": "w03/slides.html#beyond-conditional-probability",
    "href": "w03/slides.html#beyond-conditional-probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects"
  },
  {
    "objectID": "w03/slides.html#preview-do-calculus",
    "href": "w03/slides.html#preview-do-calculus",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events"
  },
  {
    "objectID": "w03/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w03/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!"
  },
  {
    "objectID": "w03/slides.html#importance-of-observed-vs.-latent-distinction",
    "href": "w03/slides.html#importance-of-observed-vs.-latent-distinction",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Importance of Observed vs.¬†Latent Distinction!",
    "text": "Importance of Observed vs.¬†Latent Distinction!\n\nAcross many different fields, hidden stumbling-block in your project may be failure to model this distinction and pursue its implications!\n\n\n\n\n\n\n\n\n\nFigure¬†1: Your model failing to achieve its goal bc you haven‚Äôt yet distinguished observed vs.¬†latent variables"
  },
  {
    "objectID": "w03/slides.html#example-from-cognitive-neuroscience-visual-perception",
    "href": "w03/slides.html#example-from-cognitive-neuroscience-visual-perception",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Example from Cognitive Neuroscience: Visual Perception",
    "text": "Example from Cognitive Neuroscience: Visual Perception\n\n\n\n\n\n\n\nWe ‚Äúsee‚Äù 3D objects like a basketballs, but our eyes are (curved) 2D surfaces!\n\\(\\Rightarrow\\) Our brains construct 3D environment by combining 2D info (observed photons-hitting-light-cones) with latent heuristic info:\n\nInstantaneous Binocular Disparity, fusing info from two slightly-offset eyes,\nShort-term Motion Parallax: How does object shift over short temporal ‚Äúwindows‚Äù of movement?\nLong-term mental models (orange-ish circle with this line pattern is usually a basketball, which is usually this big, etc.)\n\n\n\n\n\n\n\nImage source (a very cool article!)\n\n\n\n\n\n\nSimilar examples in many other fields \\(\\leadsto\\) science is a strange waltz of general models vs.¬†field-specific details, but there‚Äôs one model that is infinitely helpful imo‚Ä¶"
  },
  {
    "objectID": "w03/slides.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "href": "w03/slides.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!",
    "text": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!\n\nUsing ‚ÄúUr‚Äù in the same sense as ‚ÄúAmerica‚Äôs Ur-Choropleths‚Äù‚Ä¶\nHMMs are our ‚ÄúUr-Models‚Äù for Computational Social Science specifically\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs consider an extremely currently-popular strand of CSS research, and step through why (a) it may be harder than it initially seems, but (b) we can use HMMs to ‚Äúorganize‚Äù/manage/visualize the complexity!\n\n\nAs in, how America‚Äôs Ur-Choropleths are two visualizations you can keep at hand before launching into a specific nation-wide choropleth about a specific issue!"
  },
  {
    "objectID": "w03/slides.html#studying-fake-news",
    "href": "w03/slides.html#studying-fake-news",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Studying ‚ÄúFake News‚Äù",
    "text": "Studying ‚ÄúFake News‚Äù\n\n\n\n\n\n\n\nStudying ‚Äúfake news‚Äù with ML and/or Deep Learning and/or Big Data is very popular in Computational Social Science: let‚Äôs use HMMs to see why it might be more‚Ä¶ difficult/complicated than it seems at first üôà\n\n\n\n\n\nThe (implicit) model in studies like Iyengar and Kinder (2010) is something like:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThus allowing results to be summarized in a table like:\n\n\n\n\n\n\n\n\n\n\n\nIyengar and Kinder (2010)"
  },
  {
    "objectID": "w03/slides.html#the-devil-in-the-details-i",
    "href": "w03/slides.html#the-devil-in-the-details-i",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details I",
    "text": "The Devil in the Details I\n\nResidents of the New Haven, Connecticut area participated in one of two experiments, each of which spanned six consecutive days [‚Ä¶] took place in November 1980, shortly after the presidential election\n\n\nWe measured problem importance with four questions that appeared in both the pretreatment and posttreatment questionnaires:\n\nPlease indicate how important you consider these problems to be.\nShould the federal government do more to develop solutions to these problems, even if it means raising taxes?\nHow much do you yourself care about these problems?\nThese days how much do you talk about these problems?"
  },
  {
    "objectID": "w03/slides.html#the-devil-in-the-details-ii",
    "href": "w03/slides.html#the-devil-in-the-details-ii",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details II",
    "text": "The Devil in the Details II"
  },
  {
    "objectID": "w03/slides.html#randomization-and-fine-tuned-treatment",
    "href": "w03/slides.html#randomization-and-fine-tuned-treatment",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Randomization and Fine-Tuned Treatment",
    "text": "Randomization and Fine-Tuned Treatment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶These are the types of things we usually don‚Äôt have control over as data scientists (we‚Äôre just handed a .csv!)"
  },
  {
    "objectID": "w03/slides.html#lets-model-it",
    "href": "w03/slides.html#lets-model-it",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Let‚Äôs Model It!",
    "text": "Let‚Äôs Model It!"
  },
  {
    "objectID": "w03/slides.html#the-final-piece-plate-notation",
    "href": "w03/slides.html#the-final-piece-plate-notation",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Final Piece: Plate Notation",
    "text": "The Final Piece: Plate Notation\n\nFor describing general distributions, there is often a ‚Äúsingle node generating a bunch of nodes‚Äù structure:\n\n\n\n\n\n\n\nPGM notation has a built-in tool for this: plates!"
  },
  {
    "objectID": "w03/slides.html#crucial-css-model-we-can-now-dive-into",
    "href": "w03/slides.html#crucial-css-model-we-can-now-dive-into",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Crucial CSS Model We Can Now Dive Into!",
    "text": "Crucial CSS Model We Can Now Dive Into!\n\nImage source"
  },
  {
    "objectID": "w03/slides.html#what-does-this-give-us",
    "href": "w03/slides.html#what-does-this-give-us",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Does This Give Us?",
    "text": "What Does This Give Us?"
  },
  {
    "objectID": "w03/slides.html#before-we-branch-off-of-pgms",
    "href": "w03/slides.html#before-we-branch-off-of-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Before We Branch Off Of PGMs",
    "text": "Before We Branch Off Of PGMs\n(Even in non-causal settings)\n\nImage source\nWe don‚Äôt exactly think ‚ÄúShakespeare decided on a set of topics,‚Äù"
  },
  {
    "objectID": "w03/slides.html#getting-shot-by-a-firing-squad",
    "href": "w03/slides.html#getting-shot-by-a-firing-squad",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Getting Shot By A Firing Squad",
    "text": "Getting Shot By A Firing Squad\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(O\\)\n\\(C\\)\n\\(A\\)\n\\(B\\)\n\\(D\\)\n\n\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n\n\\[\n\\begin{align*}\n\\Pr(D) &= 0.4 \\\\\n\\Pr(D \\mid A) &= 1\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w03/slides.html#references",
    "href": "w03/slides.html#references",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nIyengar, Shanto, and Donald R. Kinder. 2010. News That Matters: Television & American Opinion. University of Chicago Press.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1."
  },
  {
    "objectID": "w03/index.html",
    "href": "w03/index.html",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#so-whats-the-problem",
    "href": "w03/index.html#so-whats-the-problem",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-fundamental-problem-of-causal-inference",
    "href": "w03/index.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#what-is-to-be-done",
    "href": "w03/index.html#what-is-to-be-done",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#probability",
    "href": "w03/index.html#probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring.",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#beyond-conditional-probability",
    "href": "w03/index.html#beyond-conditional-probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#preview-do-calculus",
    "href": "w03/index.html#preview-do-calculus",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w03/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#importance-of-observed-vs.-latent-distinction",
    "href": "w03/index.html#importance-of-observed-vs.-latent-distinction",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Importance of Observed vs.¬†Latent Distinction!",
    "text": "Importance of Observed vs.¬†Latent Distinction!\n\nAcross many different fields, hidden stumbling-block in your project may be failure to model this distinction and pursue its implications!\n\n\n\n\n\n\n\n\n\nFigure¬†1: Your model failing to achieve its goal bc you haven‚Äôt yet distinguished observed vs.¬†latent variables",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#example-from-cognitive-neuroscience-visual-perception",
    "href": "w03/index.html#example-from-cognitive-neuroscience-visual-perception",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Example from Cognitive Neuroscience: Visual Perception",
    "text": "Example from Cognitive Neuroscience: Visual Perception\n\n\n\n\n\n\n\nWe ‚Äúsee‚Äù 3D objects like a basketballs, but our eyes are (curved) 2D surfaces!\n\\(\\Rightarrow\\) Our brains construct 3D environment by combining 2D info (observed photons-hitting-light-cones) with latent heuristic info:\n\nInstantaneous Binocular Disparity, fusing info from two slightly-offset eyes,\nShort-term Motion Parallax: How does object shift over short temporal ‚Äúwindows‚Äù of movement?\nLong-term mental models (orange-ish circle with this line pattern is usually a basketball, which is usually this big, etc.)\n\n\n\n\n\n\n\nImage source (a very cool article!)\n\n\n\n\n\n\nSimilar examples in many other fields \\(\\leadsto\\) science is a strange waltz of general models vs.¬†field-specific details, but there‚Äôs one model that is infinitely helpful imo‚Ä¶",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "href": "w03/index.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!",
    "text": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!\n\nUsing ‚ÄúUr‚Äù in the same sense as ‚ÄúAmerica‚Äôs Ur-Choropleths‚Äù‚Ä¶\nHMMs are our ‚ÄúUr-Models‚Äù for Computational Social Science specifically\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs consider an extremely currently-popular strand of CSS research, and step through why (a) it may be harder than it initially seems, but (b) we can use HMMs to ‚Äúorganize‚Äù/manage/visualize the complexity!\n\n\nAs in, how America‚Äôs Ur-Choropleths are two visualizations you can keep at hand before launching into a specific nation-wide choropleth about a specific issue!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#studying-fake-news",
    "href": "w03/index.html#studying-fake-news",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Studying ‚ÄúFake News‚Äù",
    "text": "Studying ‚ÄúFake News‚Äù\n\n\n\n\n\n\n\n\n\n\nStudying ‚Äúfake news‚Äù with ML and/or Deep Learning and/or Big Data is very popular in Computational Social Science: let‚Äôs use HMMs to see why it might be more‚Ä¶ difficult/complicated than it seems at first üôà\n\n\n\n\n\nThe (implicit) model in studies like Iyengar and Kinder (2010) is something like:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThus allowing results to be summarized in a table like:\n\n\n\n\n\n\n\n\n\n\n\nIyengar and Kinder (2010)",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-devil-in-the-details-i",
    "href": "w03/index.html#the-devil-in-the-details-i",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details I",
    "text": "The Devil in the Details I\n\nResidents of the New Haven, Connecticut area participated in one of two experiments, each of which spanned six consecutive days [‚Ä¶] took place in November 1980, shortly after the presidential election\n\n\nWe measured problem importance with four questions that appeared in both the pretreatment and posttreatment questionnaires:\n\nPlease indicate how important you consider these problems to be.\nShould the federal government do more to develop solutions to these problems, even if it means raising taxes?\nHow much do you yourself care about these problems?\nThese days how much do you talk about these problems?",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-devil-in-the-details-ii",
    "href": "w03/index.html#the-devil-in-the-details-ii",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details II",
    "text": "The Devil in the Details II",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#randomization-and-fine-tuned-treatment",
    "href": "w03/index.html#randomization-and-fine-tuned-treatment",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Randomization and Fine-Tuned Treatment",
    "text": "Randomization and Fine-Tuned Treatment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶These are the types of things we usually don‚Äôt have control over as data scientists (we‚Äôre just handed a .csv!)",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#lets-model-it",
    "href": "w03/index.html#lets-model-it",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Let‚Äôs Model It!",
    "text": "Let‚Äôs Model It!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-final-piece-plate-notation",
    "href": "w03/index.html#the-final-piece-plate-notation",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Final Piece: Plate Notation",
    "text": "The Final Piece: Plate Notation\n\nFor describing general distributions, there is often a ‚Äúsingle node generating a bunch of nodes‚Äù structure:\n\n\n\n\n\n\n\nPGM notation has a built-in tool for this: plates!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#crucial-css-model-we-can-now-dive-into",
    "href": "w03/index.html#crucial-css-model-we-can-now-dive-into",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Crucial CSS Model We Can Now Dive Into!",
    "text": "Crucial CSS Model We Can Now Dive Into!\n\n\n\nImage source",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#what-does-this-give-us",
    "href": "w03/index.html#what-does-this-give-us",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Does This Give Us?",
    "text": "What Does This Give Us?",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#before-we-branch-off-of-pgms",
    "href": "w03/index.html#before-we-branch-off-of-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Before We Branch Off Of PGMs",
    "text": "Before We Branch Off Of PGMs\n(Even in non-causal settings)\n\n\n\nImage source\n\n\n\nWe don‚Äôt exactly think ‚ÄúShakespeare decided on a set of topics,‚Äù",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#getting-shot-by-a-firing-squad",
    "href": "w03/index.html#getting-shot-by-a-firing-squad",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Getting Shot By A Firing Squad",
    "text": "Getting Shot By A Firing Squad\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(O\\)\n\\(C\\)\n\\(A\\)\n\\(B\\)\n\\(D\\)\n\n\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n\n\\[\n\\begin{align*}\n\\Pr(D) &= 0.4 \\\\\n\\Pr(D \\mid A) &= 1\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#references",
    "href": "w03/index.html#references",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nIyengar, Shanto, and Donald R. Kinder. 2010. News That Matters: Television & American Opinion. University of Chicago Press.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1.",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#footnotes",
    "href": "w03/index.html#footnotes",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Appendix Slide‚Ü©Ô∏é",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html",
    "href": "w04/index.html",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "href": "w04/index.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?",
    "text": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?\n\nMy answer: allows you to adapt to specifics/idiosyncrasies of your problem!\nLanguage metaphor: Learning models vs.¬†learning modeling language \\(\\Leftrightarrow\\) Learning phrases in a language vs.¬†learning to speak the language\n‚ÄúHello, one hamburger please‚Äù is good, but what if you‚Ä¶\n Are allergic to ketchup and need to make sure it‚Äôs removed\n Want to replace sesame seed bun with poppy seed bun, if they have it\n Prefer spicy, but not too spicy, mustard  Bun only  Animal style  ‚Ä¶\n\n\n\n\n\n\n\n\nLanguages give us a syntax‚Ä¶\n\n\n\nS\n\\(\\rightarrow\\)\nNP VP\n\n\nNP\n\\(\\rightarrow\\)\nDetP N | AdjP NP\n\n\nVP\n\\(\\rightarrow\\)\nV NP\n\n\nAdjP\n\\(\\rightarrow\\)\nAdj | Adv AdjP\n\n\nN\n\\(\\rightarrow\\)\nfrog | tadpole\n\n\nV\n\\(\\rightarrow\\)\nsees | likes\n\n\nAdj\n\\(\\rightarrow\\)\nbig | small\n\n\nAdv\n\\(\\rightarrow\\)\nvery\n\n\nDetP\n\\(\\rightarrow\\)\na | the\n\n\n\n\n\n\n\n‚Ä¶For expressing arbitrary (infinitely many!) sentences",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "href": "w04/index.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)",
    "text": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)\nNeed a language that can communicate the following info to estimation algorithm:\n\n Unit of observation is tadpole, but unit of analysis is tank\n Ultimately, I care about \\(Y =\\) survival rate (dependent var), as function of \\(X =\\) tank properties (independent var)\n ‚Ä¶But the \\(n_i = 48\\) tanks actually come in \\(n_j = 3\\) types: small (10 bois), medium (25), large (35) (Bonus: What if there are different numbers of tanks per type?)\n I need it to account for impact of tank size, then  pool info across tank sizes\n\n\n\n\nFrom McElreath (2020)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#example-2-dissertation-nightmare",
    "href": "w04/index.html#example-2-dissertation-nightmare",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 2: Dissertation Nightmare",
    "text": "Example 2: Dissertation Nightmare\n\n\n\n\n\n\n\n\n\nAbove: Data from Soviet archives; Above Right: US Military archives; Below Right: NATO archives",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#nightmarish-without-a-modeling-language",
    "href": "w04/index.html#nightmarish-without-a-modeling-language",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Nightmarish Without a Modeling Language!",
    "text": "Nightmarish Without a Modeling Language!\n\nModeling language \\(\\Rightarrow\\) Unambiguously ‚Äúencode‚Äù idiosyncratic domain knowledge\nDissertation: Cold War \\(\\times\\) ‚ÄúThird World‚Äù \\(\\leadsto\\) Cuban üá®üá∫ trans-continental operations1\nMain narrative (for estimation): 1975 (South Africa invades Angola, 14 Oct ‚Üí üá®üá∫ intervention, 4 Nov) to 1979 (USSR requests üá®üá∫ troops to Ethiopia for Ogaden War)\n[Ontology] Fix 1979 geographic entities at National level (as modeling choice, like fixing 2000 USD to measure inflation): \\(\\textsf{Cuba}_{1979}\\), \\(\\textsf{Angola}_{1979}\\), \\(\\textsf{PDRY}_{1979}\\), \\(\\textsf{YAR}_{1979}\\)\nDifferent tokens (Think NLP: \"Congo\", \"DRC\", \"Republic of Congo\") can then be contextualized: can ‚Äútrack‚Äù and link data appropriately despite splits, merges, name changes\nSay we have data on ‚ÄúNumber of Communist Militants in \\(X\\)‚Äù (Hoover Yearbook)‚Ä¶\n\n\n\n\nEntity\nData from 1947-1971 at...\nData from 1971-Present at...\n\n\n\n\n\\(\\textsf{Pakistan}_{1979}\\)\nNational Level:\n\\(\\frac{62}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúPakistan‚Äù\n\n\nSubnational Level:\n‚ÄúWest Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)\n\n\n\\(\\textsf{Bangladesh}_{1979}\\)\nNational Level:\n\\(\\frac{70}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúBangladesh‚Äù\n\n\nSubnational Level:\n‚ÄúEast Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#the-fork-x-leftarrow-z-rightarrow-y",
    "href": "w04/index.html#the-fork-x-leftarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)",
    "text": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\nfork_df &lt;- tibble(\n    Z = rbern(n_d),\n    X = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\nplot_freqs &lt;- function(df, plot_title, y_lab=TRUE) {\n  df_cor &lt;- cor(df$X, df$Y)\n  df_label &lt;- paste0(\"Cor(X,Y) = \",round(df_cor,3))\n  freq_df &lt;- df |&gt;\n    group_by(X, Y) |&gt;\n    summarize(count=n())\n  freq_plot &lt;- freq_df |&gt;\n    ggplot(\n      aes(x=factor(X), y=factor(Y), fill=count)\n    ) +\n    geom_tile() +\n    coord_equal() +\n    scale_fill_distiller(\n      palette=\"Greens\", direction=1,\n      limits=c(0,5000)\n    ) +\n    geom_label(\n      aes(label=count),\n      fill=\"white\", color=\"black\", size=7\n    ) +\n    labs(\n      title = plot_title,\n      subtitle = df_label,\n      x=\"X\", y=\"Y\"\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      plot.title = element_text(size=21),\n      plot.subtitle = element_text(size=18)\n    ) +\n    remove_legend()\n  if (!y_lab) {\n    freq_plot &lt;- freq_plot + theme(\n      axis.title.y = element_blank()\n    )\n  }\n  return(freq_plot)\n}\n# The full df\nfull_label &lt;- paste0(\"Raw Data (n = 10K)\")\nfull_plot &lt;- plot_freqs(fork_df, full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 0\nz0_df &lt;- fork_df |&gt; filter(Z == 0)\nz0_n &lt;- nrow(z0_df)\nz0_label &lt;- paste0(\"Z == 0 (\",z0_n,\" obs)\")\nz0_plot &lt;- plot_freqs(z0_df, z0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 1\nz1_df &lt;- fork_df |&gt; filter(Z == 1)\nz1_n &lt;- nrow(z1_df)\nz1_label &lt;- paste0(\"Z == 1 (\",z1_n,\" obs)\")\nz1_plot &lt;- plot_freqs(z1_df, z1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\nfull_plot | z0_plot | z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"$Slope_{Z=0} = \",z0_slope,\"$\")\nz0_leg_label &lt;- TeX(paste0(\"0 $(m=\",z0_slope,\")$\"))\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"$Slope_{Z=1} = \",z1_slope,\"$\")\nz_texlabel &lt;- TeX(paste0(z0_label, \" | \", z1_label))\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "href": "w04/index.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)",
    "text": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\npipe_df &lt;- tibble(\n    X = rbern(n_d),\n    Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\n# The full df\npipe_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\npipe_full_plot &lt;- plot_freqs(pipe_df, pipe_full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 0\npipe_z0_df &lt;- pipe_df |&gt; filter(Z == 0)\npipe_z0_n &lt;- nrow(pipe_z0_df)\npipe_z0_label &lt;- paste0(\"Z == 0 (\",pipe_z0_n,\" obs)\")\npipe_z0_plot &lt;- plot_freqs(pipe_z0_df, pipe_z0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 1\npipe_z1_df &lt;- pipe_df |&gt; filter(Z == 1)\npipe_z1_n &lt;- nrow(pipe_z1_df)\npipe_z1_label &lt;- paste0(\"Z == 1 (\",pipe_z1_n,\" obs)\")\npipe_z1_plot &lt;- plot_freqs(pipe_z1_df, pipe_z1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\npipe_full_plot | pipe_z0_plot | pipe_z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",cpipe_z0_slope,\"$\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",cpipe_z1_slope,\"$\")\ncpipe_z_texlabel &lt;- TeX(paste0(cpipe_z0_label, \" | \", cpipe_z1_label))\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    subtitle=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#the-collider-x-rightarrow-z-leftarrow-y",
    "href": "w04/index.html#the-collider-x-rightarrow-z-leftarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)",
    "text": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)\n\n\n\n\n\nCode\nset.seed(5650)\ncoll_df &lt;- tibble(\n    X = rbern(n_d),\n    Y = rbern(n_d),\n    Z = rbern(n_d, ifelse(X + Y &gt; 0, 0.9, 0.2)),\n)\n\n\n\n\nCode\n# The full df\ncoll_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\ncoll_full_plot &lt;- plot_freqs(coll_df, coll_full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 0\ncoll_z0_df &lt;- coll_df |&gt; filter(Z == 0)\ncoll_z0_n &lt;- nrow(coll_z0_df)\ncoll_z0_label &lt;- paste0(\"Z == 0 (\",coll_z0_n,\" obs)\")\ncoll_z0_plot &lt;- plot_freqs(coll_z0_df, coll_z0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 1\ncoll_z1_df &lt;- coll_df |&gt; filter(Z == 1)\ncoll_z1_n &lt;- nrow(coll_z1_df)\ncoll_z1_label &lt;- paste0(\"Z == 1 (\",coll_z1_n,\" obs)\")\ncoll_z1_plot &lt;- plot_freqs(coll_z1_df, coll_z1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\ncoll_full_plot | coll_z0_plot | coll_z1_plot\n\n\n\n\n\n\n\n\n\n\nConditioning on colliders induces correlation where there previously was none ‚ò†Ô∏è\n\n\n\n\n\nCode\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\n\n\n\n\nCode\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",ccoll_z0_slope,\"$\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",ccoll_z1_slope,\"$\")\nccoll_z_texlabel &lt;- TeX(paste0(ccoll_z0_label, \" | \", ccoll_z1_label))\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",ccoll_slope\n    ),\n    subtitle=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶This is why we have to think, rather than just ‚Äúcontrol for everything‚Äù! üò≠",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#proxies-for-z",
    "href": "w04/index.html#proxies-for-z",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Proxies for \\(Z\\)",
    "text": "Proxies for \\(Z\\)\n\n\n\n\n\nCode\nset.seed(5650)\nprox_df &lt;- tibble(\n  X = rbern(n_d),\n  Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n  Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n  A = rbern(n_d, (1-Z)*0.1 + Z*0.9)\n)\n\n\n\n\nCode\n# The full df\nprox_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\nprox_full_plot &lt;- plot_freqs(prox_df, prox_full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on A == 0\nprox_a0_df &lt;- prox_df |&gt; filter(A == 0)\nprox_a0_n &lt;- nrow(prox_a0_df)\nprox_a0_label &lt;- paste0(\"A == 0 (\",prox_a0_n,\" obs)\")\nprox_a0_plot &lt;- plot_freqs(prox_a0_df, prox_a0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on A == 1\nprox_a1_df &lt;- prox_df |&gt; filter(A == 1)\nprox_a1_n &lt;- nrow(prox_a1_df)\nprox_a1_label &lt;- paste0(\"A == 1 (\",prox_a1_n,\" obs)\")\nprox_a1_plot &lt;- plot_freqs(prox_a1_df, prox_a1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\nprox_full_plot | prox_a0_plot | prox_a1_plot\n\n\n\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A\\) gives us some (not all!) information about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\nn_c &lt;- 300\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"$Slope_{A=0} = \",cprox_a0_slope,\"$\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"$Slope_{A=1} = \",cprox_a1_slope,\"$\")\ncprox_a_texlabel &lt;- TeX(paste0(cprox_a0_label, \" | \", cprox_a1_label))\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_text(size=18)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#references",
    "href": "w04/index.html#references",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "References",
    "text": "References\n\n\nBezuidenhout, Dana, Sarah Forthal, Kara Rudolph, and Matthew R Lamb. 2024. ‚ÄúSingle World Intervention Graphs (SWIGs): A Practical Guide.‚Äù American Journal of Epidemiology, September, kwae353. https://doi.org/10.1093/aje/kwae353.\n\n\nGleijeses, Piero. 2013. Visions of Freedom: Havana, Washington, Pretoria, and the Struggle for Southern Africa, 1976-1991. UNC Press Books.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and STAN. CRC Press.",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#footnotes",
    "href": "w04/index.html#footnotes",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHelpful metaphor (Gleijeses 2013): Cuba \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for USSR (Soviet $ but Cuban training of PAIGC ‚Üí MPLA), as Israel \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for US (US $ but Israeli training of SAVAK ‚Üí SADF)‚Ü©Ô∏é",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/slides.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "href": "w04/slides.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?",
    "text": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?\n\nMy answer: allows you to adapt to specifics/idiosyncrasies of your problem!\nLanguage metaphor: Learning models vs.¬†learning modeling language \\(\\Leftrightarrow\\) Learning phrases in a language vs.¬†learning to speak the language\n‚ÄúHello, one hamburger please‚Äù is good, but what if you‚Ä¶\n Are allergic to ketchup and need to make sure it‚Äôs removed\n Want to replace sesame seed bun with poppy seed bun, if they have it\n Prefer spicy, but not too spicy, mustard  Bun only  Animal style  ‚Ä¶\n\n\n\n\n\n\n\n\nLanguages give us a syntax‚Ä¶\n\n\n\nS\n\\(\\rightarrow\\)\nNP VP\n\n\nNP\n\\(\\rightarrow\\)\nDetP N | AdjP NP\n\n\nVP\n\\(\\rightarrow\\)\nV NP\n\n\nAdjP\n\\(\\rightarrow\\)\nAdj | Adv AdjP\n\n\nN\n\\(\\rightarrow\\)\nfrog | tadpole\n\n\nV\n\\(\\rightarrow\\)\nsees | likes\n\n\nAdj\n\\(\\rightarrow\\)\nbig | small\n\n\nAdv\n\\(\\rightarrow\\)\nvery\n\n\nDetP\n\\(\\rightarrow\\)\na | the\n\n\n\n\n\n\n\n‚Ä¶For expressing arbitrary (infinitely many!) sentences"
  },
  {
    "objectID": "w04/slides.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "href": "w04/slides.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)",
    "text": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)\nNeed a language that can communicate the following info to estimation algorithm:\n\n Unit of observation is tadpole, but unit of analysis is tank\n Ultimately, I care about \\(Y =\\) survival rate (dependent var), as function of \\(X =\\) tank properties (independent var)\n ‚Ä¶But the \\(n_i = 48\\) tanks actually come in \\(n_j = 3\\) types: small (10 bois), medium (25), large (35) (Bonus: What if there are different numbers of tanks per type?)\n I need it to account for impact of tank size, then  pool info across tank sizes\n\n\nFrom McElreath (2020)"
  },
  {
    "objectID": "w04/slides.html#example-2-dissertation-nightmare",
    "href": "w04/slides.html#example-2-dissertation-nightmare",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 2: Dissertation Nightmare",
    "text": "Example 2: Dissertation Nightmare\n\n\n\n\n\n\n\n\n\nAbove: Data from Soviet archives; Above Right: US Military archives; Below Right: NATO archives"
  },
  {
    "objectID": "w04/slides.html#nightmarish-without-a-modeling-language",
    "href": "w04/slides.html#nightmarish-without-a-modeling-language",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Nightmarish Without a Modeling Language!",
    "text": "Nightmarish Without a Modeling Language!\n\nModeling language \\(\\Rightarrow\\) Unambiguously ‚Äúencode‚Äù idiosyncratic domain knowledge\nDissertation: Cold War \\(\\times\\) ‚ÄúThird World‚Äù \\(\\leadsto\\) Cuban üá®üá∫ trans-continental operations1\nMain narrative (for estimation): 1975 (South Africa invades Angola, 14 Oct ‚Üí üá®üá∫ intervention, 4 Nov) to 1979 (USSR requests üá®üá∫ troops to Ethiopia for Ogaden War)\n[Ontology] Fix 1979 geographic entities at National level (as modeling choice, like fixing 2000 USD to measure inflation): \\(\\textsf{Cuba}_{1979}\\), \\(\\textsf{Angola}_{1979}\\), \\(\\textsf{PDRY}_{1979}\\), \\(\\textsf{YAR}_{1979}\\)\nDifferent tokens (Think NLP: \"Congo\", \"DRC\", \"Republic of Congo\") can then be contextualized: can ‚Äútrack‚Äù and link data appropriately despite splits, merges, name changes\nSay we have data on ‚ÄúNumber of Communist Militants in \\(X\\)‚Äù (Hoover Yearbook)‚Ä¶\n\n\n\n\nEntity\nData from 1947-1971 at...\nData from 1971-Present at...\n\n\n\n\n\\(\\textsf{Pakistan}_{1979}\\)\nNational Level:\n\\(\\frac{62}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúPakistan‚Äù\n\n\nSubnational Level:\n‚ÄúWest Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)\n\n\n\\(\\textsf{Bangladesh}_{1979}\\)\nNational Level:\n\\(\\frac{70}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúBangladesh‚Äù\n\n\nSubnational Level:\n‚ÄúEast Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)\n\n\n\nHelpful metaphor (Gleijeses 2013): Cuba \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for USSR (Soviet $ but Cuban training of PAIGC ‚Üí MPLA), as Israel \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for US (US $ but Israeli training of SAVAK ‚Üí SADF)"
  },
  {
    "objectID": "w04/slides.html#the-fork-x-leftarrow-z-rightarrow-y",
    "href": "w04/slides.html#the-fork-x-leftarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)",
    "text": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\nfork_df &lt;- tibble(\n    Z = rbern(n_d),\n    X = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\nplot_freqs &lt;- function(df, plot_title, y_lab=TRUE) {\n  df_cor &lt;- cor(df$X, df$Y)\n  df_label &lt;- paste0(\"Cor(X,Y) = \",round(df_cor,3))\n  freq_df &lt;- df |&gt;\n    group_by(X, Y) |&gt;\n    summarize(count=n())\n  freq_plot &lt;- freq_df |&gt;\n    ggplot(\n      aes(x=factor(X), y=factor(Y), fill=count)\n    ) +\n    geom_tile() +\n    coord_equal() +\n    scale_fill_distiller(\n      palette=\"Greens\", direction=1,\n      limits=c(0,5000)\n    ) +\n    geom_label(\n      aes(label=count),\n      fill=\"white\", color=\"black\", size=7\n    ) +\n    labs(\n      title = plot_title,\n      subtitle = df_label,\n      x=\"X\", y=\"Y\"\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      plot.title = element_text(size=21),\n      plot.subtitle = element_text(size=18)\n    ) +\n    remove_legend()\n  if (!y_lab) {\n    freq_plot &lt;- freq_plot + theme(\n      axis.title.y = element_blank()\n    )\n  }\n  return(freq_plot)\n}\n# The full df\nfull_label &lt;- paste0(\"Raw Data (n = 10K)\")\nfull_plot &lt;- plot_freqs(fork_df, full_label)\n# Conditioning on Z = 0\nz0_df &lt;- fork_df |&gt; filter(Z == 0)\nz0_n &lt;- nrow(z0_df)\nz0_label &lt;- paste0(\"Z == 0 (\",z0_n,\" obs)\")\nz0_plot &lt;- plot_freqs(z0_df, z0_label, y_lab=FALSE)\n# Conditioning on Z = 1\nz1_df &lt;- fork_df |&gt; filter(Z == 1)\nz1_n &lt;- nrow(z1_df)\nz1_label &lt;- paste0(\"Z == 1 (\",z1_n,\" obs)\")\nz1_plot &lt;- plot_freqs(z1_df, z1_label, y_lab=FALSE)\nfull_plot | z0_plot | z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"$Slope_{Z=0} = \",z0_slope,\"$\")\nz0_leg_label &lt;- TeX(paste0(\"0 $(m=\",z0_slope,\")$\"))\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"$Slope_{Z=1} = \",z1_slope,\"$\")\nz_texlabel &lt;- TeX(paste0(z0_label, \" | \", z1_label))\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w04/slides.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "href": "w04/slides.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)",
    "text": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\npipe_df &lt;- tibble(\n    X = rbern(n_d),\n    Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\n# The full df\npipe_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\npipe_full_plot &lt;- plot_freqs(pipe_df, pipe_full_label)\n# Conditioning on Z = 0\npipe_z0_df &lt;- pipe_df |&gt; filter(Z == 0)\npipe_z0_n &lt;- nrow(pipe_z0_df)\npipe_z0_label &lt;- paste0(\"Z == 0 (\",pipe_z0_n,\" obs)\")\npipe_z0_plot &lt;- plot_freqs(pipe_z0_df, pipe_z0_label, y_lab=FALSE)\n# Conditioning on Z = 1\npipe_z1_df &lt;- pipe_df |&gt; filter(Z == 1)\npipe_z1_n &lt;- nrow(pipe_z1_df)\npipe_z1_label &lt;- paste0(\"Z == 1 (\",pipe_z1_n,\" obs)\")\npipe_z1_plot &lt;- plot_freqs(pipe_z1_df, pipe_z1_label, y_lab=FALSE)\npipe_full_plot | pipe_z0_plot | pipe_z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",cpipe_z0_slope,\"$\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",cpipe_z1_slope,\"$\")\ncpipe_z_texlabel &lt;- TeX(paste0(cpipe_z0_label, \" | \", cpipe_z1_label))\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    subtitle=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w04/slides.html#the-collider-x-rightarrow-z-leftarrow-y",
    "href": "w04/slides.html#the-collider-x-rightarrow-z-leftarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)",
    "text": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)\n\n\n\n\n\nCode\nset.seed(5650)\ncoll_df &lt;- tibble(\n    X = rbern(n_d),\n    Y = rbern(n_d),\n    Z = rbern(n_d, ifelse(X + Y &gt; 0, 0.9, 0.2)),\n)\n\n\n\n\nCode\n# The full df\ncoll_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\ncoll_full_plot &lt;- plot_freqs(coll_df, coll_full_label)\n# Conditioning on Z = 0\ncoll_z0_df &lt;- coll_df |&gt; filter(Z == 0)\ncoll_z0_n &lt;- nrow(coll_z0_df)\ncoll_z0_label &lt;- paste0(\"Z == 0 (\",coll_z0_n,\" obs)\")\ncoll_z0_plot &lt;- plot_freqs(coll_z0_df, coll_z0_label, y_lab=FALSE)\n# Conditioning on Z = 1\ncoll_z1_df &lt;- coll_df |&gt; filter(Z == 1)\ncoll_z1_n &lt;- nrow(coll_z1_df)\ncoll_z1_label &lt;- paste0(\"Z == 1 (\",coll_z1_n,\" obs)\")\ncoll_z1_plot &lt;- plot_freqs(coll_z1_df, coll_z1_label, y_lab=FALSE)\ncoll_full_plot | coll_z0_plot | coll_z1_plot\n\n\n\n\n\n\n\n\n\n\nConditioning on colliders induces correlation where there previously was none ‚ò†Ô∏è\n\n\n\n\n\nCode\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\n\n\n\n\nCode\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",ccoll_z0_slope,\"$\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",ccoll_z1_slope,\"$\")\nccoll_z_texlabel &lt;- TeX(paste0(ccoll_z0_label, \" | \", ccoll_z1_label))\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",ccoll_slope\n    ),\n    subtitle=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶This is why we have to think, rather than just ‚Äúcontrol for everything‚Äù! üò≠"
  },
  {
    "objectID": "w04/slides.html#proxies-for-z",
    "href": "w04/slides.html#proxies-for-z",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Proxies for \\(Z\\)",
    "text": "Proxies for \\(Z\\)\n\n\n\n\n\nCode\nset.seed(5650)\nprox_df &lt;- tibble(\n  X = rbern(n_d),\n  Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n  Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n  A = rbern(n_d, (1-Z)*0.1 + Z*0.9)\n)\n\n\n\n\nCode\n# The full df\nprox_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\nprox_full_plot &lt;- plot_freqs(prox_df, prox_full_label)\n# Conditioning on A == 0\nprox_a0_df &lt;- prox_df |&gt; filter(A == 0)\nprox_a0_n &lt;- nrow(prox_a0_df)\nprox_a0_label &lt;- paste0(\"A == 0 (\",prox_a0_n,\" obs)\")\nprox_a0_plot &lt;- plot_freqs(prox_a0_df, prox_a0_label, y_lab=FALSE)\n# Conditioning on A == 1\nprox_a1_df &lt;- prox_df |&gt; filter(A == 1)\nprox_a1_n &lt;- nrow(prox_a1_df)\nprox_a1_label &lt;- paste0(\"A == 1 (\",prox_a1_n,\" obs)\")\nprox_a1_plot &lt;- plot_freqs(prox_a1_df, prox_a1_label, y_lab=FALSE)\nprox_full_plot | prox_a0_plot | prox_a1_plot\n\n\n\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A\\) gives us some (not all!) information about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\nn_c &lt;- 300\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"$Slope_{A=0} = \",cprox_a0_slope,\"$\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"$Slope_{A=1} = \",cprox_a1_slope,\"$\")\ncprox_a_texlabel &lt;- TeX(paste0(cprox_a0_label, \" | \", cprox_a1_label))\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_text(size=18)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )"
  },
  {
    "objectID": "w04/slides.html#references",
    "href": "w04/slides.html#references",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "References",
    "text": "References\n\n\nBezuidenhout, Dana, Sarah Forthal, Kara Rudolph, and Matthew R Lamb. 2024. ‚ÄúSingle World Intervention Graphs (SWIGs): A Practical Guide.‚Äù American Journal of Epidemiology, September, kwae353. https://doi.org/10.1093/aje/kwae353.\n\n\nGleijeses, Piero. 2013. Visions of Freedom: Havana, Washington, Pretoria, and the Struggle for Southern Africa, 1976-1991. UNC Press Books.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and STAN. CRC Press."
  },
  {
    "objectID": "w05/index.html",
    "href": "w05/index.html",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#pooling-none-full-and-adaptive",
    "href": "w05/index.html#pooling-none-full-and-adaptive",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pooling: None, Full, and Adaptive",
    "text": "Pooling: None, Full, and Adaptive\n\n\n\nFrom Gelman and Hill (2007)\n\n\n\\[\n\\hat{\\alpha}_j^{\\text {multilevel }} = \\frac{\\frac{n_j}{\\sigma_y^2} \\bar{y}_j+\\frac{1}{\\sigma_\\alpha^2} \\bar{y}_{\\text {all }}}{\\frac{n_j}{\\sigma_y^2}+\\frac{1}{\\sigma_\\alpha^2}}\n\\]",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#why-adaptive-none-or-full",
    "href": "w05/index.html#why-adaptive-none-or-full",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Why Adaptive >> None or Full?",
    "text": "Why Adaptive &gt;&gt; None or Full?\n\n\n\n\n\n\\[\n\\alpha_j \\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha), \\text{ for }j = 1, \\ldots, J\n\\]\n\nIn the limit of \\(\\sigma_\\alpha \\rightarrow \\infty\\), the soft constraints do nothing, and there is no pooling;\nAs \\(\\sigma_\\alpha \\rightarrow 0\\), they pull the estimates all the way to zero, yielding the complete-pooling estimate",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/index.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag_closed, Z=\"Z\", lwd=5)\nadj_sets_closed &lt;- adjustmentSets(\n    pipe_dag_closed\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_closed\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/index.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#conditioning-on-a-proxy-for-z",
    "href": "w05/index.html#conditioning-on-a-proxy-for-z",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Conditioning on a Proxy for \\(Z\\)",
    "text": "Conditioning on a Proxy for \\(Z\\)\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A \\Rightarrow\\) some (not all!) info about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cprox_a0_slope,\"&lt;/span&gt;\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cprox_a1_slope,\"&lt;/span&gt;\")\ncprox_a_texlabel &lt;- paste0(cprox_a0_label, \" | \", cprox_a1_label)\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.5*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_markdown(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "href": "w05/index.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening",
    "text": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\ncoll_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(coll_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(coll_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(coll_dag, lwd=5)\nadj_sets_coll &lt;- adjustmentSets(\n    coll_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_coll\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#is-this-just-a-corner-case",
    "href": "w05/index.html#is-this-just-a-corner-case",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Is This Just a Corner Case?",
    "text": "Is This Just a Corner Case?\n\nGrant applications are regularly judged on two criteria: novelty and rigor\nJudges grade proposal separately on the two criteria, grant is awarded to top \\(N\\) applications based on combined scores‚Ä¶\n\n\nCode\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = runif(n_c, 0, 10),\n    rigorous = runif(n_c, 0, 10),\n    awarded = ifelse(newsworthy + rigorous &gt; 10, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = rnorm(n_c, 5, 1),\n    rigorous = rnorm(n_c, 5, 1),\n    awarded = ifelse(newsworthy + rigorous &gt; 12, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#blocking-backdoor-paths",
    "href": "w05/index.html#blocking-backdoor-paths",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Blocking Backdoor Paths",
    "text": "Blocking Backdoor Paths\ndagitty: R interface to dagitty.net\n\n\nCode\nrt_dag &lt;- dagitty(\"dag{\nX [exposure]\nY [outcome]\nU [unobserved]\nX -&gt; Y\nX &lt;- U &lt;- A -&gt; C -&gt; Y\nU -&gt; B &lt;- C\n}\")\ncoordinates(rt_dag) &lt;- list(\n    x=c(U=0, X=0, A=0.5, B=0.5, C=1, Y=1),\n    y=c(X=0.75, Y=0.75, B=0.5, U=0.25, C=0.25, A=0)\n)\ndrawdag(rt_dag, cex=2, lwd=4, radius=6)\n\n\n\n\n\n\n\n\n\n\n\nTwo backdoor paths!\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\leftarrow A \\rightarrow C \\rightarrow Y\\): Open or closed?\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\rightarrow B \\leftarrow C \\rightarrow Y\\): Open or closed?\n\n\n\nCode\nadjustmentSets(rt_dag)\n\n\n{ C }\n{ A }",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#references",
    "href": "w05/index.html#references",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. https://www.dropbox.com/scl/fi/asbumi3g0gqa4xl9va7wp/Andrew-Gelman-Jennifer-Hill-Data-Analysis-Using-Regression-and-Multilevel_Hierarchical-Models.pdf?rlkey=zf8icjhm7rswvxrpm7d10m65o&dl=1.",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/slides.html#pooling-none-full-and-adaptive",
    "href": "w05/slides.html#pooling-none-full-and-adaptive",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pooling: None, Full, and Adaptive",
    "text": "Pooling: None, Full, and Adaptive\n\nFrom Gelman and Hill (2007)\\[\n\\hat{\\alpha}_j^{\\text {multilevel }} = \\frac{\\frac{n_j}{\\sigma_y^2} \\bar{y}_j+\\frac{1}{\\sigma_\\alpha^2} \\bar{y}_{\\text {all }}}{\\frac{n_j}{\\sigma_y^2}+\\frac{1}{\\sigma_\\alpha^2}}\n\\]"
  },
  {
    "objectID": "w05/slides.html#why-adaptive-none-or-full",
    "href": "w05/slides.html#why-adaptive-none-or-full",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Why Adaptive >> None or Full?",
    "text": "Why Adaptive &gt;&gt; None or Full?\n\n\\[\n\\alpha_j \\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha), \\text{ for }j = 1, \\ldots, J\n\\]\n\nIn the limit of \\(\\sigma_\\alpha \\rightarrow \\infty\\), the soft constraints do nothing, and there is no pooling;\nAs \\(\\sigma_\\alpha \\rightarrow 0\\), they pull the estimates all the way to zero, yielding the complete-pooling estimate"
  },
  {
    "objectID": "w05/slides.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/slides.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag_closed, Z=\"Z\", lwd=5)\nadj_sets_closed &lt;- adjustmentSets(\n    pipe_dag_closed\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_closed\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}"
  },
  {
    "objectID": "w05/slides.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/slides.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w05/slides.html#conditioning-on-a-proxy-for-z",
    "href": "w05/slides.html#conditioning-on-a-proxy-for-z",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Conditioning on a Proxy for \\(Z\\)",
    "text": "Conditioning on a Proxy for \\(Z\\)\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A \\Rightarrow\\) some (not all!) info about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cprox_a0_slope,\"&lt;/span&gt;\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cprox_a1_slope,\"&lt;/span&gt;\")\ncprox_a_texlabel &lt;- paste0(cprox_a0_label, \" | \", cprox_a1_label)\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.5*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_markdown(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )"
  },
  {
    "objectID": "w05/slides.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "href": "w05/slides.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening",
    "text": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\ncoll_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(coll_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(coll_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(coll_dag, lwd=5)\nadj_sets_coll &lt;- adjustmentSets(\n    coll_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_coll\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w05/slides.html#is-this-just-a-corner-case",
    "href": "w05/slides.html#is-this-just-a-corner-case",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Is This Just a Corner Case?",
    "text": "Is This Just a Corner Case?\n\nGrant applications are regularly judged on two criteria: novelty and rigor\nJudges grade proposal separately on the two criteria, grant is awarded to top \\(N\\) applications based on combined scores‚Ä¶\n\n\nCode\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = runif(n_c, 0, 10),\n    rigorous = runif(n_c, 0, 10),\n    awarded = ifelse(newsworthy + rigorous &gt; 10, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = rnorm(n_c, 5, 1),\n    rigorous = rnorm(n_c, 5, 1),\n    awarded = ifelse(newsworthy + rigorous &gt; 12, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )"
  },
  {
    "objectID": "w05/slides.html#blocking-backdoor-paths",
    "href": "w05/slides.html#blocking-backdoor-paths",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Blocking Backdoor Paths",
    "text": "Blocking Backdoor Paths\ndagitty: R interface to dagitty.net\n\n\n\nCode\nrt_dag &lt;- dagitty(\"dag{\nX [exposure]\nY [outcome]\nU [unobserved]\nX -&gt; Y\nX &lt;- U &lt;- A -&gt; C -&gt; Y\nU -&gt; B &lt;- C\n}\")\ncoordinates(rt_dag) &lt;- list(\n    x=c(U=0, X=0, A=0.5, B=0.5, C=1, Y=1),\n    y=c(X=0.75, Y=0.75, B=0.5, U=0.25, C=0.25, A=0)\n)\ndrawdag(rt_dag, cex=2, lwd=4, radius=6)\n\n\n\n\n\n\n\n\n\n\n\n\nTwo backdoor paths!\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\leftarrow A \\rightarrow C \\rightarrow Y\\): Open or closed?\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\rightarrow B \\leftarrow C \\rightarrow Y\\): Open or closed?\n\n\n\nCode\nadjustmentSets(rt_dag)\n\n\n{ C }\n{ A }"
  },
  {
    "objectID": "w05/slides.html#references",
    "href": "w05/slides.html#references",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. https://www.dropbox.com/scl/fi/asbumi3g0gqa4xl9va7wp/Andrew-Gelman-Jennifer-Hill-Data-Analysis-Using-Regression-and-Multilevel_Hierarchical-Models.pdf?rlkey=zf8icjhm7rswvxrpm7d10m65o&dl=1."
  },
  {
    "objectID": "writeups/informative-priors/FlatPriors_MiniLab.html",
    "href": "writeups/informative-priors/FlatPriors_MiniLab.html",
    "title": "Skeptical, Weakly-Informative, and Strongly-Informative Priors",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nrng = np.random.default_rng(seed=5650)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncb_palette = ['#e69f00','#56b4e9','#009e73']\nsns.set_palette(cb_palette)\nimport patchworklib as pw;\nimport pymc as pm\nimport arviz as az"
  },
  {
    "objectID": "writeups/informative-priors/FlatPriors_MiniLab.html#part-1-informative-beta-prior-model",
    "href": "writeups/informative-priors/FlatPriors_MiniLab.html#part-1-informative-beta-prior-model",
    "title": "Skeptical, Weakly-Informative, and Strongly-Informative Priors",
    "section": "[Part 1] Informative (Beta) Prior Model",
    "text": "[Part 1] Informative (Beta) Prior Model\n\nwith pm.Model() as inf_model:\n    result_obs = pm.Data('result_obs', one_flip_result)\n    p_heads = pm.Beta(\"p_heads\", alpha=2, beta=2)\n    result = pm.Bernoulli(\"result\", p=p_heads, observed=result_obs)\npm.model_to_graphviz(inf_model)\n\n\n\n\n\n\n\n\n\ndef draw_prior_sample(model, return_idata=False):\n    with model:\n        prior_idata = pm.sample_prior_predictive(draws=5000, random_seed=5650)\n    prior_df = prior_idata.prior.to_dataframe().reset_index().drop(columns='chain')\n    if return_idata:\n        return prior_idata, prior_df\n    return prior_df\ninf_n0_df = draw_prior_sample(inf_model)\n\nSampling: [p_heads, result]\n\n\n\ndef gen_dist_plot(dist_df, plot_title):\n    ax = pw.Brick(figsize=(3.5, 2.25))\n    sns.histplot(\n        x=\"p_heads\", data=dist_df, ax=ax,\n        bins=25\n    );\n    ax.set_title(plot_title)\n    return ax\ninf_n0_plot = gen_dist_plot(inf_n0_df, \"Beta(1.5, 1.5) Prior on p\")\ninf_n0_plot.savefig()\n\n\n\n\n\n\n\n\n\ndef draw_post_sample(model, num_draws=5000):\n    with model:\n        post_idata = pm.sample(draws=num_draws, random_state=5650)\n    post_df = post_idata.posterior.to_dataframe().reset_index().drop(columns='chain')\n    return post_df\ninf_n1_df = draw_post_sample(inf_model)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\ninf_n1_plot = gen_dist_plot(inf_n1_df, \"Posterior (N = 1)\")\ninf_n1_plot.savefig()\n\n\n\n\n\n\n\n\n\nObserve \\(N = 2\\) Flips\n\nwith inf_model:\n    pm.set_data({'result_obs': two_flips_result})\ninf_n2_df = draw_post_sample(inf_model)    \n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\ninf_n2_plot = gen_dist_plot(inf_n2_df, \"Posterior (N = 2)\")\ninf_n2_plot.savefig()\n\n\n\n\n\n\n\n\n\ndef plot_n_dists(n_df):\n    ax = pw.Brick(figsize=(5, 3.5));\n    sns.kdeplot(\n        x=\"p_heads\", hue=\"n\", fill=True, ax=ax, data=n_df,\n        common_norm=False\n    );\n    display(ax.savefig())\ninf_n0_df['n'] = 0\ninf_n1_df['n'] = 1\ninf_n2_df['n'] = 2\ninf_3_df = pd.concat([inf_n0_df, inf_n1_df, inf_n2_df])\nplot_n_dists(inf_3_df)\n\n\n\n\n\n\n\n\n\n\nObserve \\(N = 5\\) Flips\n\nwith inf_model:\n    pm.set_data({'result_obs': five_flips_result})\ninf_n5_df = draw_post_sample(inf_model)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\ninf_n5_plot = gen_dist_plot(inf_n5_df, \"Posterior (N = 5)\")\ninf_n5_plot.savefig()\n\n\n\n\n\n\n\n\n\ninf_n5_df['n'] = 5\ninf_4_df = pd.concat([inf_3_df, inf_n5_df])\nplot_n_dists(inf_4_df)\n\n\n\n\n\n\n\n\n\n\nObserve \\(N = 10\\) Flips\n\nwith inf_model:\n    pm.set_data({'result_obs': ten_flips_result})\ninf_n10_df = draw_post_sample(inf_model)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\ninf_n10_df['n'] = 10\ninf_5_df = pd.concat([inf_4_df, inf_n10_df])\nplot_n_dists(inf_5_df)\n\n\n\n\n\n\n\n\n\n\nObserve \\(N = 25\\) Flips"
  },
  {
    "objectID": "writeups/informative-priors/FlatPriors_MiniLab.html#part-2-flat-uniform-prior",
    "href": "writeups/informative-priors/FlatPriors_MiniLab.html#part-2-flat-uniform-prior",
    "title": "Skeptical, Weakly-Informative, and Strongly-Informative Priors",
    "section": "[Part 2] Flat (Uniform) Prior",
    "text": "[Part 2] Flat (Uniform) Prior\n\nwith pm.Model() as unif_model:\n    result_obs = pm.Data('result_obs', one_flip_result)\n    p_heads = pm.Beta(\"p_heads\", 1, 1)\n    result = pm.Bernoulli(\"result\", p=p_heads, observed=result_obs)\npm.model_to_graphviz(unif_model)\n\n\n\n\n\n\n\n\n\nunif_n0_df = draw_prior_sample(unif_model)\n\nSampling: [p_heads, result]\n\n\n\n\n\n\n\n\n\ndraw\np_heads\n\n\n\n\n0\n0\n0.970164\n\n\n1\n1\n0.085831\n\n\n2\n2\n0.101310\n\n\n3\n3\n0.315523\n\n\n4\n4\n0.285908\n\n\n...\n...\n...\n\n\n4995\n4995\n0.254631\n\n\n4996\n4996\n0.166792\n\n\n4997\n4997\n0.772046\n\n\n4998\n4998\n0.296384\n\n\n4999\n4999\n0.331467\n\n\n\n\n5000 rows √ó 2 columns\n\n\n\n\nunif_n0_plot = gen_dist_plot(unif_n0_df, \"Uniform Prior\")\nunif_n0_plot.savefig()\n\n\n\n\n\n\n\n\n\nPosterior After \\(N = 1\\) Observed Flips\n\nunif_n1_df = draw_post_sample(unif_model)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\nunif_n1_plot = gen_dist_plot(unif_n1_df, f\"Posterior After Observing X = {one_flip_result}\")\nunif_n1_plot.savefig()\n\n\n\n\n\n\n\n\n\n\nPosterior After \\(N = 2\\) Flips\n\nwith unif_model:\n    pm.set_data({'result_obs': two_flips_result})\nunif_n2_df = draw_post_sample(unif_model)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\nunif_n2_plot = gen_dist_plot(unif_n2_df, f\"Posterior After Observing {two_flips_result}\")\nunif_n2_plot.savefig()\n\n\n\n\n\n\n\n\n\nunif_n0_df['n'] = 0\nunif_n1_df['n'] = 1\nunif_n2_df['n'] = 2\nunif_3_df = pd.concat([unif_n0_df, unif_n1_df, unif_n2_df])\nplot_n_dists(unif_3_df)\n\n\n\n\n\n\n\n\n\nwith unif_model:\n    pm.set_data({'result_obs': five_flips_result})\nunif_n5_df = draw_post_sample(unif_model)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\nunif_n5_df['n'] = 5\nunif_4_df = pd.concat([unif_3_df, unif_n5_df])\nplot_n_dists(unif_4_df)\n\n\n\n\n\n\n\n\n\n\nObserve \\(N = 10\\) Flips\n\nwith unif_model:\n    pm.set_data({'result_obs': ten_flips_result})\nunif_n10_df = draw_post_sample(unif_model)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\nunif_n10_df['n'] = 10\nunif_5_df = pd.concat([unif_4_df, unif_n10_df])\nplot_n_dists(unif_5_df)"
  },
  {
    "objectID": "writeups/informative-priors/FlatPriors_MiniLab.html#part-3-skeptical-jeffreys-prior",
    "href": "writeups/informative-priors/FlatPriors_MiniLab.html#part-3-skeptical-jeffreys-prior",
    "title": "Skeptical, Weakly-Informative, and Strongly-Informative Priors",
    "section": "[Part 3] Skeptical (Jeffreys) Prior",
    "text": "[Part 3] Skeptical (Jeffreys) Prior\nThis prior is itself derived from an approach to Bayesian statistics called ‚ÄúObjective Bayes‚Äù, within which the Jeffreys Prior for the Bernoulli parameter \\(p\\) has a special status.\nFor our purposes, however, we can just view it as a ‚Äúskeptical‚Äù prior: it encodes an assumption that the coin is very biased, i.e., that before seeing any actual coin flips we think that \\(p = 0\\) and \\(p = 1\\) are more likely than any of the values in between (any of the values \\(p \\in (0, 1)\\)). This means that‚Äîrelative to the Beta and Uniform cases‚Äîsomeone with these priors would require a very ‚Äúeven‚Äù mixture of heads and tails to ‚Äúcancel out‚Äù their pre-existing belief that the coin is biased!\n\nwith pm.Model() as flat_model:\n    result_obs = pm.Data('result_obs', one_flip_result)\n    p_heads = pm.Beta(\"p_heads\", 0.5, 0.5)\n    result = pm.Bernoulli(\"result\", p=p_heads, observed=result_obs)\npm.model_to_graphviz(flat_model)\n\n\n\n\n\n\n\n\n\nflat_n0_df = draw_prior_sample(flat_model)\n\nSampling: [p_heads, result]\n\n\n\nflat_n0_plot = gen_dist_plot(flat_n0_df, \"Flat(-ish) Prior\")\nflat_n0_plot.savefig()\n\n\n\n\n\n\n\n\n\nObserve \\(N = 1\\) Flip\n\nflat_n1_df = draw_post_sample(flat_model)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\nflat_n1_plot = gen_dist_plot(flat_n1_df, f\"Posterior After Observing {one_flip_result}\")\nflat_n1_plot.savefig()\n\n\n\n\n\n\n\n\n\n\nObserve \\(N = 2\\) Flips\n\nwith flat_model:\n    pm.set_data({'result_obs': two_flips_result})\npm.model_to_graphviz(flat_model)\n\n\n\n\n\n\n\n\n\nflat_n2_df = draw_post_sample(flat_model)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\nflat_n2_plot = gen_dist_plot(flat_n2_df, \"Posterior\")\nflat_n2_plot.savefig()\n\n\n\n\n\n\n\n\n\nflat_n0_df['n'] = 0\nflat_n1_df['n'] = 1\nflat_n2_df['n'] = 2\nflat_3_df = pd.concat([flat_n0_df, flat_n1_df, flat_n2_df])\nplot_n_dists(flat_3_df)\n\n\n\n\n\n\n\n\n\nwith flat_model:\n    pm.set_data({'result_obs': five_flips_result})\nflat_n5_df = draw_post_sample(flat_model)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\nflat_n5_df['n'] = 5\nflat_4_df = pd.concat([flat_3_df, flat_n5_df])\nplot_n_dists(flat_4_df)\n\n\n\n\n\n\n\n\n\n\nObserve \\(N = 10\\) Flips\n\nwith flat_model:\n    pm.set_data({'result_obs': ten_flips_result})\nflat_n10_df = draw_post_sample(flat_model)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [p_heads]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 5_000 draw iterations (4_000 + 20_000 draws total) took 3 seconds.\n\n\n\nflat_n10_df['n'] = 10\nflat_5_df = pd.concat([flat_4_df, flat_n10_df])\nplot_n_dists(flat_5_df)"
  },
  {
    "objectID": "writeups/informative-priors/FlatPriors_MiniLab.html#part-4-which-one-learned-most-efficiently",
    "href": "writeups/informative-priors/FlatPriors_MiniLab.html#part-4-which-one-learned-most-efficiently",
    "title": "Skeptical, Weakly-Informative, and Strongly-Informative Priors",
    "section": "[Part 4] Which One Learned Most Efficiently?",
    "text": "[Part 4] Which One Learned Most Efficiently?\n\nAfter \\(N = 2\\)?\n\ndef plot_dist_comparison(combined_df):\n    ax = pw.Brick(figsize=(5, 3.5));\n    sns.kdeplot(\n        x=\"p_heads\", hue=\"prior\", fill=True, ax=ax, data=combined_df,\n        common_norm=False\n    );\n    display(ax.savefig())\n\n\ninf_n2_df['prior'] = 'Beta(2, 2)'\nunif_n2_df['prior'] = 'Beta(1, 1)'\nflat_n2_df['prior'] = 'Beta(0.5, 0.5)'\nall_n2_df = pd.concat([inf_n2_df, unif_n2_df, flat_n2_df])\nplot_dist_comparison(all_n2_df)\n\n\n\n\n\n\n\n\n\n\nAfter \\(N = 5\\)?\n\ninf_n5_df['prior'] = 'Beta(2, 2)'\nunif_n5_df['prior'] = 'Beta(1, 1)'\nflat_n5_df['prior'] = 'Beta(0.5, 0.5)'\nall_n5_df = pd.concat([inf_n5_df, unif_n5_df, flat_n5_df])\nplot_dist_comparison(all_n5_df)\n\n\n\n\n\n\n\n\n\n\nAfter \\(N = 10\\)?\n\ninf_n10_df['prior'] = 'Beta(2, 2)'\nunif_n10_df['prior'] = 'Beta(1, 1)'\nflat_n10_df['prior'] = 'Beta(0.5, 0.5)'\nall_n10_df = pd.concat([inf_n10_df, unif_n10_df, flat_n10_df])\nplot_dist_comparison(all_n10_df)\n\n\n\n\n\n\n\n\n\nwith inf_model:\n    print(pm.find_MAP())\n\n\n\n\n\n\n\n{'p_heads_logodds__': array(-0.33647223), 'p_heads': array(0.41666667)}\n\n\n\nwith unif_model:\n    print(pm.find_MAP())\n\n\n\n\n\n\n\n{'p_heads_logodds__': array(-0.4054651), 'p_heads': array(0.4)}\n\n\n\nwith flat_model:\n    print(pm.find_MAP())\n\n\n\n\n\n\n\n{'p_heads_logodds__': array(-0.47956846), 'p_heads': array(0.38235403)}"
  },
  {
    "objectID": "writeups/hw2-guide/HW2_Guide.html",
    "href": "writeups/hw2-guide/HW2_Guide.html",
    "title": "DSAN 5650 HW2 Guide",
    "section": "",
    "text": "%run jupyter_fixes.ipynb\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport patchworklib as pw\nimport seaborn as sns\n\nimport pymc as pm\nimport arviz as az\n\n&lt;Figure size 100x100 with 0 Axes&gt;"
  },
  {
    "objectID": "writeups/hw2-guide/HW2_Guide.html#part-4-a-non-regression-example",
    "href": "writeups/hw2-guide/HW2_Guide.html#part-4-a-non-regression-example",
    "title": "DSAN 5650 HW2 Guide",
    "section": "[Part 4] A Non-Regression Example",
    "text": "[Part 4] A Non-Regression Example\n\nWhy All the Different Models With Different Priors in the Beginning?\nThis part was intended to basically combine two key Bayesian Workflow ‚Äúlessons‚Äù into one problem, but was maybe over-complicated because of this üò®. The two lessons are:\n\nThe fact that the regressions you implemented in HW2A and HW2B are far from the only models that you can implement using PyMC, and then\nHow you can customize your model, using prior predictive checks, to develop a parameterization that better captures the range of possible data, before the second step of then going and estimating the specific values of these parameters.\n\nThe importance of these prior predictive checks, that Part 4 is trying to drive home, is also captured in the following excerpts from Gabry et al.¬†(2019):\n\nIf we specify proper priors for all parameters in the model, a Bayesian model yields a joint prior distribution on parameters and data, and hence a prior marginal distribution [a prior predictive distribution] for the data, i.e.¬†Bayesian models with proper priors are generative models.\n\nwhich means that\n\nwe can visualize simulations from the prior marginal distribution of the data to assess the consistency of the chosen priors with domain knowledge.\n\nThe implication here is that prior predictive model modeling is in fact crucial to the scientific method! However, rather than subjecting you to a full-on deep-dive into how different priors can represent different ‚Äústates‚Äù of knowledge about scientific hypotheses, for now you can just focus on the goal of developing weakly informative priors, defined as follows (in the same paper):\n\nA prior leads to a weakly informative joint prior data-generating process if draws from the prior data-generating distribution \\(p(y)\\) [the prior predictive distribution] could represent any data set that could plausibly be observed.\n\n\n\nA Simpler Prior-Focused Workflow\nIt may have been a bad idea to jump directly to count data here, since that‚Äôs usually the third kind of model that you learn in e.g.¬†a Statistical Learning course (like DSAN 5300)‚Ä¶ So, here let‚Äôs step back to the second kind of model that you usually learn: logistic regression for binary outcome data (the first kind of model you usually learn is basic linear regression for continuous outcome data, which is covered in HW2A and HW2B).\nThe goal of weakly informative priors would be, as mentioned above, to cover different plausible datasets that could be observed out in the world for the phenomenon you‚Äôre trying to model.\nSo, here as a simple case let‚Äôs try to develop a weakly informative prior for NBA team outcomes:\n\nThe worst team in NBA history, by win percentage, was the 2011-2012 Charlotte Bobcats[1], who won 7 games out of a total of 66 that season (there are usually 82, but there was a labor dispute that year which cut the season short), for a win probability of 0.106. Extrapolating to a standard 82-game season, this means they were on pace to win 9 games (rounded to the nearest integer)\nThe best team in NBA history, by win percentage, was the 2015-2016 Golden State Warriors, who won 73 games out of a total of 82, for a win probability of 0.890\nThe Orlando Magic, this past season (2024-2025), achieved the most-average possible record, winning 41 games out of a total of 82, for a win probability of exactly 0.500.\n\nSo, if we were trying to model the different win probabilities for a given team across a given season, a weakly informative prior should at least be able to capture this range of possibilities, from 9 to 73 games[2].\n\n\n\nFor those who don‚Äôt know this, Jeff for some reason really likes sports and is able to remember a bunch of random statistics from a bunch of random players/teams across different US professional sports. So‚Ä¶ these are from memory, sorry if they‚Äôre wrong.\nDon‚Äôt worry ‚Äì as we‚Äôll see below, part of the whole modeling process is constructing a model that can satisfy different people‚Äôs opinions about what is ‚Äúplausible‚Äù. So, we‚Äôll get to the fact that it‚Äôs possible for teams to be worse than the 2011-2012 Bobcats or better than the 2015-2016 Warriors!\n\n\n\nindices = {'game_num': list(range(1, 82+1))}\nwith pm.Model(coords=indices) as nba_model:\n    p_win = pm.Uniform(\"p_win\", 0, 1)\n    win = pm.Bernoulli(\"win\", p=p_win, dims=\"game_num\")\n    num_wins = pm.Deterministic(\"num_wins\", pm.math.sum(win))\npm.model_to_graphviz(nba_model)\n\n\n\n\n\n\n\n\n\nwith nba_model:\n    nba_prior_idata = pm.sample_prior_predictive(random_seed=5650)\n\nSampling: [p_win, win]\n\n\n\naz.summary(nba_prior_idata)\n\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nnum_wins\n42.246\n24.658\n4.000\n81.000\n1.105\n0.495\n470.0\n436.0\nNaN\n\n\nwin[1]\n0.480\n0.500\n0.000\n1.000\n0.022\n0.001\n507.0\n500.0\nNaN\n\n\nwin[2]\n0.534\n0.499\n0.000\n1.000\n0.022\n0.002\n503.0\n500.0\nNaN\n\n\nwin[3]\n0.518\n0.500\n0.000\n1.000\n0.021\n0.001\n545.0\n500.0\nNaN\n\n\nwin[4]\n0.524\n0.500\n0.000\n1.000\n0.023\n0.001\n471.0\n471.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nwin[79]\n0.528\n0.500\n0.000\n1.000\n0.023\n0.001\n474.0\n474.0\nNaN\n\n\nwin[80]\n0.530\n0.500\n0.000\n1.000\n0.022\n0.001\n500.0\n500.0\nNaN\n\n\nwin[81]\n0.498\n0.500\n0.000\n1.000\n0.022\n0.000\n537.0\n500.0\nNaN\n\n\nwin[82]\n0.564\n0.496\n0.000\n1.000\n0.023\n0.003\n461.0\n461.0\nNaN\n\n\np_win\n0.516\n0.294\n0.041\n0.969\n0.013\n0.006\n472.0\n444.0\nNaN\n\n\n\n\n84 rows √ó 9 columns\n\n\n\n\nnba_prior_df = nba_prior_idata.prior.to_dataframe().reset_index().drop(columns=\"chain\")\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"num_wins\", data=nba_prior_df, ax=ax,\n);\nax.axvline(x = 9, ls='dashed', color='black', alpha=0.9);\nax.axvline(x = 73, ls='dashed', color='black', alpha=0.9);\nax.savefig()\n\n\n\n\n\n\n\n\nAnd indeed, from this plot we can see that we have a weakly informative prior with respect to our above criteria: the prior predictive distribution assigns a positive probability mass to all values between 9 and 73.\nIn fact, now we can address the footnote above: here, even someone who believes that any number of wins between 0 and 82 inclusive is plausible would find this satisfactory as a weakly informative prior, since it actually assigns a positive probability mass to all values between 0 and 82.\nThis need not be the case, however ‚Äì meaning, there are priors which the ‚Äú9 to 73 is plausible‚Äù person would call ‚Äúweakly informative‚Äù while the ‚Äú0 to 82 is plausible‚Äù person would not.\nHere‚Äôs an example, using the Truncated operator that PyMC provides, which allows us to do the following:\n\nDefine a Binomial distribution to represent a number of games between 0 and 82, but then\nTruncate the pmf of the ‚Äútrue‚Äù 0-to-82 distribution down to a 9-to-73 distribution, by ‚Äúchopping off‚Äù the values outside of \\(\\{9, 10, \\ldots, 73\\}\\) and re-normalizing the probability masses to sum to 1.\n\nIt‚Äôs a bit silly, as you‚Äôll see from the visualization, and we‚Äôre modeling the season as a whole rather than individual games like in the above model, but yeah it‚Äôs here to show you an example prior which can violate the ‚Äúweakly informative‚Äù criteria!\n\nwith pm.Model() as trunc_model:\n    num_wins_raw = pm.TruncatedNormal(\"num_wins_raw\", mu=41, sigma=20, lower=8.51, upper=73.49)\n    num_wins = pm.Deterministic(\"num_wins\", pm.math.round(num_wins_raw))\npm.model_to_graphviz(trunc_model)\n\n\n\n\n\n\n\n\n\nwith trunc_model:\n    trunc_prior_idata = pm.sample_prior_predictive(draws=5000, random_seed=5650)\n\nSampling: [num_wins_raw]\n\n\n\naz.summary(trunc_prior_idata)\n\narviz - WARNING - Shape validation failed: input_shape: (1, 5000), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nnum_wins_raw\n41.135\n15.854\n14.794\n71.13\n0.229\n0.12\n4808.0\n4653.0\nNaN\n\n\nnum_wins\n41.134\n15.852\n12.000\n68.00\n0.229\n0.12\n4823.0\n4600.0\nNaN\n\n\n\n\n\n\n\n\ntrunc_prior_df = trunc_prior_idata.prior.to_dataframe().reset_index().drop(columns=\"chain\")\nax = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"num_wins\", data=trunc_prior_df, ax=ax,\n);\nax.axvline(x = 9, ls='dashed', color='black', alpha=0.9);\nax.axvline(x = 73, ls='dashed', color='black', alpha=0.9);\nax.savefig()\n\n\n\n\n\n\n\n\nThe visualization shows, if you look really closely, that we indeed have a weakly-informative prior relative to the first (‚ÄúOnly 9 to 73 is plausible‚Äù) person‚Äôs criteria, but it‚Äôs not weakly-informative relative to the second (‚Äú0 to 82 is plausible‚Äù) person‚Äôs. We could also directly verify this by converting the data from the 10,000 draws into a proper probability distribution and checking that it in fact assigns positive probability mass to each value from 9 to 73:\n\ntrunc_prior_df['num_wins'] = trunc_prior_df['num_wins'].astype(int)\nwin_counts_df = trunc_prior_df['num_wins'].value_counts().to_frame().reset_index().sort_values(by='num_wins')\nwin_counts_df['prob'] = win_counts_df['count'] / win_counts_df['count'].sum()\nwin_counts_df\n\n\n\n\n\n\n\n\nnum_wins\ncount\nprob\n\n\n\n\n64\n9\n24\n0.0048\n\n\n60\n10\n36\n0.0072\n\n\n62\n11\n31\n0.0062\n\n\n55\n12\n44\n0.0088\n\n\n59\n13\n38\n0.0076\n\n\n...\n...\n...\n...\n\n\n51\n69\n51\n0.0102\n\n\n57\n70\n42\n0.0084\n\n\n56\n71\n43\n0.0086\n\n\n61\n72\n33\n0.0066\n\n\n63\n73\n30\n0.0060\n\n\n\n\n65 rows √ó 3 columns\n\n\n\nAnd we see, indeed, a low-but-positive probability mass on each number of wins from 9 to 73!"
  },
  {
    "objectID": "writeups/hw2-guide/HW2_Guide.html#part-5-firing-squad",
    "href": "writeups/hw2-guide/HW2_Guide.html#part-5-firing-squad",
    "title": "DSAN 5650 HW2 Guide",
    "section": "[Part 5] Firing Squad",
    "text": "[Part 5] Firing Squad\nIt‚Äôs a bit hard to come up with starter code for this question but, let‚Äôs try! Note how the Firing Squad example boils down to just a bunch of dyadic (two-node) relationships like:\n\nIf the Court Orders come in, then the Captain announces the order\nIf the Captain announces the order, Soldier \\(A\\) shoots\nIf the Captain announces the order, Soldier \\(B\\) shoots\nIf Soldier \\(A\\) shoots, the Prisoner dies\nIf Soldier \\(B\\) shoots, the Prisoner dies\n\nSo, maybe the starter code can just be one ‚Äúlink‚Äù in this chain of causality. Let‚Äôs model a dog named Selena who has been trained to dance (\\(D\\)) when the command ‚Äú¬°Baila conmigo!‚Äù (\\(BC\\)) is spoken. We‚Äôll model a single hangout session with the dog as follows:\n\nIn a given hangout session, the dog trainer issues the ‚Äú¬°Baila conmigo!‚Äù (\\(BC\\)) command with probability \\(0.8\\)\nIf the command is issued, Selena dances (\\(D\\)) with 100% certainty. Otherwise, she doesn‚Äôt dance.\n\n\nWorking Through a Solution\nLet‚Äôs start by interpreting the facts in the problem as explicit probabilities, to make sure we‚Äôre on the same page:\n\nIn a given hangout session, the dog trainer issues the ‚Äú¬°Baila conmigo!‚Äù (\\(BC\\)) command with probability \\(0.8\\): \\(\\Pr(BC) = 0.8\\)\nIf the command is issued, Selena dances (\\(D\\)) with 100% certainty: \\(\\Pr(D \\mid BC) = 1.0\\)\n\nLet‚Äôs say we start by just trying to model the command \\(BC\\) as happening with probability \\(0.8\\), and the dance \\(D\\) happening with probability \\(1\\), without linking the two together. We‚Äôd get something like the following:\n\nwith pm.Model() as bad_dance_model:\n    BC = pm.Bernoulli(\"BC\", p=0.8)\n    D = pm.Bernoulli(\"D\", p=1.0)\npm.model_to_graphviz(bad_dance_model)\n\n\n\n\n\n\n\n\nAnd by displaying the PGM like this, we can see that something has gone wrong with our modeling, since the command \\(BC\\) is supposed to have an effect on dancing \\(D\\): there‚Äôs supposed to be an arrow from \\(BC\\) to \\(D\\)!\nSo, this points us towards the fact that the parameter p, the probability of the pm.Bernoulli() Random Variable D, should depend on BC. In this case, we can use pm.math.switch() as basically an if statement which determines the final value of p:\n\nwith pm.Model() as dance_model:\n    BC = pm.Bernoulli(\"BC\", p=0.8)\n    D = pm.Bernoulli(\"D\", p=pm.math.switch(BC, 1.0, 0.0))\npm.model_to_graphviz(dance_model)\n\n\n\n\n\n\n\n\nAnd now the PGM looks as expected.\n\n\nProbabilities Without \\(\\textsf{do}()\\)\nOff rip, let‚Äôs ‚Äúextract‚Äù some general probabilistic facts about this stochastic system we‚Äôve now set up, before we apply \\(\\textsf{do}()\\) or observe data or anything like that.\nTo achieve this, we use the first of three different ways you can derive information from a pm.Model() object: PyMC‚Äôs sample_prior_predictive() function[1]. The way I think of this function is that it tells us what we can say about the different variables without having any additional information besides the model itself: any information, that is, besides the probability distributions of the RVs and how they‚Äôre linked together.\nSampling from the prior distribution looks as follows:\n\n\n\nThe names of the functions in PyMC are extremely confusing imo (though they do update them all the time given user feedback!): the two types of posterior distributions have separate sampling functions, sample() for the posterior and sample_posterior_predictive() for the posterior predictive distribution. However, for reasons I don‚Äôt entirely understand, we use sample_prior_predictive() to obtain both the prior and prior predictive distributions.\n\n\n\nwith dance_model:\n    dance_prior_idata = pm.sample_prior_predictive(random_seed=5650)\n\nSampling: [BC, D]\n\n\nPrinting out the information in dance_prior_idata on its own isn‚Äôt that helpful, unless you already know how the xarray library works (which is good to know!), but there are two other ways to display the results that are a bit more helpful.\nFirst, there is a library called arviz ‚Äì which actually used to be part of PyMC until it was split off as an independent Bayesian inference visualization library ‚Äì that you can use to display summary statistics of the sampling:\n\naz.summary(dance_prior_idata.prior)\n\narviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nD\n0.762\n0.426\n0.0\n1.0\n0.021\n0.013\n412.0\n412.0\nNaN\n\n\nBC\n0.762\n0.426\n0.0\n1.0\n0.021\n0.013\n412.0\n412.0\nNaN\n\n\n\n\n\n\n\nSecond, to use the full results of the sampling, you can just convert the idata object into a Pandas DataFrame by using the .to_dataframe() function:\n\ndance_prior_df = dance_prior_idata.prior.to_dataframe()\ndance_prior_df\n\n\n\n\n\n\n\n\n\nD\nBC\n\n\nchain\ndraw\n\n\n\n\n\n\n0\n0\n0\n0\n\n\n1\n1\n1\n\n\n2\n1\n1\n\n\n3\n0\n0\n\n\n4\n0\n0\n\n\n...\n...\n...\n\n\n495\n0\n0\n\n\n496\n1\n1\n\n\n497\n1\n1\n\n\n498\n1\n1\n\n\n499\n1\n1\n\n\n\n\n500 rows √ó 2 columns\n\n\n\nIf we use this on its own, you‚Äôll notice that it produces a fancy DataFrame with a dual index: first indexed by chain and then by draw. This can be helpful in general, but to me it makes it complicated (since for example chain is useless here while draw can‚Äôt be treated like a normal column since it‚Äôs an index), so I like to use .reset_index() to transform these from indices into regular columns, and then I usually drop chain (this only becomes useful when you start doing fancy Markov Chain Monte Carlo estimation where you use multiple chains so that you can basically throw away non-convergent estimations):\n\ndance_prior_df = dance_prior_idata.prior.to_dataframe().reset_index().drop(columns=\"chain\")\ndance_prior_df\n\n\n\n\n\n\n\n\ndraw\nD\nBC\n\n\n\n\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n\n\n2\n2\n1\n1\n\n\n3\n3\n0\n0\n\n\n4\n4\n0\n0\n\n\n...\n...\n...\n...\n\n\n495\n495\n0\n0\n\n\n496\n496\n1\n1\n\n\n497\n497\n1\n1\n\n\n498\n498\n1\n1\n\n\n499\n499\n1\n1\n\n\n\n\n500 rows √ó 3 columns\n\n\n\nAnd now that we have the samples in this form, we can plot them using e.g.¬†seaborn like any other data in a Pandas DataFrame! As a ‚Äúwrapper‚Äù around seaborn that makes it easier to arrange multiple plots in a grid, I use a library called patchworklib, which is what the pw.Brick() and the ax.savefig() lines are for here. For example, I can create plots of the distributions for both \\(BC\\) and \\(D\\) here and then place them side-by-side using |:\n\nax_bc = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"BC\", data=dance_prior_df, ax=ax_bc, discrete=True, shrink=0.2\n);\nax_d = pw.Brick(figsize=(3.5, 2.25));\nsns.histplot(\n    x=\"D\", data=dance_prior_df, ax=ax_d, discrete=True, shrink=0.2\n);\nax_prior = ax_bc | ax_d;\nax_prior.savefig()\n\n\n\n\n\n\n\n\n\n\nApplying the \\(\\textsf{do}()\\) Operator\nThe above plots represent properties of the stochastic system we‚Äôve defined, at the first ‚Äì associational ‚Äì level of the Ladder of Causal Inference. But, as we‚Äôve discussed ad nauseam in class, to learn about the second ‚Äì causal ‚Äì level of this ladder, we‚Äôd like to see what happens when we intervene to force our model to represent the world where the command \\(BC\\) is issued: how does this affect the ‚Äúdownstream‚Äù dance variable \\(D\\)?\nBy intervening to force \\(BC \\leftarrow 0\\), and then separately to force \\(BC \\leftarrow 1\\), and looking at the resulting values of \\(D\\), we can infer precisely the causal impact of \\(BC\\) on \\(D\\), rather than just the co-occurrence-based conditional probability \\(\\Pr(D \\mid BC)\\).\n\nfrom pymc.model.transform.conditioning import do\ndo_BC1_model = do(dance_model, {'BC': 1})\npm.model_to_graphviz(do_BC1_model)\n\n\n\n\n\n\n\n\n\nwith do_BC1_model:\n    BC1_idata = pm.sample_prior_predictive(random_seed=5650)\n\nSampling: [D]\n\n\n\nBC1_idata_df = BC1_idata.prior.to_dataframe()\nBC1_idata_df\n\n\n\n\n\n\n\n\n\nD\n\n\nchain\ndraw\n\n\n\n\n\n0\n0\n1\n\n\n1\n1\n\n\n2\n1\n\n\n3\n1\n\n\n4\n1\n\n\n...\n...\n\n\n495\n1\n\n\n496\n1\n\n\n497\n1\n\n\n498\n1\n\n\n499\n1\n\n\n\n\n500 rows √ó 1 columns\n\n\n\n\nBC1_idata_df['D'].mean()\n\n1.0\n\n\n\nfrom pymc.model.transform.conditioning import do\ndo_BC0_model = do(dance_model, {'BC': 0})\npm.model_to_graphviz(do_BC0_model)\n\n\n\n\n\n\n\n\n\nwith do_BC0_model:\n    BC0_idata = pm.sample_prior_predictive(random_seed=5650)\nBC0_idata_df = BC0_idata.prior.to_dataframe()\nBC0_idata_df\n\nSampling: [D]\n\n\n\n\n\n\n\n\n\n\nD\n\n\nchain\ndraw\n\n\n\n\n\n0\n0\n0\n\n\n1\n0\n\n\n2\n0\n\n\n3\n0\n\n\n4\n0\n\n\n...\n...\n\n\n495\n0\n\n\n496\n0\n\n\n497\n0\n\n\n498\n0\n\n\n499\n0\n\n\n\n\n500 rows √ó 1 columns\n\n\n\n\nBC0_idata_df['D'].mean()\n\n0.0\n\n\nAnd so we get our causal effect:\n\\[\n\\mathbb{E}[D \\mid \\textsf{do}(BC \\leftarrow 1)] - \\mathbb{E}[D \\mid \\textsf{do}(BC \\leftarrow 0)] = 1 - 0 = 1\n\\]\n\n\n\\(\\textsf{do}()\\) vs.¬†Conditional Probability in this Model\nNow, what would the conditional probability \\(\\Pr(D \\mid BC)\\) give us here? Let‚Äôs check using the dance_prior_df DataFrame we created earlier, but just subsetting it to check co-occurrences rather than using the fancy do() function in PyMC:\n\nBC1_df = dance_prior_df[dance_prior_df['BC'] == 1].copy()\nBC1_df['D'].mean()\n\n1.0\n\n\n\nBC0_df = dance_prior_df[dance_prior_df['BC'] == 0].copy()\nBC0_df['D'].mean()\n\n0.0\n\n\nAnd so we have, in this case, that\n\\[\n\\mathbb{E}[D \\mid BC = 1] - \\mathbb{E}[D \\mid BC = 0] = 1 - 0 = 1,\n\\]\nmeaning that the causal effect is equal to the conditional effect:\n\\[\n\\mathbb{E}[D \\mid \\textsf{do}(BC \\leftarrow 1)] - \\mathbb{E}[D \\mid \\textsf{do}(BC \\leftarrow 0)] = \\mathbb{E}[D \\mid BC = 1] - \\mathbb{E}[D \\mid BC = 0]\n\\]\nWhy did we get this result in this case? If we tried to generalize from this (which lots of early scientists do), we might think it means that \\(\\textsf{do}()\\) is unnecessary overkill ‚Äì it looks like we can just use the conditional operator \\(|\\)‚Ä¶ But, in fact, the above equality only holds because the model was so simple that there were no backdoor paths which could bias the conditional probabilities away from the direct causal effect.\n\n\nThe Relation to Backdoor Paths\nThe Firing Squad example, however, is not so simple: on top of the direct path \\(A \\rightarrow D\\), there is a backdoor path \\(A \\leftarrow C \\rightarrow B \\rightarrow D\\), which has exactly the effect of making the \\(\\textsf{do}()\\)-based difference\n\\[\n\\mathbb{E}[D = 1 \\mid \\textsf{do}(A \\leftarrow 1)] - \\mathbb{E}[D = 1 \\mid \\textsf{do}(A \\leftarrow 0)]\n\\]\nnot equal to the conditional difference\n\\[\n\\mathbb{E}[D = 1 \\mid A = 1] - \\mathbb{E}[D = 1 \\mid A = 0].\n\\]\nIf we had enough data on \\(D\\) with respect to the behavior of \\(A\\) (for example, data on \\(A\\) shooting and not-shooting in different Firing Squads) and \\(C\\) (for example, cases where \\(C\\) decided not to follow the court orders, and refused to order the shooting), then we know from class that we could in fact infer the causal \\(\\textsf{do}()\\)-based effect from this observational data by closing the backdoor path:\nSince \\(C\\) is a fork in the triple \\(A \\leftarrow C \\rightarrow B\\), the first triple in the backdoor path here, we know that we could compute \\(\\mathbb{E}[D = 1 \\mid A = 1] - \\mathbb{E}[D = 1 \\mid A = 0]\\) separately for the two different values of \\(C\\) (\\(C = 1\\) and \\(C = 0\\)), and if our causal model was indeed the truth about the world then we would successfully obtain the \\(\\textsf{do}()\\)-based effect without actually having to intervene in the system!"
  },
  {
    "objectID": "writeups/birthday-instrument/index.html",
    "href": "writeups/birthday-instrument/index.html",
    "title": "Birthdays as Instruments for Catholic School Effects",
    "section": "",
    "text": "In this writeup, the goal is to walk through a third approach (in addition to adjusting for covariates and propensity score weighting) you can take towards tackling the issue of confounding. The following diagram illustrates what we‚Äôre hoping to achieve:\n\nWe want to estimate the (average) causal effect of Catholic Schooling \\(D\\) on Post-Grad Earnings \\(Y\\).\nHowever, there is an unmeasured covariate, Work Ethic \\(X\\) (a fork relative to \\(D \\leftarrow X \\rightarrow Y\\)), which is confounding this effect \\(D \\rightarrow Y\\).\nSo, as Angrist and Krueger (1991) proposed, we can ‚Äúwork around‚Äù this confounding (literally, given how the diagram is set up) by using day of birth relative to school cohort as an instrument:\n\nSince this day of birth has no direct causal relationship with work ethic, and\nHas no direct causal relationship with Post-Grad Earnings (only an indirect effect on it through Catholic Schooling \\(D\\)),\nThis cohort-relative date of birth can play the same role that a random coin flip plays in the ‚Äúgold standard‚Äù of randomized medical trials!\n\nUnder these assumptions (which need to be argued for! And are argued for in Angrist and Krueger (1991)), we can recover the causal effect of \\(D\\) on \\(Y\\) via the ‚Äúclassical IV estimator‚Äù:\n\n\\[\n\\beta_{\\text{IV}}^{D \\rightarrow Y} = \\frac{\\text{Cov}[Z,Y]}{\\text{Cov}[D,Y]}\n\\]\n\n\n\nCausal diagram illustrating the role of our proposed instrument \\(Z\\): a student‚Äôs day of birth within the cohort they enter school with. If \\(Z\\) has (a) no direct causal relationship with Work Ethic (\\(X\\)) and (b) no direct causal relationship with Post-Grad Earnings \\(Y\\) (that is, only an indirect effect on it through Catholic Schooling \\(D\\)), then we can recover the causal effect of Catholic Schooling on Post-Grad Earnings by computing \\(\\beta_{\\text{IV}}^{D \\rightarrow Y} = \\frac{\\text{Cov}[Z,Y]}{\\text{Cov}[D,Y]}\\)\n\n\nIn this writeup, we will generate simulated data on birthdates, school-dropout behavior, and eventual earnings, then use the above IV estimator formula to recover the causal effect of Catholic Schooling on Post-Grad Earnings. We can then verify the accuracy of this estimate, since we generated the data in the first place! The beauty of generative modeling üòâ"
  },
  {
    "objectID": "writeups/birthday-instrument/index.html#overview",
    "href": "writeups/birthday-instrument/index.html#overview",
    "title": "Birthdays as Instruments for Catholic School Effects",
    "section": "",
    "text": "In this writeup, the goal is to walk through a third approach (in addition to adjusting for covariates and propensity score weighting) you can take towards tackling the issue of confounding. The following diagram illustrates what we‚Äôre hoping to achieve:\n\nWe want to estimate the (average) causal effect of Catholic Schooling \\(D\\) on Post-Grad Earnings \\(Y\\).\nHowever, there is an unmeasured covariate, Work Ethic \\(X\\) (a fork relative to \\(D \\leftarrow X \\rightarrow Y\\)), which is confounding this effect \\(D \\rightarrow Y\\).\nSo, as Angrist and Krueger (1991) proposed, we can ‚Äúwork around‚Äù this confounding (literally, given how the diagram is set up) by using day of birth relative to school cohort as an instrument:\n\nSince this day of birth has no direct causal relationship with work ethic, and\nHas no direct causal relationship with Post-Grad Earnings (only an indirect effect on it through Catholic Schooling \\(D\\)),\nThis cohort-relative date of birth can play the same role that a random coin flip plays in the ‚Äúgold standard‚Äù of randomized medical trials!\n\nUnder these assumptions (which need to be argued for! And are argued for in Angrist and Krueger (1991)), we can recover the causal effect of \\(D\\) on \\(Y\\) via the ‚Äúclassical IV estimator‚Äù:\n\n\\[\n\\beta_{\\text{IV}}^{D \\rightarrow Y} = \\frac{\\text{Cov}[Z,Y]}{\\text{Cov}[D,Y]}\n\\]\n\n\n\nCausal diagram illustrating the role of our proposed instrument \\(Z\\): a student‚Äôs day of birth within the cohort they enter school with. If \\(Z\\) has (a) no direct causal relationship with Work Ethic (\\(X\\)) and (b) no direct causal relationship with Post-Grad Earnings \\(Y\\) (that is, only an indirect effect on it through Catholic Schooling \\(D\\)), then we can recover the causal effect of Catholic Schooling on Post-Grad Earnings by computing \\(\\beta_{\\text{IV}}^{D \\rightarrow Y} = \\frac{\\text{Cov}[Z,Y]}{\\text{Cov}[D,Y]}\\)\n\n\nIn this writeup, we will generate simulated data on birthdates, school-dropout behavior, and eventual earnings, then use the above IV estimator formula to recover the causal effect of Catholic Schooling on Post-Grad Earnings. We can then verify the accuracy of this estimate, since we generated the data in the first place! The beauty of generative modeling üòâ"
  },
  {
    "objectID": "writeups/birthday-instrument/index.html#uniformly-distributed-birthdays-and-catholic-schooling",
    "href": "writeups/birthday-instrument/index.html#uniformly-distributed-birthdays-and-catholic-schooling",
    "title": "Birthdays as Instruments for Catholic School Effects",
    "section": "Uniformly-Distributed Birthdays and Catholic Schooling",
    "text": "Uniformly-Distributed Birthdays and Catholic Schooling\n\n\nCode\nlibrary(tidyverse) |&gt; suppressPackageStartupMessages()\nlibrary(lubridate) |&gt; suppressPackageStartupMessages()\nlibrary(extraDistr) |&gt; suppressPackageStartupMessages()\nlibrary(scales) |&gt; suppressPackageStartupMessages()\nset.seed(5650)\nn &lt;- 10000\ndob_vals &lt;- lubridate::make_date(1999, 8, 27)\ncath_vals &lt;- rbern(n, prob=0.5)\noffset_vals &lt;- rdunif(n, min=0, max=364)\nstudent_df &lt;- tibble(day_of_yr=offset_vals, catholic=factor(cath_vals)) |&gt;\n  mutate(\n    dob=dob_vals + duration(day_of_yr, unit=\"days\") - duration(1, unit=\"days\"),\n    turns_18 = lubridate::as_date(dob + duration(18, unit=\"years\"))\n  ) |&gt;\n  arrange(dob)\n#duration(sample(0:364, n()), unit = \"days\"))\nstudent_df &lt;- student_df |&gt; mutate(\n  year_abbr = as.character(lubridate::year(dob)),\n  month_str = factor(\n    paste0(\"\",month.abb[lubridate::month(dob)],\"\",str_sub(year_abbr, 3, 4)),\n    levels=c(\n      \"Aug99\",\"Sep99\",\"Oct99\",\"Nov99\",\"Dec99\",\"Jan00\",\"Feb00\",\n      \"Mar00\",\"Apr00\",\"May00\",\"Jun00\",\"Jul00\",\"Aug00\"\n    )\n  )\n)\nstudent_df |&gt; head()\n\n\n\n\n\n\nday_of_yr\ncatholic\ndob\nturns_18\nyear_abbr\nmonth_str\n\n\n\n\n0\n0\n1999-08-26\n2017-08-25\n1999\nAug99\n\n\n0\n0\n1999-08-26\n2017-08-25\n1999\nAug99\n\n\n0\n1\n1999-08-26\n2017-08-25\n1999\nAug99\n\n\n0\n1\n1999-08-26\n2017-08-25\n1999\nAug99\n\n\n0\n0\n1999-08-26\n2017-08-25\n1999\nAug99\n\n\n0\n0\n1999-08-26\n2017-08-25\n1999\nAug99"
  },
  {
    "objectID": "writeups/birthday-instrument/index.html#number-of-students-per-month",
    "href": "writeups/birthday-instrument/index.html#number-of-students-per-month",
    "title": "Birthdays as Instruments for Catholic School Effects",
    "section": "Number of Students Per Month",
    "text": "Number of Students Per Month\n\n\nCode\nmonth_df &lt;- student_df |&gt; group_by(month_str, catholic) |&gt; summarize(month_total=n())\nmonth_df |&gt; ggplot(aes(x=month_str,y=month_total, fill=catholic)) +\n  geom_bar(stat='identity') +\n  theme_classic()"
  },
  {
    "objectID": "writeups/birthday-instrument/index.html#now-the-dropping-out-simulation",
    "href": "writeups/birthday-instrument/index.html#now-the-dropping-out-simulation",
    "title": "Birthdays as Instruments for Catholic School Effects",
    "section": "Now the Dropping-Out Simulation",
    "text": "Now the Dropping-Out Simulation\n\nFirst day of school year: Aug 26\nLast day of school: Jun 18\n\\(\\implies\\) This cohort = all who turn 5 yrs old between Aug 27 2004 and Aug 26 2005\n\\(\\implies\\) All born between Aug 27 1999 and Aug 26 2000\n\n\n\nCode\nstudent_df &lt;- student_df |&gt; mutate(\n  can_drop_out = as.numeric(turns_18 &lt;= lubridate::make_date(2018, 6, 18))\n)\nstudent_df |&gt; group_by(dob) |&gt; summarize(sum(can_drop_out)) |&gt; head()\n\n\n\n\n\n\ndob\nsum(can_drop_out)\n\n\n\n\n1999-08-26\n25\n\n\n1999-08-27\n30\n\n\n1999-08-28\n30\n\n\n1999-08-29\n29\n\n\n1999-08-30\n23\n\n\n1999-08-31\n24\n\n\n\n\n\n\nWhat percentage of students will actually have a chance to drop out? i.e., What proportion of students are born so that they turn 18 before the last day of school?\n\n\nCode\n(elig_prop &lt;- student_df |&gt; summarize(elig_prop=mean(can_drop_out)) |&gt; pull())\n\n\n[1] 0.8174\n\n\nCode\n# National mean 8.6%\nraw_dropout_rate &lt;- 0.086\n(dropout_rate &lt;- (1/elig_prop) * raw_dropout_rate)\n\n\n[1] 0.1052116\n\n\n\nDCPS School Year Calendar\n\n\n\nCode\n# Simulate desire to drop out\nset.seed(5651)\nwtd_vals &lt;- rbern(nrow(student_df), prob=dropout_rate)\nstudent_df$wants_to_drop &lt;- wtd_vals\nstudent_df &lt;- student_df |&gt; mutate(\n  dropout = as.numeric(wants_to_drop==1 & can_drop_out==1)\n)\nmean(student_df$dropout)\n\n\n[1] 0.0846\n\n\n\n\nCode\nstudent_df |&gt; group_by(turns_18) |&gt; summarize(\n  dob_dropouts=sum(dropout),\n  wants_dropout=sum(wants_to_drop),\n  total=n(), \n) |&gt; head()\n\n\n\n\n\n\nturns_18\ndob_dropouts\nwants_dropout\ntotal\n\n\n\n\n2017-08-25\n1\n1\n25\n\n\n2017-08-26\n5\n5\n30\n\n\n2017-08-27\n7\n7\n30\n\n\n2017-08-28\n3\n3\n29\n\n\n2017-08-29\n1\n1\n23\n\n\n2017-08-30\n3\n3\n24"
  },
  {
    "objectID": "writeups/birthday-instrument/index.html#compute-cumulative-number-of-dropouts-over-time",
    "href": "writeups/birthday-instrument/index.html#compute-cumulative-number-of-dropouts-over-time",
    "title": "Birthdays as Instruments for Catholic School Effects",
    "section": "Compute Cumulative Number of Dropouts Over Time",
    "text": "Compute Cumulative Number of Dropouts Over Time\n\n\nCode\ndob_df &lt;- student_df |&gt; group_by(dob, catholic) |&gt; summarize(\n  dob_dropout=sum(dropout)\n)\n\n\n`summarise()` has grouped output by 'dob'. You can override using the `.groups`\nargument.\n\n\nCode\n#dob_df$drop_todate &lt;- cumsum(dob_df$dob_dropout)\n#dob_df |&gt; group_by(catholic) |&gt; summarize(drop_todate=cumsum(dob_dropout)) |&gt; ungroup()\ndob_df &lt;- dob_df |&gt; group_by(catholic) |&gt; arrange(dob) |&gt;\n  mutate(drop_todate=cumsum(dob_dropout)) |&gt;\n  ungroup()"
  },
  {
    "objectID": "writeups/birthday-instrument/index.html#plot-cumulative-dropouts-by-day",
    "href": "writeups/birthday-instrument/index.html#plot-cumulative-dropouts-by-day",
    "title": "Birthdays as Instruments for Catholic School Effects",
    "section": "Plot Cumulative Dropouts by Day",
    "text": "Plot Cumulative Dropouts by Day\n\n\nCode\ndob_df |&gt; ggplot(aes(x=dob, y=drop_todate, color=catholic)) +\n  geom_line(linewidth=0.5) +\n  geom_vline(\n    xintercept=lubridate::make_date(2000, 6, 18),\n    linetype='dashed',\n  ) +\n  theme_classic()"
  },
  {
    "objectID": "writeups/birthday-instrument/index.html#plot-cumulative-dropouts-by-month",
    "href": "writeups/birthday-instrument/index.html#plot-cumulative-dropouts-by-month",
    "title": "Birthdays as Instruments for Catholic School Effects",
    "section": "Plot Cumulative Dropouts by Month",
    "text": "Plot Cumulative Dropouts by Month\n\n\nCode\ndob_df &lt;- dob_df |&gt; mutate(\n  year_abbr = as.character(lubridate::year(dob)),\n  month_str = factor(\n    paste0(\"\",month.abb[lubridate::month(dob)],\"\",str_sub(year_abbr, 3, 4)),\n    levels=c(\n      \"Aug99\",\"Sep99\",\"Oct99\",\"Nov99\",\"Dec99\",\"Jan00\",\"Feb00\",\n      \"Mar00\",\"Apr00\",\"May00\",\"Jun00\",\"Jul00\",\"Aug00\"\n    )\n  )\n)\nmonth_drop_df &lt;- dob_df |&gt; group_by(month_str, catholic) |&gt;\n  summarize(month_dropout=sum(dob_dropout))\n\n\n`summarise()` has grouped output by 'month_str'. You can override using the\n`.groups` argument.\n\n\nCode\nmonth_drop_df$total_dropouts &lt;- cumsum(month_drop_df$month_dropout)\nmonth_drop_df &lt;- month_drop_df |&gt; mutate(\n  next_month = lead(month_str, default=\"Aug00\"),\n  prev_month_dropouts = lag(total_dropouts, default=0),\n  change = total_dropouts - prev_month_dropouts\n)\n# month_drop_df\nmonth_drop_df |&gt; ggplot(aes(x=month_str, y=total_dropouts, fill=catholic)) +\n  geom_bar(stat='identity') +\n  # geom_segment(aes(xend=next_month), linetype=\"dashed\") +\n  geom_text(\n    aes(label=month_dropout),\n    position=position_dodge(width=0.9),\n    vjust=-0.25, size=3.5, show.legend=FALSE\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n(Sanity check: wtd_df = students who want to drop, per month)\n\n\nCode\nwtd_df &lt;- student_df |&gt; group_by(month_str, catholic) |&gt;\n  summarize(total_wtd=sum(wants_to_drop))\n\n\n`summarise()` has grouped output by 'month_str'. You can override using the\n`.groups` argument.\n\n\nCode\nwtd_df |&gt; ggplot(aes(x=month_str, y=total_wtd, fill=catholic)) +\n  geom_bar(stat='identity') +\n  theme_classic()"
  },
  {
    "objectID": "writeups/birthday-instrument/index.html#compute-total-years-with-decimals-of-schooling",
    "href": "writeups/birthday-instrument/index.html#compute-total-years-with-decimals-of-schooling",
    "title": "Birthdays as Instruments for Catholic School Effects",
    "section": "Compute Total Years (with Decimals) of Schooling",
    "text": "Compute Total Years (with Decimals) of Schooling\n\n\nCode\ngrad_day &lt;- lubridate::make_date(2018, 6, 18)\nstudent_df &lt;- student_df |&gt; mutate(\n  schooling_start = lubridate::make_date(2005, 8, 26),\n  schooling_end = as_date(ifelse(dropout==1, as_date(turns_18), as_date(grad_day))),\n  school_days = difftime(schooling_end, schooling_start, units=\"days\"),\n  school_yrs = interval(schooling_start, schooling_end) / years(1)\n)\nstudent_df |&gt; head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nday_of_yr\ncatholic\ndob\nturns_18\nyear_abbr\nmonth_str\ncan_drop_out\nwants_to_drop\ndropout\nschooling_start\nschooling_end\nschool_days\nschool_yrs\n\n\n\n\n0\n0\n1999-08-26\n2017-08-25\n1999\nAug99\n1\n0\n0\n2005-08-26\n2018-06-18\n4679 days\n12.81096\n\n\n0\n0\n1999-08-26\n2017-08-25\n1999\nAug99\n1\n0\n0\n2005-08-26\n2018-06-18\n4679 days\n12.81096\n\n\n0\n1\n1999-08-26\n2017-08-25\n1999\nAug99\n1\n0\n0\n2005-08-26\n2018-06-18\n4679 days\n12.81096\n\n\n0\n1\n1999-08-26\n2017-08-25\n1999\nAug99\n1\n0\n0\n2005-08-26\n2018-06-18\n4679 days\n12.81096\n\n\n0\n0\n1999-08-26\n2017-08-25\n1999\nAug99\n1\n0\n0\n2005-08-26\n2018-06-18\n4679 days\n12.81096\n\n\n0\n0\n1999-08-26\n2017-08-25\n1999\nAug99\n1\n0\n0\n2005-08-26\n2018-06-18\n4679 days\n12.81096"
  },
  {
    "objectID": "writeups/birthday-instrument/index.html#simulated-earnings-n-years-later",
    "href": "writeups/birthday-instrument/index.html#simulated-earnings-n-years-later",
    "title": "Birthdays as Instruments for Catholic School Effects",
    "section": "Simulated Earnings \\(n\\) Years Later",
    "text": "Simulated Earnings \\(n\\) Years Later\n\n\nCode\nstudent_df$earnings_noise &lt;- rnorm(nrow(student_df), 0, 500)\nstudent_df &lt;- student_df |&gt; mutate(\n  earnings = 10000 * school_yrs + 5000 * as.numeric(catholic) + earnings_noise\n)\nstudent_df |&gt; ggplot(aes(x=earnings, fill=catholic)) + \n  geom_density() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nCode\nstudent_df |&gt; ggplot(aes(x=school_yrs, fill=catholic)) +\n  geom_density(alpha=0.5) +\n  theme_classic()"
  },
  {
    "objectID": "writeups/birthday-instrument/index.html#mean-years-of-schooling-by-birth-month",
    "href": "writeups/birthday-instrument/index.html#mean-years-of-schooling-by-birth-month",
    "title": "Birthdays as Instruments for Catholic School Effects",
    "section": "Mean Years of Schooling by Birth Month",
    "text": "Mean Years of Schooling by Birth Month\n\n\nCode\nschooling_df &lt;- student_df |&gt; group_by(month_str, catholic) |&gt; summarize(\n  mean_yrs=mean(school_yrs),\n  n=n()\n)\n\n\n`summarise()` has grouped output by 'month_str'. You can override using the\n`.groups` argument.\n\n\nCode\n# Plot mean school days by month\nschooling_df |&gt; ggplot(aes(x=month_str, y=mean_yrs, color=catholic)) +\n  geom_vline(aes(xintercept=month_str), linetype='dashed', linewidth=0.5, alpha=0.18) +\n  geom_point(size=2) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nCode\n(max_schooling &lt;- max(student_df$school_yrs))\n\n\n[1] 12.81096\n\n\n\n\nCode\nq1 &lt;- c(\"Aug99\",\"Sep99\",\"Oct99\",\"Nov99\")\nq2 &lt;- c(\"Dec99\",\"Jan00\",\"Feb00\")\nq3 &lt;- c(\"Mar00\",\"Apr00\",\"May00\")\nq4 &lt;- c(\"Jun00\",\"Jul00\",\"Aug00\")\nschooling_df &lt;- schooling_df |&gt; mutate(\n  qtr = case_match(\n    month_str,\n    q1 ~ 1,\n    q2 ~ 2,\n    q3 ~ 3,\n    q4 ~ 4\n  )\n)\nqtr_df &lt;- schooling_df |&gt; group_by(qtr, catholic) |&gt; summarize(\n  mean_yrs_qtr=mean(mean_yrs),\n  n_qtr=sum(n)\n)\n\n\n`summarise()` has grouped output by 'qtr'. You can override using the `.groups`\nargument.\n\n\nCode\nqtr_df |&gt; ggplot(aes(x=qtr, y=mean_yrs_qtr, color=catholic)) +\n  geom_point(\n    stat='identity', size=2, position=position_dodge2(width=0.4)\n  ) +\n  geom_segment(\n    aes(yend=max_schooling),\n    # position=position_jitterdodge(\n    #   #dodge.width=0.5\n    # ),\n    position=position_dodge2(width=0.4),\n    linetype='dotted', linewidth=0.5\n  ) +\n  # ylim(12.759, 12.82) +\n  geom_hline(\n    aes(\n      yintercept=max_schooling,\n      #linetype='Max Possible'\n    ),\n    linewidth=0.5\n  ) +\n  # scale_linetype_manual(\"\", values=c(\"dashed\")) +\n  # geom_text(x=2.5, y=12.81, label='Maximum Possible (Non-Dropout Amount)', vjust=-1) +\n  labs(\n    title=\"Schooling Gap By Quarter of Birth\",\n    x=\"DOB Quarter\",\n    y=\"Mean Years of Schooling\"\n  ) +\n  theme_classic(base_size=14)\n\n\n\n\n\n\n\n\n\n\n\nCode\nschooling_df &lt;- schooling_df |&gt; mutate(\n  half = factor(\n    ifelse(qtr==1 | qtr==2, 0, 1),\n    levels=c(0,1)\n  )\n)\nhalf_df &lt;- schooling_df |&gt; group_by(half, catholic) |&gt; summarize(\n  mean_yrs_half=mean(mean_yrs),\n  n_half=sum(n)\n)\n\n\n`summarise()` has grouped output by 'half'. You can override using the\n`.groups` argument.\n\n\nCode\nhalf_df |&gt; ggplot(aes(x=half, y=mean_yrs_half, color=catholic)) +\n  geom_point(\n    stat='identity', size=2.5,\n    position=position_dodge2(width=0.4)\n  ) +\n  geom_segment(\n    aes(yend=max_schooling),\n    position=position_dodge2(width=0.4),\n    linetype='dotted', linewidth=0.6\n  ) +\n  # ylim(12.759, 12.82) +\n  geom_hline(\n    aes(\n      yintercept=max_schooling,\n      #linetype='Max Possible'\n    ),\n    linewidth=0.5\n  ) +\n  # scale_linetype_manual(\"\", values=c(\"dashed\")) +\n  # geom_text(x=1.5, y=12.81, label='Maximum Possible (Non-Dropout Amount)', vjust=-1) +\n  labs(\n    title=\"Schooling Gap By DOB Half\",\n    x=\"DOB Half\",\n    y=\"Mean Years of Schooling\"\n  ) +\n  #xlim(0.5, 2.5) +\n  # ylim(12.76, 12.82) +\n  theme_classic(base_size=14)\n\n\n\n\n\n\n\n\n\n\n\nCode\nh0 &lt;- c(\"Aug99\",\"Sep99\",\"Oct99\",\"Nov99\",\"Dec99\",\"Jan00\",\"Feb00\")\nh1 &lt;- c(\"Mar00\",\"Apr00\",\"May00\",\"Jun00\",\"Jul00\",\"Aug00\")\nstudent_df &lt;- student_df |&gt; mutate(\n  half = factor(\n    ifelse(month_str %in% h0, 0, 1),\n    levels=c(0,1)\n  )\n)\nearn_df &lt;- student_df |&gt; group_by(half, catholic) |&gt;\n  summarize(mean_earnings=mean(earnings))\n\n\n`summarise()` has grouped output by 'half'. You can override using the\n`.groups` argument.\n\n\nCode\nearn_df\n\n\n\n\n\n\nhalf\ncatholic\nmean_earnings\n\n\n\n\n0\n0\n132518.2\n\n\n0\n1\n137524.1\n\n\n1\n0\n132994.3\n\n\n1\n1\n138005.8\n\n\n\n\n\n\nCode\n# earn_df |&gt; pivot_wider(-half)\nearn_df |&gt; ggplot(aes(x=half, y=log(mean_earnings), color=catholic, group=catholic)) +\n  geom_point(stat='identity', size=2.5) +\n  geom_line() +\n  # ylim(12.759, 12.82) +\n  # scale_linetype_manual(\"\", values=c(\"dashed\")) +\n  labs(\n    title=\"Earnings Gap By DOB Half\",\n    x=\"DOB Half\",\n    y=\"Mean Earnings\"\n  ) +\n  #xlim(0.5, 2.5) +\n  # ylim(127400, 128100) +\n  theme_classic(base_size=14)\n\n\n\n\n\n\n\n\n\n\n\nCode\nstudent_df &lt;- student_df |&gt; mutate(\n  qtr = case_match(\n    month_str,\n    q1 ~ 1,\n    q2 ~ 2,\n    q3 ~ 3,\n    q4 ~ 4\n  )\n)\nearn_df &lt;- student_df |&gt; group_by(qtr, catholic) |&gt;\n  summarize(mean_earnings=mean(earnings))\n\n\n`summarise()` has grouped output by 'qtr'. You can override using the `.groups`\nargument.\n\n\nCode\nearn_df\n\n\n\n\n\n\nqtr\ncatholic\nmean_earnings\n\n\n\n\n1\n0\n132374.0\n\n\n1\n1\n137431.5\n\n\n2\n0\n132665.2\n\n\n2\n1\n137625.5\n\n\n3\n0\n132902.5\n\n\n3\n1\n137922.4\n\n\n4\n0\n133094.5\n\n\n4\n1\n138098.6\n\n\n\n\n\n\nCode\n# earn_df |&gt; pivot_wider(-half)\nearn_df |&gt; ggplot(aes(x=qtr, y=log(mean_earnings), color=catholic, group=catholic)) +\n  geom_point(stat='identity', size=2.5) +\n  geom_line() +\n  # ylim(12.759, 12.82) +\n  # scale_linetype_manual(\"\", values=c(\"dashed\")) +\n  labs(\n    title=\"Earnings Gap By DOB Half\",\n    x=\"DOB Quarter\",\n    y=\"Mean Earnings\"\n  ) +\n  #xlim(0.5, 2.5) +\n  # ylim(127400, 128100) +\n  theme_classic(base_size=14)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlm(mean_earnings ~ as.numeric(qtr), data=earn_df)\n\n\n\nCall:\nlm(formula = mean_earnings ~ as.numeric(qtr), data = earn_df)\n\nCoefficients:\n    (Intercept)  as.numeric(qtr)  \n       134677.1            234.9  \n\n\n\n\nCode\nlm(as.numeric(catholic) ~ as.numeric(qtr), data=earn_df)\n\n\n\nCall:\nlm(formula = as.numeric(catholic) ~ as.numeric(qtr), data = earn_df)\n\nCoefficients:\n    (Intercept)  as.numeric(qtr)  \n      1.500e+00        2.402e-16  \n\n\n\n\nCode\nlm(mean_earnings ~ as.numeric(catholic), data=earn_df)\n\n\n\nCall:\nlm(formula = mean_earnings ~ as.numeric(catholic), data = earn_df)\n\nCoefficients:\n         (Intercept)  as.numeric(catholic)  \n              127749                  5010"
  },
  {
    "objectID": "w11/index.html",
    "href": "w11/index.html",
    "title": "Week 11: Sensitivity Analysis",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#topic-models",
    "href": "w11/index.html#topic-models",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Topic Models",
    "text": "Topic Models\n\nIntuition is just: let‚Äôs model latent topics ‚Äúunderlying‚Äù observed words\n\n\n\n\nSection\nKeywords\n\n\n\n\nU.S. News\nstate, court, federal, republican\n\n\nWorld News\ngovernment, country, officials, minister\n\n\nArts\nmusic, show, art, dance\n\n\nSports\ngame, league, team, coach\n\n\nReal Estate\nhome, bedrooms, bathrooms, building\n\n\n\n\nAlready, by just classifying articles based on these keyword counts:\n\n\n\n\n\nArts\nReal Estate\nSports\nU.S. News\nWorld News\n\n\n\n\nCorrect\n3020\n690\n4860\n1330\n1730\n\n\nIncorrect\n750\n60\n370\n1100\n590\n\n\nAccuracy\n0.801\n0.920\n0.929\n0.547\n0.746",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#topic-models-as-pgms",
    "href": "w11/index.html#topic-models-as-pgms",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Topic Models as PGMs",
    "text": "Topic Models as PGMs\n\n\n\nFrom Blei (2012)\n\n\n\n‚Ä¶Unlocks a world of social modeling through text!",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#cross-sectional-analysis",
    "href": "w11/index.html#cross-sectional-analysis",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Cross-Sectional Analysis",
    "text": "Cross-Sectional Analysis\nBlaydes, Grimmer, and McQueen (2018)",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#influence-over-time",
    "href": "w11/index.html#influence-over-time",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Influence Over Time",
    "text": "Influence Over Time\n\n\n\nFrom Barron et al. (2018)",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#textual-influence-over-time",
    "href": "w11/index.html#textual-influence-over-time",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Textual Influence Over Time",
    "text": "Textual Influence Over Time",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#the-beta-distribution-textbetaalpha-beta-the-workhorse-prior",
    "href": "w11/index.html#the-beta-distribution-textbetaalpha-beta-the-workhorse-prior",
    "title": "Week 11: Sensitivity Analysis",
    "section": "The Beta Distribution \\(\\text{Beta}(\\alpha, \\beta)\\): The ‚ÄúWorkhorse‚Äù Prior!",
    "text": "The Beta Distribution \\(\\text{Beta}(\\alpha, \\beta)\\): The ‚ÄúWorkhorse‚Äù Prior!\n\nBiased coin framing: \\(\\alpha\\) is ‚Äúpseudo-count‚Äù of heads, \\(\\beta\\) = ‚Äúpseudo-count‚Äù of tails\n\n\n\n\n \\(p \\sim\\) ‚Äú\\(\\text{Beta}(0, 0)\\)‚Äù\n\n\n\nCode\n# b00_df = pd.DataFrame({'p_heads': np.arange(0, 1+1/25, 1/25)})\n# b00_df['Probability'] = 0\nax = pw.Brick(figsize=(2, 1.25))\nsns.histplot(\n  x=[], ax=ax, # data=b00_df\n);\nax.set_xlabel('p_heads');\nax.set_ylabel('Probability')\nax.set_xticks([0, 0.25, 0.5, 0.75, 1]);\nax.set_ylim(0, 1);\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\n \\(p \\sim\\) ‚Äú\\(\\text{Beta}(1, 0)\\)‚Äù\n\n\n\nCode\nb10_df = pd.DataFrame({\n  'draw': [0],\n  'p_heads': [0],\n})\nax = pw.Brick(figsize=(2, 1.25))\nsns.histplot(\n  x='p_heads', stat='probability', ax=ax,\n  bins=25,\n  data=b10_df\n);\nax.set_xlim(0, 1);\nax.set_ylim(0, 1);\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\n \\(p \\sim\\) ‚Äú\\(\\text{Beta}(0, 1)\\)‚Äù\n\n\n\nCode\nb01_df = pd.DataFrame({\n  'draw': [0],\n  'p_heads': [1],\n})\nax = pw.Brick(figsize=(2, 1.25))\nsns.histplot(\n  x='p_heads', stat='probability', ax=ax,\n  bins=25,\n  data=b01_df\n);\nax.set_xlim(0, 1);\nax.set_ylim(0, 1);\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Informative Prior\n\n\n\nCode\n# Informative Prior\nwith pm.Model() as informative_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=2, beta=2)\n  result = pm.Bernoulli(\"result\", p=p_heads)\ninformative_df = draw_prior_sample(informative_model)\ninformative_plot = gen_dist_plot(informative_df, \"Beta(2, 2) Prior on p\");\ninformative_plot.savefig()\n\n\nSampling: [p_heads, result]\n\n\n\n\n\n\n\n\nFig¬†1. ‚ÄúI‚Äôll start by assuming a fair coin‚Äù\n\n\n\n\n\n\n\n Weak Prior\n\n\n\nCode\n# Weakly Informative\nwith pm.Model() as weak_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=1, beta=1)\n  result = pm.Bernoulli(\"result\", p=p_heads)\nweak_df = draw_prior_sample(weak_model)\nweak_plot = gen_dist_plot(weak_df, \"Beta(1, 1) Prior on p\");\nweak_plot.savefig()\n\n\nSampling: [p_heads, result]\n\n\n\n\n\n\n\n\nFig¬†2. ‚ÄúI have no idea, I‚Äôll assume all biases equally likely‚Äù\n\n\n\n\n\n\n\n Skeptical (Jeffreys) Prior\n\n\n\nCode\n# Skeptical / Jeffreys\nwith pm.Model() as skeptical_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=0.5, beta=0.5)\n  result = pm.Bernoulli(\"result\", p=p_heads)\nskeptical_df = draw_prior_sample(skeptical_model)\nskeptical_plot = gen_dist_plot(skeptical_df, \"Beta(0.5, 0.5) Prior on p\");\n# And combine\nskeptical_plot.savefig()\n\n\nSampling: [p_heads, result]\n\n\n\n\n\n\n\n\nFig¬†3. ‚ÄúI don‚Äôt trust this coin, it‚Äôll take lots of flips with H/T balance to convince me it‚Äôs fair!‚Äù",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#modeling-domain-knowledge-with-priors",
    "href": "w11/index.html#modeling-domain-knowledge-with-priors",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Modeling Domain Knowledge with Priors",
    "text": "Modeling Domain Knowledge with Priors\nPopulation proportion framing: \\(\\frac{\\alpha}{\\alpha + \\beta}\\) = mean, \\(\\alpha + \\beta\\) = ‚Äúprecision‚Äù\n\n\n\n\nCode\nwith pm.Model() as heads_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=2, beta=1)\n  result = pm.Bernoulli(\"result\", p=p_heads)\nheads_df = draw_prior_sample(heads_model)\nheads_plot = gen_dist_plot(heads_df, \"Beta(2, 1) Prior on p\");\nheads_plot.savefig()\n\n\nSampling: [p_heads, result]\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith pm.Model() as tails_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=1, beta=2)\n  result = pm.Bernoulli(\"result\", p=p_heads)\ntails_df = draw_prior_sample(tails_model)\ntails_plot = gen_dist_plot(tails_df, \"Beta(1, 2) Prior on p\");\ntails_plot.savefig()\n\n\nSampling: [p_heads, result]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith pm.Model() as beta23_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=2, beta=3)\n  result = pm.Bernoulli(\"result\", p=p_heads)\nbeta23_df = draw_prior_sample(beta23_model)\nbeta23_plot = gen_dist_plot(beta23_df, \"Beta(2, 3) Prior on p\");\nbeta23_plot.savefig()\n\n\nSampling: [p_heads, result]\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith pm.Model() as beta100_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=40, beta=60)\n  result = pm.Bernoulli(\"result\", p=p_heads)\nbeta100_df = draw_prior_sample(beta100_model)\nbeta100_plot = gen_dist_plot(beta100_df, \"Beta(40, 60) Prior on p\");\nbeta100_plot.savefig()\n\n\nSampling: [p_heads, result]",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#does-aging-cause-sadness",
    "href": "w11/index.html#does-aging-cause-sadness",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Does Aging Cause Sadness?",
    "text": "Does Aging Cause Sadness?\n\n\n\n\nEach year, 20 people are born with uniformly-distributed happiness values\n\n\n\nEach year, each person ages one year; Happiness does not change\n\n\n\nAt age 18, individuals can become married; Odds of marriage each year proportional to individual's happiness; Once married, they remain married\n\n\n\nAt age 70 individuals leave the sample (They move to Boca Raton, Florida)\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nrng = np.random.default_rng(seed=5650)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncb_palette = ['#e69f00','#56b4e9','#009e73']\nsns.set_palette(cb_palette)\nimport patchworklib as pw\nfrom scipy.special import expit\n\n# The original R code:\n# sim_happiness &lt;- function( seed=1977 , N_years=1000 , max_age=65 , N_births=20 , aom=18 ) {\n#     set.seed(seed)\n#     H &lt;- M &lt;- A &lt;- c()\n#     for ( t in 1:N_years ) {\n#         A &lt;- A + 1 # age existing individuals\n#         A &lt;- c( A , rep(1,N_births) ) # newborns\n#         H &lt;- c( H , seq(from=-2,to=2,length.out=N_births) ) # sim happiness trait - never changes\n#         M &lt;- c( M , rep(0,N_births) ) # not yet married\n#         # for each person over 17, chance get married\n#         for ( i in 1:length(A) ) {\n#             if ( A[i] &gt;= aom & M[i]==0 ) {\n#                 M[i] &lt;- rbern(1,inv_logit(H[i]-4))\n#             }\n#         }\n#         # mortality\n#         deaths &lt;- which( A &gt; max_age )\n#         if ( length(deaths)&gt;0 ) {\n#             A &lt;- A[ -deaths ]\n#             H &lt;- H[ -deaths ]\n#             M &lt;- M[ -deaths ]\n#        }\n#     }\n#     d &lt;- data.frame(age=A,married=M,happiness=H)\n#     return(d)\n\n# DGP: happiness -&gt; marriage &lt;- age\nyears = 70\nnum_births = 41\ncolnames = ['age','a','h','m']\nsim_dfs = []\nA = np.zeros(shape=(num_births,1))\nH = np.linspace(-2, 2, num=num_births)\nM = np.zeros(shape=(num_births,1))\ndef update_m(row):\n  if row['m'] == 0:\n    return int(rng.binomial(\n      n=1,\n      p=expit(row['h'] - 3.875),\n      size=1,\n    ))\n  return 1\ndef sim_cohort_to(max_age):\n  sim_df = pd.DataFrame({\n      'age': [1 for _ in range(num_births)],\n      'h': np.linspace(-2, 2, num=num_births),\n      'm': [0 for _ in range(num_births)],\n    }\n  )\n  for t in range(2, max_age + 1):\n    sim_df['age'] = sim_df['age'] + 1\n    if t &gt;= 18:\n      sim_df['m'] = sim_df.apply(update_m, axis=1)\n  return sim_df\nall_sim_dfs = []\nfor cur_max_age in range(1, 71):\n  cur_sim_df = sim_cohort_to(cur_max_age)\n  all_sim_dfs.append(cur_sim_df)\nfull_sim_df = pd.concat(all_sim_dfs)\n\n# full_sim_df.head()\ncbg_palette = ['#c6c6c666'] + cb_palette\nfull_sim_df['m_label'] = full_sim_df['m'].apply(lambda x: \"Unmarried\" if x == 0 else \"Married\")\nfull_sim_df = full_sim_df.rename(columns={'age': 'Age', 'h': 'Happiness'})\nax = pw.Brick(figsize=(5.25,2.75));\nsns.scatterplot(\n  x='Age', y='Happiness', hue='m_label',\n  data=full_sim_df,\n  ax=ax,\n  palette=cbg_palette,\n  s=22,\n  legend=True,\n);\nax.move_legend(\"upper center\", bbox_to_anchor=(0.5, 1.15), ncol=2);\nax.legend_.set_title(\"\");\nax.axvline(x=17.5, color='black', ls='dashed', lw=1);\nax.savefig()\nmean_hap_df = full_sim_df.groupby('Age')['Happiness'].mean().reset_index()\nmean_hap_df['m_label'] = \"All\"\nmean_hap_df['Happiness_mean'] = 0\ngroup_hap_df = full_sim_df[full_sim_df['Age'] &gt;= 18].groupby(['Age','m_label'])['Happiness'].mean().reset_index()\nmarried_df = group_hap_df[group_hap_df['m_label'] == \"Married\"].copy()\nunmarried_df = group_hap_df[group_hap_df['m_label'] == \"Unmarried\"].copy()\n# Moving averages\nwin_size = 3\nmarried_df['Happiness_mean'] = married_df['Happiness'].rolling(window=win_size).mean()\nunmarried_df['Happiness_mean'] = unmarried_df['Happiness'].rolling(window=win_size).mean()\n# Merge back together\ncombined_df = pd.concat([mean_hap_df, married_df, unmarried_df], ignore_index=True)\n# display(combined_df.tail())\n\nax = pw.Brick(figsize=(3.75,2.4));\nax.set_xlim(0, 70);\n# ax.set_ylim(-1.2, 1.2);\ncbg_palette = ['black', cb_palette[0], '#b2b2b2ff']\n\n# And plot\nsns.lineplot(\n  x='Age', y='Happiness_mean',\n  hue='m_label',\n  # marker=\".\",\n  # s=20,\n  data=combined_df,\n  palette=cbg_palette,\n  # color=cbg_palette[0],\n  # order=3,\n  ax=ax\n);\nax.set_title(\"Mean Happiness by Status\");\nax.legend_.set_title(\"\");\nax.set_xlabel(\"Mean Happiness\");\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\n\nFig¬†4. Happiness by Age, 70 birth cohorts of size 41 each(DC minimum marriage age = 18)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶What‚Äôs happening here? \\(\\textsf{Happy} \\rightarrow {}^{ü§î}\\textsf{Marriage}^{ü§î} \\leftarrow \\textsf{Age}\\)",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/index.html#references",
    "href": "w11/index.html#references",
    "title": "Week 11: Sensitivity Analysis",
    "section": "References",
    "text": "References\n\n\nBarron, Alexander T. J., Jenny Huang, Rebecca L. Spang, and Simon DeDeo. 2018. ‚ÄúIndividuals, Institutions, and Innovation in the Debates of the French Revolution.‚Äù Proceedings of the National Academy of Sciences 115 (18): 4607‚Äì12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBlaydes, Lisa, Justin Grimmer, and Alison McQueen. 2018. ‚ÄúMirrors for Princes and Sultans: Advice on the Art of Governance in the Medieval Christian and Islamic Worlds.‚Äù The Journal of Politics 80 (4): 1150‚Äì67. https://doi.org/10.1086/699246.\n\n\nBlei, David M. 2012. ‚ÄúProbabilistic Topic Models.‚Äù Commun. ACM 55 (4): 77‚Äì84. https://doi.org/10.1145/2133806.2133826.",
    "crumbs": [
      "Week 11: {{< var w11.date-md >}}"
    ]
  },
  {
    "objectID": "w11/slides.html#topic-models",
    "href": "w11/slides.html#topic-models",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Topic Models",
    "text": "Topic Models\n\nIntuition is just: let‚Äôs model latent topics ‚Äúunderlying‚Äù observed words\n\n\n\n\nSection\nKeywords\n\n\n\n\nU.S. News\nstate, court, federal, republican\n\n\nWorld News\ngovernment, country, officials, minister\n\n\nArts\nmusic, show, art, dance\n\n\nSports\ngame, league, team, coach\n\n\nReal Estate\nhome, bedrooms, bathrooms, building\n\n\n\n\nAlready, by just classifying articles based on these keyword counts:\n\n\n\n\n\nArts\nReal Estate\nSports\nU.S. News\nWorld News\n\n\n\n\nCorrect\n3020\n690\n4860\n1330\n1730\n\n\nIncorrect\n750\n60\n370\n1100\n590\n\n\nAccuracy\n0.801\n0.920\n0.929\n0.547\n0.746"
  },
  {
    "objectID": "w11/slides.html#topic-models-as-pgms",
    "href": "w11/slides.html#topic-models-as-pgms",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Topic Models as PGMs",
    "text": "Topic Models as PGMs\n\nFrom Blei (2012)\n‚Ä¶Unlocks a world of social modeling through text!"
  },
  {
    "objectID": "w11/slides.html#cross-sectional-analysis",
    "href": "w11/slides.html#cross-sectional-analysis",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Cross-Sectional Analysis",
    "text": "Cross-Sectional Analysis\nBlaydes, Grimmer, and McQueen (2018)"
  },
  {
    "objectID": "w11/slides.html#influence-over-time",
    "href": "w11/slides.html#influence-over-time",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Influence Over Time",
    "text": "Influence Over Time\n\nFrom Barron et al. (2018)"
  },
  {
    "objectID": "w11/slides.html#textual-influence-over-time",
    "href": "w11/slides.html#textual-influence-over-time",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Textual Influence Over Time",
    "text": "Textual Influence Over Time"
  },
  {
    "objectID": "w11/slides.html#the-beta-distribution-textbetaalpha-beta-the-workhorse-prior",
    "href": "w11/slides.html#the-beta-distribution-textbetaalpha-beta-the-workhorse-prior",
    "title": "Week 11: Sensitivity Analysis",
    "section": "The Beta Distribution \\(\\text{Beta}(\\alpha, \\beta)\\): The ‚ÄúWorkhorse‚Äù Prior!",
    "text": "The Beta Distribution \\(\\text{Beta}(\\alpha, \\beta)\\): The ‚ÄúWorkhorse‚Äù Prior!\n\nBiased coin framing: \\(\\alpha\\) is ‚Äúpseudo-count‚Äù of heads, \\(\\beta\\) = ‚Äúpseudo-count‚Äù of tails\n\n\n\n\n \\(p \\sim\\) ‚Äú\\(\\text{Beta}(0, 0)\\)‚Äù\n\n\n\nCode\n# b00_df = pd.DataFrame({'p_heads': np.arange(0, 1+1/25, 1/25)})\n# b00_df['Probability'] = 0\nax = pw.Brick(figsize=(2, 1.25))\nsns.histplot(\n  x=[], ax=ax, # data=b00_df\n);\nax.set_xlabel('p_heads');\nax.set_ylabel('Probability')\nax.set_xticks([0, 0.25, 0.5, 0.75, 1]);\nax.set_ylim(0, 1);\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\n \\(p \\sim\\) ‚Äú\\(\\text{Beta}(1, 0)\\)‚Äù\n\n\n\nCode\nb10_df = pd.DataFrame({\n  'draw': [0],\n  'p_heads': [0],\n})\nax = pw.Brick(figsize=(2, 1.25))\nsns.histplot(\n  x='p_heads', stat='probability', ax=ax,\n  bins=25,\n  data=b10_df\n);\nax.set_xlim(0, 1);\nax.set_ylim(0, 1);\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\n \\(p \\sim\\) ‚Äú\\(\\text{Beta}(0, 1)\\)‚Äù\n\n\n\nCode\nb01_df = pd.DataFrame({\n  'draw': [0],\n  'p_heads': [1],\n})\nax = pw.Brick(figsize=(2, 1.25))\nsns.histplot(\n  x='p_heads', stat='probability', ax=ax,\n  bins=25,\n  data=b01_df\n);\nax.set_xlim(0, 1);\nax.set_ylim(0, 1);\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\n\n\n Informative Prior\n\n\n\nCode\n# Informative Prior\nwith pm.Model() as informative_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=2, beta=2)\n  result = pm.Bernoulli(\"result\", p=p_heads)\ninformative_df = draw_prior_sample(informative_model)\ninformative_plot = gen_dist_plot(informative_df, \"Beta(2, 2) Prior on p\");\ninformative_plot.savefig()\n\n\n\n\n\n\n\n\nFig¬†1. ‚ÄúI‚Äôll start by assuming a fair coin‚Äù\n\n\n\n\n\n\n\n Weak Prior\n\n\n\nCode\n# Weakly Informative\nwith pm.Model() as weak_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=1, beta=1)\n  result = pm.Bernoulli(\"result\", p=p_heads)\nweak_df = draw_prior_sample(weak_model)\nweak_plot = gen_dist_plot(weak_df, \"Beta(1, 1) Prior on p\");\nweak_plot.savefig()\n\n\n\n\n\n\n\n\nFig¬†2. ‚ÄúI have no idea, I‚Äôll assume all biases equally likely‚Äù\n\n\n\n\n\n\n\n Skeptical (Jeffreys) Prior\n\n\n\nCode\n# Skeptical / Jeffreys\nwith pm.Model() as skeptical_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=0.5, beta=0.5)\n  result = pm.Bernoulli(\"result\", p=p_heads)\nskeptical_df = draw_prior_sample(skeptical_model)\nskeptical_plot = gen_dist_plot(skeptical_df, \"Beta(0.5, 0.5) Prior on p\");\n# And combine\nskeptical_plot.savefig()\n\n\n\n\n\n\n\n\nFig¬†3. ‚ÄúI don‚Äôt trust this coin, it‚Äôll take lots of flips with H/T balance to convince me it‚Äôs fair!‚Äù"
  },
  {
    "objectID": "w11/slides.html#modeling-domain-knowledge-with-priors",
    "href": "w11/slides.html#modeling-domain-knowledge-with-priors",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Modeling Domain Knowledge with Priors",
    "text": "Modeling Domain Knowledge with Priors\nPopulation proportion framing: \\(\\frac{\\alpha}{\\alpha + \\beta}\\) = mean, \\(\\alpha + \\beta\\) = ‚Äúprecision‚Äù\n\n\n\n\nCode\nwith pm.Model() as heads_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=2, beta=1)\n  result = pm.Bernoulli(\"result\", p=p_heads)\nheads_df = draw_prior_sample(heads_model)\nheads_plot = gen_dist_plot(heads_df, \"Beta(2, 1) Prior on p\");\nheads_plot.savefig()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith pm.Model() as tails_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=1, beta=2)\n  result = pm.Bernoulli(\"result\", p=p_heads)\ntails_df = draw_prior_sample(tails_model)\ntails_plot = gen_dist_plot(tails_df, \"Beta(1, 2) Prior on p\");\ntails_plot.savefig()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith pm.Model() as beta23_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=2, beta=3)\n  result = pm.Bernoulli(\"result\", p=p_heads)\nbeta23_df = draw_prior_sample(beta23_model)\nbeta23_plot = gen_dist_plot(beta23_df, \"Beta(2, 3) Prior on p\");\nbeta23_plot.savefig()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith pm.Model() as beta100_model:\n  p_heads = pm.Beta(\"p_heads\", alpha=40, beta=60)\n  result = pm.Bernoulli(\"result\", p=p_heads)\nbeta100_df = draw_prior_sample(beta100_model)\nbeta100_plot = gen_dist_plot(beta100_df, \"Beta(40, 60) Prior on p\");\nbeta100_plot.savefig()"
  },
  {
    "objectID": "w11/slides.html#does-aging-cause-sadness",
    "href": "w11/slides.html#does-aging-cause-sadness",
    "title": "Week 11: Sensitivity Analysis",
    "section": "Does Aging Cause Sadness?",
    "text": "Does Aging Cause Sadness?\n\n\n\n\nEach year, 20 people are born with uniformly-distributed happiness values\n\n\n\nEach year, each person ages one year; Happiness does not change\n\n\n\nAt age 18, individuals can become married; Odds of marriage each year proportional to individual's happiness; Once married, they remain married\n\n\n\nAt age 70 individuals leave the sample (They move to Boca Raton, Florida)\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nrng = np.random.default_rng(seed=5650)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncb_palette = ['#e69f00','#56b4e9','#009e73']\nsns.set_palette(cb_palette)\nimport patchworklib as pw\nfrom scipy.special import expit\n\n# The original R code:\n# sim_happiness &lt;- function( seed=1977 , N_years=1000 , max_age=65 , N_births=20 , aom=18 ) {\n#     set.seed(seed)\n#     H &lt;- M &lt;- A &lt;- c()\n#     for ( t in 1:N_years ) {\n#         A &lt;- A + 1 # age existing individuals\n#         A &lt;- c( A , rep(1,N_births) ) # newborns\n#         H &lt;- c( H , seq(from=-2,to=2,length.out=N_births) ) # sim happiness trait - never changes\n#         M &lt;- c( M , rep(0,N_births) ) # not yet married\n#         # for each person over 17, chance get married\n#         for ( i in 1:length(A) ) {\n#             if ( A[i] &gt;= aom & M[i]==0 ) {\n#                 M[i] &lt;- rbern(1,inv_logit(H[i]-4))\n#             }\n#         }\n#         # mortality\n#         deaths &lt;- which( A &gt; max_age )\n#         if ( length(deaths)&gt;0 ) {\n#             A &lt;- A[ -deaths ]\n#             H &lt;- H[ -deaths ]\n#             M &lt;- M[ -deaths ]\n#        }\n#     }\n#     d &lt;- data.frame(age=A,married=M,happiness=H)\n#     return(d)\n\n# DGP: happiness -&gt; marriage &lt;- age\nyears = 70\nnum_births = 41\ncolnames = ['age','a','h','m']\nsim_dfs = []\nA = np.zeros(shape=(num_births,1))\nH = np.linspace(-2, 2, num=num_births)\nM = np.zeros(shape=(num_births,1))\ndef update_m(row):\n  if row['m'] == 0:\n    return int(rng.binomial(\n      n=1,\n      p=expit(row['h'] - 3.875),\n      size=1,\n    ))\n  return 1\ndef sim_cohort_to(max_age):\n  sim_df = pd.DataFrame({\n      'age': [1 for _ in range(num_births)],\n      'h': np.linspace(-2, 2, num=num_births),\n      'm': [0 for _ in range(num_births)],\n    }\n  )\n  for t in range(2, max_age + 1):\n    sim_df['age'] = sim_df['age'] + 1\n    if t &gt;= 18:\n      sim_df['m'] = sim_df.apply(update_m, axis=1)\n  return sim_df\nall_sim_dfs = []\nfor cur_max_age in range(1, 71):\n  cur_sim_df = sim_cohort_to(cur_max_age)\n  all_sim_dfs.append(cur_sim_df)\nfull_sim_df = pd.concat(all_sim_dfs)\n\n# full_sim_df.head()\ncbg_palette = ['#c6c6c666'] + cb_palette\nfull_sim_df['m_label'] = full_sim_df['m'].apply(lambda x: \"Unmarried\" if x == 0 else \"Married\")\nfull_sim_df = full_sim_df.rename(columns={'age': 'Age', 'h': 'Happiness'})\nax = pw.Brick(figsize=(5.25,2.75));\nsns.scatterplot(\n  x='Age', y='Happiness', hue='m_label',\n  data=full_sim_df,\n  ax=ax,\n  palette=cbg_palette,\n  s=22,\n  legend=True,\n);\nax.move_legend(\"upper center\", bbox_to_anchor=(0.5, 1.15), ncol=2);\nax.legend_.set_title(\"\");\nax.axvline(x=17.5, color='black', ls='dashed', lw=1);\nax.savefig()\nmean_hap_df = full_sim_df.groupby('Age')['Happiness'].mean().reset_index()\nmean_hap_df['m_label'] = \"All\"\nmean_hap_df['Happiness_mean'] = 0\ngroup_hap_df = full_sim_df[full_sim_df['Age'] &gt;= 18].groupby(['Age','m_label'])['Happiness'].mean().reset_index()\nmarried_df = group_hap_df[group_hap_df['m_label'] == \"Married\"].copy()\nunmarried_df = group_hap_df[group_hap_df['m_label'] == \"Unmarried\"].copy()\n# Moving averages\nwin_size = 3\nmarried_df['Happiness_mean'] = married_df['Happiness'].rolling(window=win_size).mean()\nunmarried_df['Happiness_mean'] = unmarried_df['Happiness'].rolling(window=win_size).mean()\n# Merge back together\ncombined_df = pd.concat([mean_hap_df, married_df, unmarried_df], ignore_index=True)\n# display(combined_df.tail())\n\nax = pw.Brick(figsize=(3.75,2.4));\nax.set_xlim(0, 70);\n# ax.set_ylim(-1.2, 1.2);\ncbg_palette = ['black', cb_palette[0], '#b2b2b2ff']\n\n# And plot\nsns.lineplot(\n  x='Age', y='Happiness_mean',\n  hue='m_label',\n  # marker=\".\",\n  # s=20,\n  data=combined_df,\n  palette=cbg_palette,\n  # color=cbg_palette[0],\n  # order=3,\n  ax=ax\n);\nax.set_title(\"Mean Happiness by Status\");\nax.legend_.set_title(\"\");\nax.set_xlabel(\"Mean Happiness\");\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\n\nFig¬†4. Happiness by Age, 70 birth cohorts of size 41 each(DC minimum marriage age = 18)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶What‚Äôs happening here? \\(\\textsf{Happy} \\rightarrow {}^{ü§î}\\textsf{Marriage}^{ü§î} \\leftarrow \\textsf{Age}\\)"
  },
  {
    "objectID": "w11/slides.html#references",
    "href": "w11/slides.html#references",
    "title": "Week 11: Sensitivity Analysis",
    "section": "References",
    "text": "References\n\n\nBarron, Alexander T. J., Jenny Huang, Rebecca L. Spang, and Simon DeDeo. 2018. ‚ÄúIndividuals, Institutions, and Innovation in the Debates of the French Revolution.‚Äù Proceedings of the National Academy of Sciences 115 (18): 4607‚Äì12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBlaydes, Lisa, Justin Grimmer, and Alison McQueen. 2018. ‚ÄúMirrors for Princes and Sultans: Advice on the Art of Governance in the Medieval Christian and Islamic Worlds.‚Äù The Journal of Politics 80 (4): 1150‚Äì67. https://doi.org/10.1086/699246.\n\n\nBlei, David M. 2012. ‚ÄúProbabilistic Topic Models.‚Äù Commun. ACM 55 (4): 77‚Äì84. https://doi.org/10.1145/2133806.2133826."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5650: Causal Inference for Computational Social Science",
    "section": "",
    "text": "Welcome to the homepage for DSAN 5650: Causal Inference for Computational Social Science at Georgetown University, for the Summer 2025 session!\nThe course meets on Wednesdays from 6:30pm to 9:00pm online, via the Zoom Link provided in the sidebar.\nCheck out the syllabus (or any other link in the sidebar) for more info! Or, use the following links to view notes for individual weeks:\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1: Science from Particles to People\n\n\nMay 21\n\n\n\n\nWeek 2: Probabilistic Graphical Models (PGMs)\n\n\nMay 28\n\n\n\n\nWeek 3: From PGMs to Causal Diagrams\n\n\nJune 4\n\n\n\n\nWeek 4: Clearing the Path from Cause to Effect\n\n\nJune 11\n\n\n\n\nWeek 5: Multilevel Madness, Closing Backdoor Paths\n\n\nJune 18\n\n\n\n\nWeek 6: Bayesian Workflow, Midterm Pre-Review\n\n\nJune 25\n\n\n\n\nWeek 7: Midterm Introduction\n\n\nJuly 2\n\n\n\n\nWeek 8: Propensity Score Weighting\n\n\nJuly 9\n\n\n\n\nWeek 9: Doubly-Robust Estimation and Instrumental Variables\n\n\nJuly 16\n\n\n\n\nWeek 10: Text-as-Data\n\n\nJuly 23\n\n\n\n\nWeek 11: Sensitivity Analysis\n\n\nJuly 30\n\n\n\n\nWeek 12: Causal Forests for Heterogeneous Treatment Effects\n\n\nAugust 6\n\n\n\n\n\nNo matching items\n\n\nCourse Description:\nThis course provides students with the opportunity to take the analytical skills, machine learning algorithms, and statistical methods learned throughout their first year in the program and explore how they can be employed towards carrying out rigorous, original research in the behavioral and social sciences. With a particular emphasis on tackling the additional challenges which arise when moving from associational to causal inference, particularly when only observational (as opposed to experimental) data is available, students will become proficient in cutting-edge causal Machine Learning techniques such as propensity score matching, synthetic controls, causal program evaluation, inverse social welfare function estimation from panel data, and Double-Debiased Machine Learning.\nIn-class examples will cover continuous, discrete-choice, and textual data from a wide swath of social and behavioral sciences: economics, political science, sociology, anthropology, quantitative history, and digital humanities. After gaining experience through in-class labs and homework assignments focused on reproducing key findings from recent journal articles in each of these disciplines, students will spend the final weeks of the course on a final project demonstrating their ability to develop, evaluate, and test the robustness of a causal hypothesis.\nPrerequisites: DSAN 5000, DSAN 5100 (DSAN 5300 recommended but not required)",
    "crumbs": [
      "<i class='bi bi-house pe-1'></i> Home"
    ]
  },
  {
    "objectID": "w09/index.html",
    "href": "w09/index.html",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#doubly-robust-estimation",
    "href": "w09/index.html#doubly-robust-estimation",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Doubly-Robust Estimation",
    "text": "Doubly-Robust Estimation\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport patchworklib as pw;\n\nimport statsmodels.formula.api as smf\nfrom causalml.match import create_table_one\nfrom joblib import Parallel, delayed\nfrom sklearn.linear_model import LogisticRegression\n\ndef generate_data(N=300, seed=1):\n  np.random.seed(seed)\n  \n  # Control variables\n  male = np.random.binomial(1, 0.45, N)\n  age = np.rint(18 + np.random.beta(2, 2, N)*50)\n  hours = np.minimum(np.round(np.random.lognormal(5, 1.3, N), 1), 2000)\n  \n  # Treatment\n  pr = np.maximum(0, np.minimum(1, 0.8 + 0.3*male - np.sqrt(age-18)/10))\n  dark_mode = np.random.binomial(1, pr, N)==1\n  \n  # Outcome\n  read_time = np.round(np.random.normal(10 - 4*male + 2*np.log(hours) + 2*dark_mode, 4, N), 1)\n\n  # Generate the dataframe\n  df = pd.DataFrame({'read_time': read_time, 'dark_mode': dark_mode, 'male': male, 'age': age, 'hours': hours})\n\n  return df\n\nuser_df = generate_data(N=300)\nols_model = smf.ols(\"read_time ~ dark_mode\", data=user_df).fit()\nols_summary = ols_model.summary()\nresults_as_html = ols_summary.tables[1].as_html()\nols_summary_df = pd.read_html(results_as_html, header=0, index_col=0)[0]\nols_summary_df[['coef','std err']]\n#type(ols_summary.tables[1])[['coef','std err']]\n# .tables[1]\n# ols_df[['coef','std err']]\n\n\n\n\n\n\n\nSuper cool example courtesy of Matteo Courthoud!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\n\n\n\n\nIntercept\n19.1748\n0.402\n\n\ndark_mode[T.True]\n-0.4446\n0.571\n\n\n\n\n\n\n\n&lt;Figure size 96x96 with 0 Axes&gt;\n\n\n\n\n\n\n‚Ä¶So, is this a causal effect? Does dark theme cause users to spend less time reading?",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#unit-of-observation-article-reader",
    "href": "w09/index.html#unit-of-observation-article-reader",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Unit of Observation: (Article, Reader)",
    "text": "Unit of Observation: (Article, Reader)\n(‚Ä¶since I couldn‚Äôt figure out how to fit it on the last slide)\n\n\nCode\nuser_df.head()\n\n\n\n\n\n\n\n\n\nread_time\ndark_mode\nmale\nage\nhours\n\n\n\n\n0\n14.4\nFalse\n0\n43.0\n65.6\n\n\n1\n15.4\nFalse\n1\n55.0\n125.4\n\n\n2\n20.9\nTrue\n0\n23.0\n642.6\n\n\n3\n20.0\nFalse\n0\n41.0\n129.1\n\n\n4\n21.5\nTrue\n0\n29.0\n190.2",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#what-does-the-data-look-like",
    "href": "w09/index.html#what-does-the-data-look-like",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "What Does the Data Look Like?",
    "text": "What Does the Data Look Like?\n\n\nCode\n#ax = pw.Brick(figsize=(8,5))\nsns.pairplot(\n  data=user_df, aspect=0.8\n)",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#balance",
    "href": "w09/index.html#balance",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Balance",
    "text": "Balance\n\n\nEnter Uber‚Äôs causal inference library: causalml\n\n\nCode\nfrom IPython.display import display, HTML\nX = ['male', 'age', 'hours']\ntable1 = create_table_one(user_df, 'dark_mode', X)\nuser_df.to_csv(\"assets/user_df.csv\")\ntable1.to_csv(\"assets/table1.csv\")\nHTML(table1.to_html())\n\n\n\n\n\n\nControl\nTreatment\nSMD\n\n\nVariable\n\n\n\n\n\n\n\nn\n151\n149\n\n\n\nage\n46.01 (9.79)\n39.09 (11.53)\n-0.6469\n\n\nhours\n337.78 (464.00)\n328.57 (442.12)\n-0.0203\n\n\nmale\n0.34 (0.47)\n0.66 (0.48)\n0.6732\n\n\n\n\n\n\nAnd then WeightIt to generate a ‚Äúlove plot‚Äù:",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#augmented-inverse-propensity-weighting-aipw",
    "href": "w09/index.html#augmented-inverse-propensity-weighting-aipw",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Augmented Inverse Propensity Weighting (AIPW)",
    "text": "Augmented Inverse Propensity Weighting (AIPW)\n\\[\n\\begin{align*}\n\\hat \\tau_{AIPW} &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\text{RegEst}(X_i) + \\text{PropensityAdj}(X_i, Y_i) \\right) \\\\\n\\text{RegEst}(X_i) &= \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) \\\\\n\\text{PropensityAdj}(X_i, Y_i) &= \\frac{D_i}{\\hat{\\mathtt{e}}(X_i)} \\left( Y_i - \\hat \\mu^{(1)}(X_i) \\right) - \\frac{(1-D_i) }{1-\\hat{\\mathtt{e}}(X_i)} \\left( Y_i - \\hat \\mu^{(0)}(X_i) \\right)\n\\end{align*}\n\\]\nwhere \\(\\mu^{(d)}(x)\\) is the response function, i.e.¬†the expected value of the outcome, conditional on observable characteristics \\(x\\) and treatment status \\(d\\), and \\(e(X)\\) is the propensity score.\n\\[\n\\begin{align*}\n\\mu^{(d)}(x) &= \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, D_i = d \\right] \\\\\n\\mathtt{e}(x) &= \\mathbb E \\left[ D_i = 1 \\ \\big | \\ X_i = x \\right]\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#model-1-propensity-score",
    "href": "w09/index.html#model-1-propensity-score",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Model 1: Propensity Score",
    "text": "Model 1: Propensity Score\n\n\nCode\ndef estimate_e(df, X, D, model_e):\n    e = model_e.fit(df[X], df[D]).predict_proba(df[X])[:,1]\n    return e\nuser_df['e'] = estimate_e(user_df, X, \"dark_mode\", LogisticRegression())\nax = pw.Brick(figsize=(7, 2.75));\nsns.kdeplot(\n  x='e', hue='dark_mode', data=user_df,\n  # bins=30,\n  #stat='density',\n  common_norm=False,\n  fill=True,\n  ax=ax\n);\nax.set_xlabel(\"$e(X)$\");\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\nCode\nw = 1 / (user_df['e'] * user_df[\"dark_mode\"] + (1-user_df['e']) * (1-user_df[\"dark_mode\"]))\nsmf.wls(\"read_time ~ dark_mode\", weights=w, data=user_df).fit().summary().tables[1]\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n18.5871\n0.412\n45.158\n0.000\n17.777\n19.397\n\n\ndark_mode[T.True]\n1.0486\n0.581\n1.805\n0.072\n-0.095\n2.192",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#model-2-regression-with-controls",
    "href": "w09/index.html#model-2-regression-with-controls",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Model 2: Regression with Controls",
    "text": "Model 2: Regression with Controls\n\nFirst, with scikit-learn:\n\n\n\nCode\ndef estimate_mu(df, X, D, y, model_mu):\n    mu = model_mu.fit(df[X + [D]], df[y])\n    mu0 = mu.predict(df[X + [D]].assign(dark_mode=0))\n    mu1 = mu.predict(df[X + [D]].assign(dark_mode=1))\n    return mu0, mu1\nfrom sklearn.linear_model import LinearRegression\n\nmu0, mu1 = estimate_mu(user_df, X, \"dark_mode\", \"read_time\", LinearRegression())\nprint(np.mean(mu0), np.mean(mu1))\nprint(np.mean(mu1-mu0))\n\n\n18.265714409803312 19.651524322951012\n1.3858099131476969\n\n\n\nEnter EconML, Microsoft‚Äôs ‚ÄúOfficial‚Äù ML-based econometrics library üòé\n\n\n\nCode\nfrom econml.dr import LinearDRLearner\n\nmodel = LinearDRLearner(\n  model_propensity=LogisticRegression(),\n  model_regression=LinearRegression(),\n  random_state=5650\n)\nmodel.fit(Y=user_df[\"read_time\"], T=user_df[\"dark_mode\"], X=user_df[X]);\nmodel.ate_inference(X=user_df[X].values, T0=0, T1=1).summary().tables[0]\n\n\n\nUncertainty of Mean Point Estimate\n\n\nmean_point\nstderr_mean\nzstat\npvalue\nci_mean_lower\nci_mean_upper\n\n\n1.321\n0.549\n2.409\n0.016\n0.246\n2.396",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#double-robustness-to-the-rescue",
    "href": "w09/index.html#double-robustness-to-the-rescue",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Double-Robustness to the Rescue!",
    "text": "Double-Robustness to the Rescue!\n\n\nWrong regression model:\n\n\nCode\ndef compare_estimators(X_e, X_mu, D, y, seed):\n    df = generate_data(seed=seed)\n    e = estimate_e(df, X_e, D, LogisticRegression())\n    mu0, mu1 = estimate_mu(df, X_mu, D, y, LinearRegression())\n    slearn = mu1 - mu0\n    ipw = (df[D] / e - (1-df[D]) / (1-e)) * df[y]\n    aipw = slearn + df[D] / e * (df[y] - mu1) - (1-df[D]) / (1-e) * (df[y] - mu0)\n    return np.mean((slearn, ipw, aipw), axis=1)\n\ndef simulate_estimators(X_e, X_mu, D, y):\n    r = Parallel(n_jobs=8)(delayed(compare_estimators)(X_e, X_mu, D, y, i) for i in range(100))\n    df_tau = pd.DataFrame(r, columns=['S-learn', 'IPW', 'AIPW'])\n    return df_tau\n# The actual plots\nax = pw.Brick(figsize=(4, 3.5))\nwrong_reg_df = simulate_estimators(\n  X_e=['male', 'age'], X_mu=['hours'], D=\"dark_mode\", y=\"read_time\"\n)\nwrong_reg_plot = sns.boxplot(\n  data=pd.melt(wrong_reg_df), x='variable', y='value', hue='variable',\n  ax=ax,\n  linewidth=2\n);\nwrong_reg_plot.set(\n  title=\"Distribution of $\\hat œÑ$\", xlabel='', ylabel=''\n);\nax.axhline(2, c='r', ls=':');\nax.savefig()\n\n\n\n\n\n\n\n\n\n\nWrong propensity score model:\n\n\nCode\nax = pw.Brick(figsize=(4, 3.5))\nwrong_ps_df = simulate_estimators(\n  ['age'], ['male', 'hours'], D=\"dark_mode\", y=\"read_time\"\n)\nwrong_ps_plot = sns.boxplot(\n  data=pd.melt(wrong_ps_df), x='variable', y='value', hue='variable',\n  ax=ax,\n  linewidth=2\n);\nax.set_title(\"Distribution of $\\hat œÑ$\");\nax.axhline(2, c='r', ls=':');\nax.savefig()",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#instrumental-variable-estimation",
    "href": "w09/index.html#instrumental-variable-estimation",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Instrumental Variable Estimation",
    "text": "Instrumental Variable Estimation",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/index.html#references",
    "href": "w09/index.html#references",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Week 9: {{< var w09.date-md >}}"
    ]
  },
  {
    "objectID": "w09/slides.html#doubly-robust-estimation",
    "href": "w09/slides.html#doubly-robust-estimation",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Doubly-Robust Estimation",
    "text": "Doubly-Robust Estimation\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport patchworklib as pw;\n\nimport statsmodels.formula.api as smf\nfrom causalml.match import create_table_one\nfrom joblib import Parallel, delayed\nfrom sklearn.linear_model import LogisticRegression\n\ndef generate_data(N=300, seed=1):\n  np.random.seed(seed)\n  \n  # Control variables\n  male = np.random.binomial(1, 0.45, N)\n  age = np.rint(18 + np.random.beta(2, 2, N)*50)\n  hours = np.minimum(np.round(np.random.lognormal(5, 1.3, N), 1), 2000)\n  \n  # Treatment\n  pr = np.maximum(0, np.minimum(1, 0.8 + 0.3*male - np.sqrt(age-18)/10))\n  dark_mode = np.random.binomial(1, pr, N)==1\n  \n  # Outcome\n  read_time = np.round(np.random.normal(10 - 4*male + 2*np.log(hours) + 2*dark_mode, 4, N), 1)\n\n  # Generate the dataframe\n  df = pd.DataFrame({'read_time': read_time, 'dark_mode': dark_mode, 'male': male, 'age': age, 'hours': hours})\n\n  return df\n\nuser_df = generate_data(N=300)\nols_model = smf.ols(\"read_time ~ dark_mode\", data=user_df).fit()\nols_summary = ols_model.summary()\nresults_as_html = ols_summary.tables[1].as_html()\nols_summary_df = pd.read_html(results_as_html, header=0, index_col=0)[0]\nols_summary_df[['coef','std err']]\n#type(ols_summary.tables[1])[['coef','std err']]\n# .tables[1]\n# ols_df[['coef','std err']]\n\n\n\n\n\n\n\nSuper cool example courtesy of Matteo Courthoud!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\n\n\n\n\nIntercept\n19.1748\n0.402\n\n\ndark_mode[T.True]\n-0.4446\n0.571\n\n\n\n\n\n\n\n&lt;Figure size 96x96 with 0 Axes&gt;\n\n\n\n\n\n\n‚Ä¶So, is this a causal effect? Does dark theme cause users to spend less time reading?"
  },
  {
    "objectID": "w09/slides.html#unit-of-observation-article-reader",
    "href": "w09/slides.html#unit-of-observation-article-reader",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Unit of Observation: (Article, Reader)",
    "text": "Unit of Observation: (Article, Reader)\n(‚Ä¶since I couldn‚Äôt figure out how to fit it on the last slide)\n\n\nCode\nuser_df.head()\n\n\n\n\n\n\n\n\n\nread_time\ndark_mode\nmale\nage\nhours\n\n\n\n\n0\n14.4\nFalse\n0\n43.0\n65.6\n\n\n1\n15.4\nFalse\n1\n55.0\n125.4\n\n\n2\n20.9\nTrue\n0\n23.0\n642.6\n\n\n3\n20.0\nFalse\n0\n41.0\n129.1\n\n\n4\n21.5\nTrue\n0\n29.0\n190.2"
  },
  {
    "objectID": "w09/slides.html#what-does-the-data-look-like",
    "href": "w09/slides.html#what-does-the-data-look-like",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "What Does the Data Look Like?",
    "text": "What Does the Data Look Like?\n\n\nCode\n#ax = pw.Brick(figsize=(8,5))\nsns.pairplot(\n  data=user_df, aspect=0.8\n)"
  },
  {
    "objectID": "w09/slides.html#balance",
    "href": "w09/slides.html#balance",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Balance",
    "text": "Balance\n\n\nEnter Uber‚Äôs causal inference library: causalml\n\n\nCode\nfrom IPython.display import display, HTML\nX = ['male', 'age', 'hours']\ntable1 = create_table_one(user_df, 'dark_mode', X)\nuser_df.to_csv(\"assets/user_df.csv\")\ntable1.to_csv(\"assets/table1.csv\")\nHTML(table1.to_html())\n\n\n\n\n\n\nControl\nTreatment\nSMD\n\n\nVariable\n\n\n\n\n\n\n\nn\n151\n149\n\n\n\nage\n46.01 (9.79)\n39.09 (11.53)\n-0.6469\n\n\nhours\n337.78 (464.00)\n328.57 (442.12)\n-0.0203\n\n\nmale\n0.34 (0.47)\n0.66 (0.48)\n0.6732\n\n\n\n\n\n\nAnd then WeightIt to generate a ‚Äúlove plot‚Äù:"
  },
  {
    "objectID": "w09/slides.html#augmented-inverse-propensity-weighting-aipw",
    "href": "w09/slides.html#augmented-inverse-propensity-weighting-aipw",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Augmented Inverse Propensity Weighting (AIPW)",
    "text": "Augmented Inverse Propensity Weighting (AIPW)\n\\[\n\\begin{align*}\n\\hat \\tau_{AIPW} &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\text{RegEst}(X_i) + \\text{PropensityAdj}(X_i, Y_i) \\right) \\\\\n\\text{RegEst}(X_i) &= \\hat \\mu^{(1)}(X_i) - \\hat \\mu^{(0)}(X_i) \\\\\n\\text{PropensityAdj}(X_i, Y_i) &= \\frac{D_i}{\\hat{\\mathtt{e}}(X_i)} \\left( Y_i - \\hat \\mu^{(1)}(X_i) \\right) - \\frac{(1-D_i) }{1-\\hat{\\mathtt{e}}(X_i)} \\left( Y_i - \\hat \\mu^{(0)}(X_i) \\right)\n\\end{align*}\n\\]\nwhere \\(\\mu^{(d)}(x)\\) is the response function, i.e.¬†the expected value of the outcome, conditional on observable characteristics \\(x\\) and treatment status \\(d\\), and \\(e(X)\\) is the propensity score.\n\\[\n\\begin{align*}\n\\mu^{(d)}(x) &= \\mathbb E \\left[ Y_i \\ \\big | \\ X_i = x, D_i = d \\right] \\\\\n\\mathtt{e}(x) &= \\mathbb E \\left[ D_i = 1 \\ \\big | \\ X_i = x \\right]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w09/slides.html#model-1-propensity-score",
    "href": "w09/slides.html#model-1-propensity-score",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Model 1: Propensity Score",
    "text": "Model 1: Propensity Score\n\n\nCode\ndef estimate_e(df, X, D, model_e):\n    e = model_e.fit(df[X], df[D]).predict_proba(df[X])[:,1]\n    return e\nuser_df['e'] = estimate_e(user_df, X, \"dark_mode\", LogisticRegression())\nax = pw.Brick(figsize=(7, 2.75));\nsns.kdeplot(\n  x='e', hue='dark_mode', data=user_df,\n  # bins=30,\n  #stat='density',\n  common_norm=False,\n  fill=True,\n  ax=ax\n);\nax.set_xlabel(\"$e(X)$\");\nax.savefig()\n\n\n\n\n\n\n\n\n\n\n\nCode\nw = 1 / (user_df['e'] * user_df[\"dark_mode\"] + (1-user_df['e']) * (1-user_df[\"dark_mode\"]))\nsmf.wls(\"read_time ~ dark_mode\", weights=w, data=user_df).fit().summary().tables[1]\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n18.5871\n0.412\n45.158\n0.000\n17.777\n19.397\n\n\ndark_mode[T.True]\n1.0486\n0.581\n1.805\n0.072\n-0.095\n2.192"
  },
  {
    "objectID": "w09/slides.html#model-2-regression-with-controls",
    "href": "w09/slides.html#model-2-regression-with-controls",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Model 2: Regression with Controls",
    "text": "Model 2: Regression with Controls\n\nFirst, with scikit-learn:\n\n\n\nCode\ndef estimate_mu(df, X, D, y, model_mu):\n    mu = model_mu.fit(df[X + [D]], df[y])\n    mu0 = mu.predict(df[X + [D]].assign(dark_mode=0))\n    mu1 = mu.predict(df[X + [D]].assign(dark_mode=1))\n    return mu0, mu1\nfrom sklearn.linear_model import LinearRegression\n\nmu0, mu1 = estimate_mu(user_df, X, \"dark_mode\", \"read_time\", LinearRegression())\nprint(np.mean(mu0), np.mean(mu1))\nprint(np.mean(mu1-mu0))\n\n\n18.265714409803312 19.651524322951012\n1.3858099131476969\n\n\n\nEnter EconML, Microsoft‚Äôs ‚ÄúOfficial‚Äù ML-based econometrics library üòé\n\n\n\nCode\nfrom econml.dr import LinearDRLearner\n\nmodel = LinearDRLearner(\n  model_propensity=LogisticRegression(),\n  model_regression=LinearRegression(),\n  random_state=5650\n)\nmodel.fit(Y=user_df[\"read_time\"], T=user_df[\"dark_mode\"], X=user_df[X]);\nmodel.ate_inference(X=user_df[X].values, T0=0, T1=1).summary().tables[0]\n\n\n\nUncertainty of Mean Point Estimate\n\n\nmean_point\nstderr_mean\nzstat\npvalue\nci_mean_lower\nci_mean_upper\n\n\n1.321\n0.549\n2.409\n0.016\n0.246\n2.396"
  },
  {
    "objectID": "w09/slides.html#double-robustness-to-the-rescue",
    "href": "w09/slides.html#double-robustness-to-the-rescue",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Double-Robustness to the Rescue!",
    "text": "Double-Robustness to the Rescue!\n\n\nWrong regression model:\n\n\nCode\ndef compare_estimators(X_e, X_mu, D, y, seed):\n    df = generate_data(seed=seed)\n    e = estimate_e(df, X_e, D, LogisticRegression())\n    mu0, mu1 = estimate_mu(df, X_mu, D, y, LinearRegression())\n    slearn = mu1 - mu0\n    ipw = (df[D] / e - (1-df[D]) / (1-e)) * df[y]\n    aipw = slearn + df[D] / e * (df[y] - mu1) - (1-df[D]) / (1-e) * (df[y] - mu0)\n    return np.mean((slearn, ipw, aipw), axis=1)\n\ndef simulate_estimators(X_e, X_mu, D, y):\n    r = Parallel(n_jobs=8)(delayed(compare_estimators)(X_e, X_mu, D, y, i) for i in range(100))\n    df_tau = pd.DataFrame(r, columns=['S-learn', 'IPW', 'AIPW'])\n    return df_tau\n# The actual plots\nax = pw.Brick(figsize=(4, 3.5))\nwrong_reg_df = simulate_estimators(\n  X_e=['male', 'age'], X_mu=['hours'], D=\"dark_mode\", y=\"read_time\"\n)\nwrong_reg_plot = sns.boxplot(\n  data=pd.melt(wrong_reg_df), x='variable', y='value', hue='variable',\n  ax=ax,\n  linewidth=2\n);\nwrong_reg_plot.set(\n  title=\"Distribution of $\\hat œÑ$\", xlabel='', ylabel=''\n);\nax.axhline(2, c='r', ls=':');\nax.savefig()\n\n\n\n\n\n\n\n\n\n\nWrong propensity score model:\n\n\nCode\nax = pw.Brick(figsize=(4, 3.5))\nwrong_ps_df = simulate_estimators(\n  ['age'], ['male', 'hours'], D=\"dark_mode\", y=\"read_time\"\n)\nwrong_ps_plot = sns.boxplot(\n  data=pd.melt(wrong_ps_df), x='variable', y='value', hue='variable',\n  ax=ax,\n  linewidth=2\n);\nax.set_title(\"Distribution of $\\hat œÑ$\");\nax.axhline(2, c='r', ls=':');\nax.savefig()"
  },
  {
    "objectID": "w09/slides.html#instrumental-variable-estimation",
    "href": "w09/slides.html#instrumental-variable-estimation",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "Instrumental Variable Estimation",
    "text": "Instrumental Variable Estimation"
  },
  {
    "objectID": "w09/slides.html#references",
    "href": "w09/slides.html#references",
    "title": "Week 9: Doubly-Robust Estimation and Instrumental Variables",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Science from Particles to People",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#prof.-jeff-introduction",
    "href": "w01/index.html#prof.-jeff-introduction",
    "title": "Week 1: Science from Particles to People",
    "section": "Prof.¬†Jeff Introduction!",
    "text": "Prof.¬†Jeff Introduction!\n\nBorn in NW DC ‚Üí high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Econ",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#the-world-outside-of-dc",
    "href": "w01/index.html#the-world-outside-of-dc",
    "title": "Week 1: Science from Particles to People",
    "section": "The World Outside of DC",
    "text": "The World Outside of DC\n Studied abroad in Beijing (Peking University/ÂåóÂ§ß) ‚Üí internship with Huawei in Hong Kong (HKUST)\n\n\n\n Stanford, MS in Computer Science\n Research Economist, UC Berkeley\n Columbia, PhD in Political Economy",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#why-is-georgetown-having-me-teach-this",
    "href": "w01/index.html#why-is-georgetown-having-me-teach-this",
    "title": "Week 1: Science from Particles to People",
    "section": "Why Is Georgetown Having Me Teach This?",
    "text": "Why Is Georgetown Having Me Teach This?\n\n\n\n\n\n\n\nQuanty things \\(\\leadsto\\) PhD in Political Economy\nPhD exam major: Political Philosophy\nPhD exam minor: International Relations\nPhD exam paper: ‚ÄúHow to Do Things with Translations‚Äù\nGame-changing Research Fellowships at‚Ä¶ (Nothing was the same -drake):\nSanta Fe Institute: dedicated to the multidisciplinary study of complex systems: physical, computational, biological, social\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\n\nFigure¬†1: Years spent questing in dungeons of academia\n\n\n\n\n\n\n\nCentre for the Study of the History of Political Thought, Queen Mary University of London (QMUL): New approaches to the history of political thought [Quentin Skinner, ‚ÄúCambridge School‚Äù] have changed how we study ideas from the past and their relevance to contemporary politics. The focus of the Centre is to explore [ts].",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#dissertation-nlp-x-history",
    "href": "w01/index.html#dissertation-nlp-x-history",
    "title": "Week 1: Science from Particles to People",
    "section": "Dissertation (NLP x History)",
    "text": "Dissertation (NLP x History)\n‚ÄúOur Word is Our Weapon‚Äù: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#ir-part-wars-of-ideas-i-cold-war",
    "href": "w01/index.html#ir-part-wars-of-ideas-i-cold-war",
    "title": "Week 1: Science from Particles to People",
    "section": "(IR Part) Wars of Ideas I: Cold War",
    "text": "(IR Part) Wars of Ideas I: Cold War\n\nCold War arms shipments (SIPRI) vs.¬†propaganda (–ü–µ—á–∞—Ç—å –°–°–°–†): here, to üá™üáπ",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "href": "w01/index.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "title": "Week 1: Science from Particles to People",
    "section": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993",
    "text": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#research-nowadays",
    "href": "w01/index.html#research-nowadays",
    "title": "Week 1: Science from Particles to People",
    "section": "Research Nowadays",
    "text": "Research Nowadays\n\nMost cited paper: ‚ÄúMonopsony in Online Labor Markets‚Äù (Uses Double-Debiased ML for Causal Inference!)\nMost recent paper: ‚ÄúOperationalizing Freedom as Non-Domination in the Labor Market‚Äù (Cambridge U Press)\nRarely cited paper but often-thought-about obsession: ‚ÄúHow To Do Things With Translations‚Äù\nRelated thing you can buy in a bookstore: [Editorial board for new translation of] Capital, Vol. 1 by Karl Marx (Princeton U Press)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#but-now-teaching",
    "href": "w01/index.html#but-now-teaching",
    "title": "Week 1: Science from Particles to People",
    "section": "But Now‚Ä¶ Teaching!",
    "text": "But Now‚Ä¶ Teaching!\n\nGrowth mindset: ‚ÄúI can‚Äôt do this‚Äù \\(\\leadsto\\) ‚ÄúI can‚Äôt do this yet!‚Äù\nThink of anything you‚Äôre able to do‚Ä¶ There was a point in life when you didn‚Äôt know how to do it! What happened? Your brain established and/or rearranged neural pathways as you struggled with it!\nHow does this neural re-arrangement work? One ‚Äúcheatcode‚Äù is spaced repetition:\n\n\n\n\nImage source",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#lil-wayne-on-spaced-repetition",
    "href": "w01/index.html#lil-wayne-on-spaced-repetition",
    "title": "Week 1: Science from Particles to People",
    "section": "Lil Wayne on Spaced Repetition",
    "text": "Lil Wayne on Spaced Repetition\n\n\n\n\n\n\nFigure¬†2: From The Carter (Documentary)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#maria-montessori-on-your-final-project",
    "href": "w01/index.html#maria-montessori-on-your-final-project",
    "title": "Week 1: Science from Particles to People",
    "section": "Maria Montessori on Your Final Project",
    "text": "Maria Montessori on Your Final Project\n\nOur teaching should be governed, not by a desire to make students learn things, but by the endeavor to keep burning within them that light which is called curiosity. (Montessori 1916)1\n\n\nTo this end: your final project is to explore some potential causal linkage from \\(X\\) to \\(Y\\) (more later!)\nThe sole requirement is: sufficient curiosity to serve as fuel for your journey from associational world to causal world",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#coarse-graining-units-of-observation",
    "href": "w01/index.html#coarse-graining-units-of-observation",
    "title": "Week 1: Science from Particles to People",
    "section": "Coarse-Graining Units of Observation",
    "text": "Coarse-Graining Units of Observation\n\n\n\nField\nExample Unit of Observation\n\n\n\n\nPhysics\nParticle\n[Particle = Fine-Graining of Molecules]\n\n\nChemistry\nMolecule = \\(\\cup\\)(Particles)\n[Molecule = Coarse-Graining of Particles]\n\n\nBiology\nCell = \\(\\cup\\)(Molecules)\n\n\nNeuro/Cog Sci\nBrain = \\(\\cup\\)(Cells)\n\n\nHuman Physiology\nBody = Brain \\(\\cup\\) Other Organs\n\n\n‚Üë Science ¬†¬† üßê something happens here‚Ä¶ ü§î ¬†¬† ‚Üì Social Science\n\n\nAnthropology\nHuman-Relational System (e.g., Kinship) = \\(\\cup\\)(Brains) \\(\\times\\) Natural Environment\n\n\nEconomics\nEconomy = Specific relational system of exchange w.r.t. scarce resources\n\n\nPolitical Economy\nEconomy-Context = Economic Exchange \\(\\cap\\) Relational Power\n\n\nSociology\nSociety = \\(\\cup\\)(Relational Systems: Kinship, Friendship, Authority, Power, Violence)\n\n\nHistory\nLongue-Dur√©e = Evolution of Societies over Time and Space",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#homo-sapienshomo-arbitratushomo-mischievous",
    "href": "w01/index.html#homo-sapienshomo-arbitratushomo-mischievous",
    "title": "Week 1: Science from Particles to People",
    "section": "Homo sapiens/Homo arbitratus/Homo mischievous",
    "text": "Homo sapiens/Homo arbitratus/Homo mischievous\n\nLatin sapiens denotes being ‚Äúdiscerning‚Äù or ‚Äúwise‚Äù\n\n\n\n\n\n\n\n\nBut‚Ä¶ technically nothing stops us from choosing to be ‚Äúunwise‚Äù whenever we‚Äôd like‚Ä¶ bc free will\n\\(\\Rightarrow\\) For this class, humans are Homo arbitratus: arbitratus denotes choosing what to do, after we‚Äôve sapiently ‚Äúdiscerned‚Äù/thought about it",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#laws-of-physics-vs.-laws-of-social-science",
    "href": "w01/index.html#laws-of-physics-vs.-laws-of-social-science",
    "title": "Week 1: Science from Particles to People",
    "section": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science",
    "text": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science\n\nIf we tell an inanimate object that we‚Äôve discovered a law saying that it will accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n‚Ä¶It will likely2 still accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n\n\n\n\n\n\n\n\nIf we tell a human we‚Äôve discovered a law saying they will quack like a duck at 7:30pm EDT every Wednesday\n\n‚Ä¶They can utilize their free will to violate this ‚Äúlaw‚Äù",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#strangely-relevant-cs-topic-the-halting-problem",
    "href": "w01/index.html#strangely-relevant-cs-topic-the-halting-problem",
    "title": "Week 1: Science from Particles to People",
    "section": "Strangely-Relevant CS Topic: The Halting Problem",
    "text": "Strangely-Relevant CS Topic: The Halting Problem\n\nKurt G√∂del \\(\\leftrightarrow\\) Alan Turing: Entscheidungsproblem\nTheorem: It is not possible to write a computer program \\(P(x)\\) that detects whether or not a computer program \\(x\\) will eventually halt (as opposed to, e.g., looping forever)\nProof: Assume \\(P(x)\\) is possible to write. Run it on mischievous_program.py. Infinite contradiction loop.\n\n\n\n\n\nhalting_problem_solver.py\n\ndef will_it_halt(program, input):\n  # Put code you think will work here\n  # Return True if program will halt, False otherwise\n\n\n\nmischievous_program.py\n\ndef my_mischievous_function(input):\n  if will_it_halt(my_mischevious_function, input):\n    while True: pass # Infinite loop\n  else:\n    return # Halt\n\n\n\n\n\n\n\n\nInput‚Üí‚ÜìProgram\n0\n1\n2\n\\(\\cdots\\)\n\n\n\n\n0\nHalt\nLoop\nLoop\n\\(\\cdots\\)\n\n\n1\nLoop\nLoop\nHalt\n\\(\\cdots\\)\n\n\n2\nLoop\nLoop\nLoop\n\\(\\cdots\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\mathbf{\\ddots}\\)\n\n\n\nLoop\nHalt\nHalt\n\\(\\mathbf{\\cdots}\\)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#the-takeaway-bayesian-humility",
    "href": "w01/index.html#the-takeaway-bayesian-humility",
    "title": "Week 1: Science from Particles to People",
    "section": "The Takeaway: [Bayesian] Humility!",
    "text": "The Takeaway: [Bayesian] Humility!\n\nSocial science, with ‚Äúscience‚Äù used in the same sense as for physics, may be a quixotic endeavor3\nInstead, we‚Äôll do social science, where we use data to‚Ä¶\n Infer tendencies: \\(\\mathcal{H}\\) = ¬´\\(X\\) tends to cause \\(Y\\)¬ª\n With some degree of veracity: \\(\\Pr(\\mathcal{H}) \\approx 0.7\\)\n Construct models that we can update with new evidence: Bayes‚Äô rule! \\(\\Pr(\\mathcal{H} \\mid E) = \\frac{\\Pr(E \\mid \\mathcal{H} ) \\Pr(\\mathcal{H})}{\\Pr(E)} \\approx 0.8\\)\nPls notice ‚Äúslippage‚Äù between aleatory probability within \\(\\mathcal{H}\\) (‚Äútends to‚Äù) vs.¬†epistemic probability ‚Äúoutside of‚Äù, talking about \\(\\mathcal{H}\\) (‚ÄúI‚Äôm 70% confident about \\(\\mathcal{H}\\)‚Äù)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#the-logic-of-violence-in-civil-war",
    "href": "w01/index.html#the-logic-of-violence-in-civil-war",
    "title": "Week 1: Science from Particles to People",
    "section": "The Logic of Violence in Civil War",
    "text": "The Logic of Violence in Civil War\n\n\n\nKalyvas (2006)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#matching-estimators",
    "href": "w01/index.html#matching-estimators",
    "title": "Week 1: Science from Particles to People",
    "section": "Matching Estimators",
    "text": "Matching Estimators\n\n\n\nImage Source",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#case-study-military-inequality-leadsto-military-success",
    "href": "w01/index.html#case-study-military-inequality-leadsto-military-success",
    "title": "Week 1: Science from Particles to People",
    "section": "Case Study: Military Inequality \\(\\leadsto\\) Military Success",
    "text": "Case Study: Military Inequality \\(\\leadsto\\) Military Success\n\nLyall (2020): ‚ÄúTreating certain ethnic groups as second-class citizens [‚Ä¶] leads victimized soldiers to subvert military authorities once war begins. The higher an army‚Äôs inequality, the greater its rates of desertion, side-switching, and casualties‚Äù\n\n\nMatching constructs pairs of belligerents that are similar across a wide range of traits thought to dictate battlefield performance but that vary in levels of prewar inequality. The more similar the belligerents, the better our estimate of inequality‚Äôs effects, as all other traits are shared and thus cannot explain observed differences in performance, helping assess how battlefield performance would have improved (declined) if the belligerent had a lower (higher) level of prewar inequality.\nSince [non-matched] cases are dropped [‚Ä¶] selected cases are more representative of average belligerents/wars than outliers with few or no matches, [providing] surer ground for testing generalizability of the book‚Äôs claims than focusing solely on canonical but unrepresentative usual suspects (Germany, the United States, Israel)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#does-inequality-cause-poor-military-performance",
    "href": "w01/index.html#does-inequality-cause-poor-military-performance",
    "title": "Week 1: Science from Particles to People",
    "section": "Does Inequality Cause Poor Military Performance?",
    "text": "Does Inequality Cause Poor Military Performance?\n\n\n\n\n\n\n\n\nCovariates\nSultanate of Morocco Spanish-Moroccan War, 1859-60\nKhanate of Kokand War with Russia, 1864-65\n\n\n\n\n\\(X\\): Military Inequality\nLow (0.01)\nExtreme (0.70)\n\n\n\\(\\mathbf{Z}\\): Matched Covariates:\n\n\n\n\nInitial relative power\n66%\n66%\n\n\nTotal fielded force\n55,000\n50,000\n\n\nRegime type\nAbsolutist Monarchy (‚àí6)\nAbsolute Monarchy (‚àí7)\n\n\nDistance from capital\n208km\n265km\n\n\nStanding army\nYes\nYes\n\n\nComposite military\nYes\nYes\n\n\nInitiator\nNo\nNo\n\n\nJoiner\nNo\nNo\n\n\nDemocratic opponent\nNo\nNo\n\n\nGreat Power\nNo\nNo\n\n\nCivil war\nNo\nNo\n\n\nCombined arms\nYes\nYes\n\n\nDoctrine\nOffensive\nOffensive\n\n\nSuperior weapons\nNo\nNo\n\n\nFortifications\nYes\nYes\n\n\nForeign advisors\nYes\nYes\n\n\nTerrain\nSemiarid coastal plain\nSemiarid grassland plain\n\n\nTopography\nRugged\nRugged\n\n\nWar duration\n126 days\n378 days\n\n\nRecent war history w/opp\nYes\nYes\n\n\nFacing colonizer\nYes\nYes\n\n\nIdentity dimension\nSunni Islam/Christian\nSunni Islam/Christian\n\n\nNew leader\nYes\nYes\n\n\nPopulation\n8‚Äì8.5 million\n5‚Äì6 million\n\n\nEthnoling fractionalization (ELF)\nHigh\nHigh\n\n\nCiv-mil relations\nRuler as commander\nRuler as commander\n\n\n\\(Y\\): Battlefield Performance:\n\n\n\n\nLoss-exchange ratio\n0.43\n0.02\n\n\nMass desertion\nNo\nYes\n\n\nMass defection\nNo\nNo\n\n\nFratricidal violence\nNo\nYes",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#particularly-fun-non-standard-examples",
    "href": "w01/index.html#particularly-fun-non-standard-examples",
    "title": "Week 1: Science from Particles to People",
    "section": "Particularly Fun Non-‚ÄúStandard‚Äù Examples",
    "text": "Particularly Fun Non-‚ÄúStandard‚Äù Examples\n\nDeDeo, French Revolution\nGrimmer, Mirrors for Sultans and Princes",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#jupyterhub",
    "href": "w01/index.html#jupyterhub",
    "title": "Week 1: Science from Particles to People",
    "section": "JupyterHub",
    "text": "JupyterHub\n\nhttps://guhub.io\n\n\n\n\nImage source",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#references",
    "href": "w01/index.html#references",
    "title": "Week 1: Science from Particles to People",
    "section": "References",
    "text": "References\n\n\nKalyvas, Stathis N. 2006. The Logic of Violence in Civil War. Cambridge University Press. https://www.dropbox.com/scl/fi/4es1r44ldnqtcroonr5g7/The-Logic-of-Violence-in-Civil-War.pdf?rlkey=uonfmw5zkx1ja5l8emv2bzfmg&dl=1.\n\n\nLyall, Jason. 2020. Divided Armies: Inequality and Battlefield Performance in Modern War. Princeton University Press.\n\n\nMontessori, Maria. 1916. Spontaneous Activity in Education: A Basic Guide to the Montessori Methods of Learning in the Classroom. Lulu Press.\n\n\nSperber, Dan. 1996. Explaining Culture: A Naturalistic Approach. Cambridge: Blackwell. https://www.dropbox.com/scl/fi/he6idaajr6vopybv13qhd/Explaining-Culture-A-Naturalistic-Approach.pdf?rlkey=8oyhnsu5ycet4osstft252jaw&dl=1.",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#footnotes",
    "href": "w01/index.html#footnotes",
    "title": "Week 1: Science from Particles to People",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúCouldn‚Äôt do classes, them lessons used to bore me, I was tryna school heads like Waldorf and Montessori‚Äù -Bar dropped by local rapper one day in freestyle that I never forgot‚Ü©Ô∏é\nObligatory quantum mechanics footnote: human agency [maybe] plays a role, in a quirky way, in physics at tiny subatomic scales‚Ä¶ but once we coarse grain to atoms (and avoid speed of light), Newton‚Äôs Laws accurate to many decimals ü§Ø‚Ü©Ô∏é\nAt least, for the time being‚Ä¶ BUT see Sperber (1996), which will come up later‚Ü©Ô∏é",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/slides.html#prof.-jeff-introduction",
    "href": "w01/slides.html#prof.-jeff-introduction",
    "title": "Week 1: Science from Particles to People",
    "section": "Prof.¬†Jeff Introduction!",
    "text": "Prof.¬†Jeff Introduction!\n\nBorn in NW DC ‚Üí high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Econ"
  },
  {
    "objectID": "w01/slides.html#the-world-outside-of-dc",
    "href": "w01/slides.html#the-world-outside-of-dc",
    "title": "Week 1: Science from Particles to People",
    "section": "The World Outside of DC",
    "text": "The World Outside of DC\n Studied abroad in Beijing (Peking University/ÂåóÂ§ß) ‚Üí internship with Huawei in Hong Kong (HKUST)\n\n\n\n Stanford, MS in Computer Science\n Research Economist, UC Berkeley\n Columbia, PhD in Political Economy"
  },
  {
    "objectID": "w01/slides.html#why-is-georgetown-having-me-teach-this",
    "href": "w01/slides.html#why-is-georgetown-having-me-teach-this",
    "title": "Week 1: Science from Particles to People",
    "section": "Why Is Georgetown Having Me Teach This?",
    "text": "Why Is Georgetown Having Me Teach This?\n\n\n\n\n\n\n\nQuanty things \\(\\leadsto\\) PhD in Political Economy\nPhD exam major: Political Philosophy\nPhD exam minor: International Relations\nPhD exam paper: ‚ÄúHow to Do Things with Translations‚Äù\nGame-changing Research Fellowships at‚Ä¶ (Nothing was the same -drake):\nSanta Fe Institute: dedicated to the multidisciplinary study of complex systems: physical, computational, biological, social\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\n\nFigure¬†1: Years spent questing in dungeons of academia\n\n\n\n\n\n\n\nCentre for the Study of the History of Political Thought, Queen Mary University of London (QMUL): New approaches to the history of political thought [Quentin Skinner, ‚ÄúCambridge School‚Äù] have changed how we study ideas from the past and their relevance to contemporary politics. The focus of the Centre is to explore [ts]."
  },
  {
    "objectID": "w01/slides.html#dissertation-nlp-x-history",
    "href": "w01/slides.html#dissertation-nlp-x-history",
    "title": "Week 1: Science from Particles to People",
    "section": "Dissertation (NLP x History)",
    "text": "Dissertation (NLP x History)\n‚ÄúOur Word is Our Weapon‚Äù: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada"
  },
  {
    "objectID": "w01/slides.html#ir-part-wars-of-ideas-i-cold-war",
    "href": "w01/slides.html#ir-part-wars-of-ideas-i-cold-war",
    "title": "Week 1: Science from Particles to People",
    "section": "(IR Part) Wars of Ideas I: Cold War",
    "text": "(IR Part) Wars of Ideas I: Cold War\n\nCold War arms shipments (SIPRI) vs.¬†propaganda (–ü–µ—á–∞—Ç—å –°–°–°–†): here, to üá™üáπ"
  },
  {
    "objectID": "w01/slides.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "href": "w01/slides.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "title": "Week 1: Science from Particles to People",
    "section": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993",
    "text": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993"
  },
  {
    "objectID": "w01/slides.html#research-nowadays",
    "href": "w01/slides.html#research-nowadays",
    "title": "Week 1: Science from Particles to People",
    "section": "Research Nowadays",
    "text": "Research Nowadays\n\nMost cited paper: ‚ÄúMonopsony in Online Labor Markets‚Äù (Uses Double-Debiased ML for Causal Inference!)\nMost recent paper: ‚ÄúOperationalizing Freedom as Non-Domination in the Labor Market‚Äù (Cambridge U Press)\nRarely cited paper but often-thought-about obsession: ‚ÄúHow To Do Things With Translations‚Äù\nRelated thing you can buy in a bookstore: [Editorial board for new translation of] Capital, Vol. 1 by Karl Marx (Princeton U Press)"
  },
  {
    "objectID": "w01/slides.html#but-now-teaching",
    "href": "w01/slides.html#but-now-teaching",
    "title": "Week 1: Science from Particles to People",
    "section": "But Now‚Ä¶ Teaching!",
    "text": "But Now‚Ä¶ Teaching!\n\nGrowth mindset: ‚ÄúI can‚Äôt do this‚Äù \\(\\leadsto\\) ‚ÄúI can‚Äôt do this yet!‚Äù\nThink of anything you‚Äôre able to do‚Ä¶ There was a point in life when you didn‚Äôt know how to do it! What happened? Your brain established and/or rearranged neural pathways as you struggled with it!\nHow does this neural re-arrangement work? One ‚Äúcheatcode‚Äù is spaced repetition:\n\n\nImage source"
  },
  {
    "objectID": "w01/slides.html#lil-wayne-on-spaced-repetition",
    "href": "w01/slides.html#lil-wayne-on-spaced-repetition",
    "title": "Week 1: Science from Particles to People",
    "section": "Lil Wayne on Spaced Repetition",
    "text": "Lil Wayne on Spaced Repetition\n\n\n\n\n\n\nFigure¬†2: From The Carter (Documentary)"
  },
  {
    "objectID": "w01/slides.html#maria-montessori-on-your-final-project",
    "href": "w01/slides.html#maria-montessori-on-your-final-project",
    "title": "Week 1: Science from Particles to People",
    "section": "Maria Montessori on Your Final Project",
    "text": "Maria Montessori on Your Final Project\n\nOur teaching should be governed, not by a desire to make students learn things, but by the endeavor to keep burning within them that light which is called curiosity. (Montessori 1916)1\n\n\nTo this end: your final project is to explore some potential causal linkage from \\(X\\) to \\(Y\\) (more later!)\nThe sole requirement is: sufficient curiosity to serve as fuel for your journey from associational world to causal world\n\n‚ÄúCouldn‚Äôt do classes, them lessons used to bore me, I was tryna school heads like Waldorf and Montessori‚Äù -Bar dropped by local rapper one day in freestyle that I never forgot"
  },
  {
    "objectID": "w01/slides.html#coarse-graining-units-of-observation",
    "href": "w01/slides.html#coarse-graining-units-of-observation",
    "title": "Week 1: Science from Particles to People",
    "section": "Coarse-Graining Units of Observation",
    "text": "Coarse-Graining Units of Observation\n\n\n\nField\nExample Unit of Observation\n\n\n\n\nPhysics\nParticle\n[Particle = Fine-Graining of Molecules]\n\n\nChemistry\nMolecule = \\(\\cup\\)(Particles)\n[Molecule = Coarse-Graining of Particles]\n\n\nBiology\nCell = \\(\\cup\\)(Molecules)\n\n\nNeuro/Cog Sci\nBrain = \\(\\cup\\)(Cells)\n\n\nHuman Physiology\nBody = Brain \\(\\cup\\) Other Organs\n\n\n‚Üë Science ¬†¬† üßê something happens here‚Ä¶ ü§î ¬†¬† ‚Üì Social Science\n\n\nAnthropology\nHuman-Relational System (e.g., Kinship) = \\(\\cup\\)(Brains) \\(\\times\\) Natural Environment\n\n\nEconomics\nEconomy = Specific relational system of exchange w.r.t. scarce resources\n\n\nPolitical Economy\nEconomy-Context = Economic Exchange \\(\\cap\\) Relational Power\n\n\nSociology\nSociety = \\(\\cup\\)(Relational Systems: Kinship, Friendship, Authority, Power, Violence)\n\n\nHistory\nLongue-Dur√©e = Evolution of Societies over Time and Space"
  },
  {
    "objectID": "w01/slides.html#homo-sapienshomo-arbitratushomo-mischievous",
    "href": "w01/slides.html#homo-sapienshomo-arbitratushomo-mischievous",
    "title": "Week 1: Science from Particles to People",
    "section": "Homo sapiens/Homo arbitratus/Homo mischievous",
    "text": "Homo sapiens/Homo arbitratus/Homo mischievous\n\nLatin sapiens denotes being ‚Äúdiscerning‚Äù or ‚Äúwise‚Äù\n\n\n\n\n\n\n\n\nBut‚Ä¶ technically nothing stops us from choosing to be ‚Äúunwise‚Äù whenever we‚Äôd like‚Ä¶ bc free will\n\\(\\Rightarrow\\) For this class, humans are Homo arbitratus: arbitratus denotes choosing what to do, after we‚Äôve sapiently ‚Äúdiscerned‚Äù/thought about it"
  },
  {
    "objectID": "w01/slides.html#laws-of-physics-vs.-laws-of-social-science",
    "href": "w01/slides.html#laws-of-physics-vs.-laws-of-social-science",
    "title": "Week 1: Science from Particles to People",
    "section": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science",
    "text": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science\n\nIf we tell an inanimate object that we‚Äôve discovered a law saying that it will accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n‚Ä¶It will likely1 still accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n\n\n\n\n\n\n\n\nIf we tell a human we‚Äôve discovered a law saying they will quack like a duck at 7:30pm EDT every Wednesday\n\n‚Ä¶They can utilize their free will to violate this ‚Äúlaw‚Äù\n\n\n\n\n\n\n\n\n\n\n\n\nObligatory quantum mechanics footnote: human agency [maybe] plays a role, in a quirky way, in physics at tiny subatomic scales‚Ä¶ but once we coarse grain to atoms (and avoid speed of light), Newton‚Äôs Laws accurate to many decimals ü§Ø"
  },
  {
    "objectID": "w01/slides.html#strangely-relevant-cs-topic-the-halting-problem",
    "href": "w01/slides.html#strangely-relevant-cs-topic-the-halting-problem",
    "title": "Week 1: Science from Particles to People",
    "section": "Strangely-Relevant CS Topic: The Halting Problem",
    "text": "Strangely-Relevant CS Topic: The Halting Problem\n\nKurt G√∂del \\(\\leftrightarrow\\) Alan Turing: Entscheidungsproblem\nTheorem: It is not possible to write a computer program \\(P(x)\\) that detects whether or not a computer program \\(x\\) will eventually halt (as opposed to, e.g., looping forever)\nProof: Assume \\(P(x)\\) is possible to write. Run it on mischievous_program.py. Infinite contradiction loop.\n\n\n\n\n\nhalting_problem_solver.py\n\ndef will_it_halt(program, input):\n  # Put code you think will work here\n  # Return True if program will halt, False otherwise\n\n\n\nmischievous_program.py\n\ndef my_mischievous_function(input):\n  if will_it_halt(my_mischevious_function, input):\n    while True: pass # Infinite loop\n  else:\n    return # Halt\n\n\n\n\n\n\n\n\nInput‚Üí‚ÜìProgram\n0\n1\n2\n\\(\\cdots\\)\n\n\n\n\n0\nHalt\nLoop\nLoop\n\\(\\cdots\\)\n\n\n1\nLoop\nLoop\nHalt\n\\(\\cdots\\)\n\n\n2\nLoop\nLoop\nLoop\n\\(\\cdots\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\mathbf{\\ddots}\\)\n\n\n\nLoop\nHalt\nHalt\n\\(\\mathbf{\\cdots}\\)"
  },
  {
    "objectID": "w01/slides.html#the-takeaway-bayesian-humility",
    "href": "w01/slides.html#the-takeaway-bayesian-humility",
    "title": "Week 1: Science from Particles to People",
    "section": "The Takeaway: [Bayesian] Humility!",
    "text": "The Takeaway: [Bayesian] Humility!\n\nSocial science, with ‚Äúscience‚Äù used in the same sense as for physics, may be a quixotic endeavor1\nInstead, we‚Äôll do social science, where we use data to‚Ä¶\n Infer tendencies: \\(\\mathcal{H}\\) = ¬´\\(X\\) tends to cause \\(Y\\)¬ª\n With some degree of veracity: \\(\\Pr(\\mathcal{H}) \\approx 0.7\\)\n Construct models that we can update with new evidence: Bayes‚Äô rule! \\(\\Pr(\\mathcal{H} \\mid E) = \\frac{\\Pr(E \\mid \\mathcal{H} ) \\Pr(\\mathcal{H})}{\\Pr(E)} \\approx 0.8\\)\nPls notice ‚Äúslippage‚Äù between aleatory probability within \\(\\mathcal{H}\\) (‚Äútends to‚Äù) vs.¬†epistemic probability ‚Äúoutside of‚Äù, talking about \\(\\mathcal{H}\\) (‚ÄúI‚Äôm 70% confident about \\(\\mathcal{H}\\)‚Äù)\n\nAt least, for the time being‚Ä¶ BUT see Sperber (1996), which will come up later"
  },
  {
    "objectID": "w01/slides.html#the-logic-of-violence-in-civil-war",
    "href": "w01/slides.html#the-logic-of-violence-in-civil-war",
    "title": "Week 1: Science from Particles to People",
    "section": "The Logic of Violence in Civil War",
    "text": "The Logic of Violence in Civil War\n\nKalyvas (2006)"
  },
  {
    "objectID": "w01/slides.html#matching-estimators",
    "href": "w01/slides.html#matching-estimators",
    "title": "Week 1: Science from Particles to People",
    "section": "Matching Estimators",
    "text": "Matching Estimators\n\nImage Source"
  },
  {
    "objectID": "w01/slides.html#case-study-military-inequality-leadsto-military-success",
    "href": "w01/slides.html#case-study-military-inequality-leadsto-military-success",
    "title": "Week 1: Science from Particles to People",
    "section": "Case Study: Military Inequality \\(\\leadsto\\) Military Success",
    "text": "Case Study: Military Inequality \\(\\leadsto\\) Military Success\n\nLyall (2020): ‚ÄúTreating certain ethnic groups as second-class citizens [‚Ä¶] leads victimized soldiers to subvert military authorities once war begins. The higher an army‚Äôs inequality, the greater its rates of desertion, side-switching, and casualties‚Äù\n\n\nMatching constructs pairs of belligerents that are similar across a wide range of traits thought to dictate battlefield performance but that vary in levels of prewar inequality. The more similar the belligerents, the better our estimate of inequality‚Äôs effects, as all other traits are shared and thus cannot explain observed differences in performance, helping assess how battlefield performance would have improved (declined) if the belligerent had a lower (higher) level of prewar inequality.\nSince [non-matched] cases are dropped [‚Ä¶] selected cases are more representative of average belligerents/wars than outliers with few or no matches, [providing] surer ground for testing generalizability of the book‚Äôs claims than focusing solely on canonical but unrepresentative usual suspects (Germany, the United States, Israel)"
  },
  {
    "objectID": "w01/slides.html#does-inequality-cause-poor-military-performance",
    "href": "w01/slides.html#does-inequality-cause-poor-military-performance",
    "title": "Week 1: Science from Particles to People",
    "section": "Does Inequality Cause Poor Military Performance?",
    "text": "Does Inequality Cause Poor Military Performance?\n\n\n\n\n\n\n\n\nCovariates\nSultanate of Morocco Spanish-Moroccan War, 1859-60\nKhanate of Kokand War with Russia, 1864-65\n\n\n\n\n\\(X\\): Military Inequality\nLow (0.01)\nExtreme (0.70)\n\n\n\\(\\mathbf{Z}\\): Matched Covariates:\n\n\n\n\nInitial relative power\n66%\n66%\n\n\nTotal fielded force\n55,000\n50,000\n\n\nRegime type\nAbsolutist Monarchy (‚àí6)\nAbsolute Monarchy (‚àí7)\n\n\nDistance from capital\n208km\n265km\n\n\nStanding army\nYes\nYes\n\n\nComposite military\nYes\nYes\n\n\nInitiator\nNo\nNo\n\n\nJoiner\nNo\nNo\n\n\nDemocratic opponent\nNo\nNo\n\n\nGreat Power\nNo\nNo\n\n\nCivil war\nNo\nNo\n\n\nCombined arms\nYes\nYes\n\n\nDoctrine\nOffensive\nOffensive\n\n\nSuperior weapons\nNo\nNo\n\n\nFortifications\nYes\nYes\n\n\nForeign advisors\nYes\nYes\n\n\nTerrain\nSemiarid coastal plain\nSemiarid grassland plain\n\n\nTopography\nRugged\nRugged\n\n\nWar duration\n126 days\n378 days\n\n\nRecent war history w/opp\nYes\nYes\n\n\nFacing colonizer\nYes\nYes\n\n\nIdentity dimension\nSunni Islam/Christian\nSunni Islam/Christian\n\n\nNew leader\nYes\nYes\n\n\nPopulation\n8‚Äì8.5 million\n5‚Äì6 million\n\n\nEthnoling fractionalization (ELF)\nHigh\nHigh\n\n\nCiv-mil relations\nRuler as commander\nRuler as commander\n\n\n\\(Y\\): Battlefield Performance:\n\n\n\n\nLoss-exchange ratio\n0.43\n0.02\n\n\nMass desertion\nNo\nYes\n\n\nMass defection\nNo\nNo\n\n\nFratricidal violence\nNo\nYes"
  },
  {
    "objectID": "w01/slides.html#particularly-fun-non-standard-examples",
    "href": "w01/slides.html#particularly-fun-non-standard-examples",
    "title": "Week 1: Science from Particles to People",
    "section": "Particularly Fun Non-‚ÄúStandard‚Äù Examples",
    "text": "Particularly Fun Non-‚ÄúStandard‚Äù Examples\n\nDeDeo, French Revolution\nGrimmer, Mirrors for Sultans and Princes"
  },
  {
    "objectID": "w01/slides.html#jupyterhub",
    "href": "w01/slides.html#jupyterhub",
    "title": "Week 1: Science from Particles to People",
    "section": "JupyterHub",
    "text": "JupyterHub\n\nhttps://guhub.io\n\n\nImage source"
  },
  {
    "objectID": "w01/slides.html#references",
    "href": "w01/slides.html#references",
    "title": "Week 1: Science from Particles to People",
    "section": "References",
    "text": "References\n\n\nKalyvas, Stathis N. 2006. The Logic of Violence in Civil War. Cambridge University Press. https://www.dropbox.com/scl/fi/4es1r44ldnqtcroonr5g7/The-Logic-of-Violence-in-Civil-War.pdf?rlkey=uonfmw5zkx1ja5l8emv2bzfmg&dl=1.\n\n\nLyall, Jason. 2020. Divided Armies: Inequality and Battlefield Performance in Modern War. Princeton University Press.\n\n\nMontessori, Maria. 1916. Spontaneous Activity in Education: A Basic Guide to the Montessori Methods of Learning in the Classroom. Lulu Press.\n\n\nSperber, Dan. 1996. Explaining Culture: A Naturalistic Approach. Cambridge: Blackwell. https://www.dropbox.com/scl/fi/he6idaajr6vopybv13qhd/Explaining-Culture-A-Naturalistic-Approach.pdf?rlkey=8oyhnsu5ycet4osstft252jaw&dl=1."
  },
  {
    "objectID": "midterm.html",
    "href": "midterm.html",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "",
    "text": "The DSAN 5650 in-class midterm will start at 6:30pm EDT on Wednesday, July 2nd. It is intended to take only 1.5 hours, but you will have 3 hours to take it just in case, so that it will be due by 9:30pm EDT.\nWe‚Äôve covered a lot of‚Ä¶ sometimes-disjointed topics thus far, so our goal for the midterm is to emphasize the most important takeaways from across the first six weeks of the class, by putting them in your brain‚Äôs short-term memory one more time before we dive into the second, computationally-focused half of the course when you come back from July 4th weekend! As a reminder, there is a very good reason to continually try and re-remember the same course topics, coming from studies of long-term memory retention:"
  },
  {
    "objectID": "midterm.html#overview",
    "href": "midterm.html#overview",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "",
    "text": "The DSAN 5650 in-class midterm will start at 6:30pm EDT on Wednesday, July 2nd. It is intended to take only 1.5 hours, but you will have 3 hours to take it just in case, so that it will be due by 9:30pm EDT.\nWe‚Äôve covered a lot of‚Ä¶ sometimes-disjointed topics thus far, so our goal for the midterm is to emphasize the most important takeaways from across the first six weeks of the class, by putting them in your brain‚Äôs short-term memory one more time before we dive into the second, computationally-focused half of the course when you come back from July 4th weekend! As a reminder, there is a very good reason to continually try and re-remember the same course topics, coming from studies of long-term memory retention:"
  },
  {
    "objectID": "midterm.html#part-1-pgms-the-language-of-dgps",
    "href": "midterm.html#part-1-pgms-the-language-of-dgps",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "Part 1: PGMs, the Language of DGPs",
    "text": "Part 1: PGMs, the Language of DGPs"
  },
  {
    "objectID": "midterm.html#part-2-statistical-matching",
    "href": "midterm.html#part-2-statistical-matching",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "Part 2: Statistical Matching",
    "text": "Part 2: Statistical Matching"
  },
  {
    "objectID": "midterm.html#references",
    "href": "midterm.html#references",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "PDF Links for Jeff‚Äôs Off-Hand References\n\n\n\n\n\n\n\n\n\n\nWhat Jeff calls it\nCitation (hover and/or click for PDF link)\n\n\n\n\n‚ÄúStatistical Rethinking‚Äù\nMcElreath (2020)\n\n\n‚ÄúThe Multilevel book‚Äù\nGelman and Hill (2007)\n\n\n‚ÄúThe Koller PGM book‚Äù\nKoller and Friedman (2009)\n\n\n‚ÄúThe non-technical Pearl book‚Äù\nPearl and Mackenzie (2018)\n\n\n‚ÄúThe technical Pearl book‚Äù (containing, e.g., axioms and proofs)\nPearl (2000)\n\n\n‚ÄúThe French Revolution paper‚Äù\nBarron et al. (2018)",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#core-textbooks",
    "href": "resources.html#core-textbooks",
    "title": "Resources",
    "section": "Core Textbooks",
    "text": "Core Textbooks\nAs mentioned on the syllabus, the field of causal inference in general (and causal inference for computational social science in particular) moves excitingly-fast, such that the material has yet to ‚Äúcongeal‚Äù into a single, all-encompassing textbook. Nonetheless, the following three books cover a substantial amount of ground (described in more detail below each citation), so that together they form a coherent ‚Äúthree-volume textbook‚Äù for this class! If you can only read three books this summer, read these :)\n\n\n\n\n\n\n Morgan and Winship (2015)\n\n\n\nMorgan and Winship, Counterfactuals and Causal Inference: Methods and Principles for Social Research [PDF]\n\n\n\n\n\n\n\n\n\nThis is the book which comes closest to being an all-encompassing, single textbook for the class! It brings together different ‚Äústrands‚Äù of causal modeling research (since each field‚Äîeconomics, bioinformatics, sociology, etc.‚Äîtends to use its own notation and vocabulary), unifying them into a single approach. The only reason we can‚Äôt use it as the main textbook is because it hasn‚Äôt been updated since 2015, and most of the assignments in this class use computational tools from 2018 onwards!\n\n\n\n\n\n\n\n\n\n\n\n Angrist and Pischke (2014)\n\n\n\nAngrist and Pischke, Mastering ‚ÄôMetrics: The Path from Cause to Effect (Angrist and Pischke 2014) [PDF]\n\n\n\n\n\n\n\n\n\nThis book is included as the second of the three ‚Äúcore‚Äù texts mainly because, it uses the language of causality specific to Econometrics, the language that is most familiar to me from my PhD training in Political Economy. However, if you tend to learn better by example, it also does a good job of foregrounding specific examples (like evaluating charter schools and community policing policies), so that the methods emerging naturally from attempts to solve these puzzles when association methods like linear regression fail to capture their causal linkages.\n\n\n\n\n\n\n\n\n\n\n\n Pearl and Mackenzie (2018)\n\n\n\nPearl and Mackenzie, The Book of Why: The New Science of Cause and Effect [EPUB]\n\n\n\n\n\n\n\n\n\nThis book contrasts with the Angrist and Pischke book in using the language of causality formed within Computer Science rather than Economics. It can be a good starting point especially if you‚Äôre unfamiliar with the heavy use of diagrams for scientific modeling‚Äîbasically, whereas Angrist and Pischke‚Äôs first instinct is to use (sometimes informal) equations like \\(y = mx + b\\) to explain steps in the procedures, Pearl and Mackenzie‚Äôs instinct would be to instead use something like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~x~\\kern .01em} \\overset{\\small m, b}{\\longrightarrow} \\enclose{circle}{\\kern.01em y~\\kern .01em}\\) to represent the same concept (in this case, a line with slope \\(m\\) and intercept \\(b\\)!).",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#topic-specific-textbooks",
    "href": "resources.html#topic-specific-textbooks",
    "title": "Resources",
    "section": "Topic-Specific Textbooks",
    "text": "Topic-Specific Textbooks\n\nProbabilistic Graphical Models\n Koller and Friedman (2009), Probabilistic Graphical Models: Principles and Techniques\nChapter 2, with an overview of probability theory and graph theory, is included in the Syllabus as the recommended reading for Week 2! Then, Chapter 3 will give you the core concepts of Bayes Nets specifically, while Chapter 6 will add in a few of the useful additional tools we use, like plate notation.\n\n\nBayesian Data Analysis\n McElreath (2020), Statistical Rethinking\nThis is‚Ä¶ possibly the book I draw on the most for the in-class and homework examples, because in my opinion it has the perfect pedagogical mixture of interesting social-scientific examples and theoretical motivations for how we can analyze those interesting examples via Bayesian modeling! So, meaning, if I introduce an example from this book, and you find yourself struggling with the logic behind it, you can go to the relevant section and read from the start of the chapter to see how McElreath ‚Äúbuilds up to‚Äù it!\n Lee and Wagenmakers (2013), Bayesian Cognitive Modeling: A Practical Course\nI wasn‚Äôt sure whether to place this in the ‚Äútopic-specific‚Äù or ‚Äúreference texts‚Äù section here, because, it serves both purposes! The split between different chapters has a super helpful logic:\n\nThe earlier chapters walk you step-by-step through how you can ‚Äúbuild up‚Äù from simpler to more complex Bayesian models of things like differences-in-means, two-sided vs.¬†one-sided hypotheses, and so on, and then\nThe chapter near the middle discusses how to use e.g.¬†Bayes Factor and log-likelihood methods to compare the different models introduced in the earlier chapters, and then finally\nThe chapters at the end apply all of the above, by taking a series of prominent research topics in Cognitive Science and showing how to effectively model them via PGMs and Bayesian inference.\n\n Gelman et al. (2013), Bayesian Data Analysis\n\n\nMultilevel Modeling\n Gelman and Hill (2007), Data Analysis Using Regression and Multilevel/Hierarchical Models",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#reference-texts",
    "href": "resources.html#reference-texts",
    "title": "Resources",
    "section": "Reference Texts",
    "text": "Reference Texts\nIn contrast to the books in the ‚ÄúCore Textbooks‚Äù section, these books are not ‚Äúthe‚Äù textbooks for the class! These are here instead as reference books, to keep on hand (a) for when the lectures or the above books are unclear on some topic, and/or (b) for deeper dives into certain topics (where the latter may become a more relevant mission for you as we move towards the final project üòâ)\n\nCausality\n Pearl (2000), Causality: Models, Reasoning, and Inference\nAs mentioned during Week 2, this is the book containing the fully-developed ‚Äúunified theory‚Äù of causality, starting from a set of axioms and deriving the possibilities of causal inference as formal theorems.\nWithin ‚Äúpure‚Äù mathematics, if you kept digging into the foundations of things like calculus or algebra, you would eventually arrive at Alfred North Whitehead and Bertrand Russell‚Äôs Principia Mathematica‚ÄîPearl (2000) is that but for causality: the foundational axioms and core theorems are all in here.\n Hume (1748), An Enquiry Concerning Human Understanding\nThis book serves as the philosophical ‚Äújumping off point‚Äù for causality: you can think of it like, there‚Äôs a nice progress-narrative of the human study of causality, that starts with the uncomfortable questions raised in Hume (1748) about the possibility (or impossibility!) of inferring knowledge of causality via inductive reasoning, and culminates in Pearl (2000).\n\n\nComputational Social Science\n**Gelman et al. (2013), *Bayesian Data Analysis (Third Edition)**\nContains info on Bayesian Workflow throughout, plus important chapters on Multilevel Modeling (Ch 5) and Bayesian Decision Analysis (Ch 9) ‚Äì the latter was referenced in the writeup on Bayesian Workflow\n Sperber (1996), Explaining Culture: A Naturalistic Approach\nMentioned in a footnote in Week 1, this is a surprisingly-old book that made waves in certain communities (like, e.g., among people like me who geek about studying culture quantitatively), by essentially proposing a ‚Äúresearch program‚Äù for the rigorous quantitative/empirical study of culture. In Jeff‚Äôs perfect world, this book would spark a progress-narrative in the same way that Hume (1748)",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#applied-examples",
    "href": "resources.html#applied-examples",
    "title": "Resources",
    "section": "Applied Examples",
    "text": "Applied Examples\n\nThe Holy Grail (But, Field = Comparative Politics)\n Kalyvas (2006), The Logic of Violence in Civil War\nThis book is essentially‚Ä¶ like, when you read stories about people spending decades hand-carving pathways through a mountain using only a hammer and chisel, this is the social science equivalent of that. A painstaking labor-of-love book that checks every single box I can think of in terms of causally-focused computational social science. It‚Äôs my model for any research I try to carry out.\n\n\nHistory\n Barron et al. (2018), ‚ÄúIndividuals, Institutions, and Innovation in the Debates of the French Revolution‚Äù\n Blaydes, Grimmer, and McQueen (2018), ‚ÄúMirrors for Princes and Sultans: Advice on the Art of Governance in the Medieval Christian and Islamic Worlds‚Äù\n\n\nEconometric Policy Evaluation\n Bj√∂rkegren, Blumenstock, and Knight (2025), ‚ÄúWhat Do Policies Value?‚Äù, Review of Economic Studies [PDF]\nAnother ‚Äúholy grail‚Äù paper, but in this case more for the Data Ethics and Policy course than this course. However, it does touch substantially on the issue of associational vs.¬†causal inference, especially in terms of how going towards causality is a ‚Äúhigh stakes‚Äù endeavor here, since we‚Äôre inferring normative ethical values from data (as opposed to descriptive statistics like the magnitude of the causal effect \\(X \\rightarrow Y\\))",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#video-resources",
    "href": "resources.html#video-resources",
    "title": "Resources",
    "section": "Video Resources",
    "text": "Video Resources\nTo me (as in, given how my brain works), there are certain topics which I‚Äôve spent hours trying to understand via reading, only to realize that there‚Äôs some simple diagram or animation out there which ‚Äúclicks‚Äù it in my mind 10000 times more effectively than the reading ever would.\nSo, to that end, these video resources are just as important as (for some topics far more important than) the resources above!1\n Ahrens (2024), ‚ÄúRobust Causal Inference using Double/Debiased Machine Learning: A Guide for Empirical Research‚Äù, MZES Methods Bites Seminar Link\nHere I literally added it to the resources folder before I realized that it has a discussion of a paper I did during the PhD. So, perhaps leaving it in here is some sort of humble brag, but finding it originally was not! (It‚Äôs what came up for me when I searched ‚ÄúDouble/Debiased Machine Learning‚Äù on YouTube in May 2025 üòú)",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#miscellaneous-commonly-used-books-that-were-not-really-using",
    "href": "resources.html#miscellaneous-commonly-used-books-that-were-not-really-using",
    "title": "Resources",
    "section": "Miscellaneous / Commonly-Used Books That We‚Äôre Not Really Using",
    "text": "Miscellaneous / Commonly-Used Books That We‚Äôre Not Really Using\n King, Keohane, and Verba (1994), Designing Social Inquiry\nI don‚Äôt really have a substantive critique of this book, or a deep reason for not using it, other than that I find myself getting really bored whenever I try to read it üôà",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#footnotes",
    "href": "resources.html#footnotes",
    "title": "Resources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor similar reasons, audiobooks may provide more effective ways to digest some topics in the course!‚Ü©Ô∏é",
    "crumbs": [
      "Resources"
    ]
  }
]