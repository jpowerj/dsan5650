[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "PDF Links for Jeff‚Äôs Off-Hand References\n\n\n\n\n\n\n\n\n\n\nWhat Jeff calls it\nCitation (click for PDF link)\n\n\n\n\n‚ÄúThe Multilevel book‚Äù\nGelman and Hill (2007)\n\n\n‚ÄúThe Koller book‚Äù\nKoller and Friedman (2009)\n\n\n‚ÄúThe non-technical Pearl book‚Äù\nPearl and Mackenzie (2018)\n\n\n‚ÄúThe technical Pearl book‚Äù (containing, e.g., axioms and proofs)\nPearl (2000)\n\n\n‚ÄúThe French Revolution paper‚Äù\nBarron et al. (2018)",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#core-textbooks",
    "href": "resources.html#core-textbooks",
    "title": "Resources",
    "section": "Core Textbooks",
    "text": "Core Textbooks\nAs mentioned on the syllabus, the field of causal inference in general (and causal inference for computational social science in particular) moves excitingly-fast, such that the material has yet to ‚Äúcongeal‚Äù into a single, all-encompassing textbook. Nonetheless, the following three books cover a substantial amount of ground (described in more detail below each citation), so that together they form a coherent ‚Äúthree-volume textbook‚Äù for this class! If you can only read three books this summer, read these :)\n\n\n\n\n\n\n Morgan and Winship (2015)\n\n\n\nMorgan and Winship, Counterfactuals and Causal Inference: Methods and Principles for Social Research [PDF]\n\n\n\n\n\n\n\n\n\nThis is the book which comes closest to being an all-encompassing, single textbook for the class! It brings together different ‚Äústrands‚Äù of causal modeling research (since each field‚Äîeconomics, bioinformatics, sociology, etc.‚Äîtends to use its own notation and vocabulary), unifying them into a single approach. The only reason we can‚Äôt use it as the main textbook is because it hasn‚Äôt been updated since 2015, and most of the assignments in this class use computational tools from 2018 onwards!\n\n\n\n\n\n\n\n\n\n\n\n Angrist and Pischke (2014)\n\n\n\nAngrist and Pischke, Mastering ‚ÄôMetrics: The Path from Cause to Effect (Angrist and Pischke 2014) [PDF]\n\n\n\n\n\n\n\n\n\nThis book is included as the second of the three ‚Äúcore‚Äù texts mainly because, it uses the language of causality specific to Econometrics, the language that is most familiar to me from my PhD training in Political Economy. However, if you tend to learn better by example, it also does a good job of foregrounding specific examples (like evaluating charter schools and community policing policies), so that the methods emerging naturally from attempts to solve these puzzles when association methods like linear regression fail to capture their causal linkages.\n\n\n\n\n\n\n\n\n\n\n\n Pearl and Mackenzie (2018)\n\n\n\nPearl and Mackenzie, The Book of Why: The New Science of Cause and Effect [EPUB]\n\n\n\n\n\n\n\n\n\nThis book contrasts with the Angrist and Pischke book in using the language of causality formed within Computer Science rather than Economics. It can be a good starting point especially if you‚Äôre unfamiliar with the heavy use of diagrams for scientific modeling‚Äîbasically, whereas Angrist and Pischke‚Äôs first instinct is to use (sometimes informal) equations like \\(y = mx + b\\) to explain steps in the procedures, Pearl and Mackenzie‚Äôs instinct would be to instead use something like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~x~\\kern .01em} \\overset{\\small m, b}{\\longrightarrow} \\enclose{circle}{\\kern.01em y~\\kern .01em}\\) to represent the same concept (in this case, a line with slope \\(m\\) and intercept \\(b\\)!).",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#topic-specific-textbooks",
    "href": "resources.html#topic-specific-textbooks",
    "title": "Resources",
    "section": "Topic-Specific Textbooks",
    "text": "Topic-Specific Textbooks\n\nProbabilistic Graphical Models\n Koller and Friedman (2009), Probabilistic Graphical Models: Principles and Techniques\nChapter 2, with an overview of probability theory and graph theory, is included in the Syllabus as the recommended reading for Week 2! Then, Chapter 3 will give you the core concepts of Bayes Nets specifically, while Chapter 6 will add in a few of the useful additional tools we use, like plate notation.\n\n\nMultilevel Modeling\n Gelman and Hill (2007), Data Analysis Using Regression and Multilevel/Hierarchical Models",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#reference-texts",
    "href": "resources.html#reference-texts",
    "title": "Resources",
    "section": "Reference Texts",
    "text": "Reference Texts\nIn contrast to the books in the ‚ÄúCore Textbooks‚Äù section, these books are not ‚Äúthe‚Äù textbooks for the class! These are here instead as reference books, to keep on hand (a) for when the lectures or the above books are unclear on some topic, and/or (b) for deeper dives into certain topics (where the latter may become a more relevant mission for you as we move towards the final project üòâ)\n Pearl (2000), Causality: Models, Reasoning, and Inference\nAs mentioned during Week 2, this is the book containing the fully-developed ‚Äúunified theory‚Äù of causality, starting from a set of axioms and deriving the possibilities of causal inference as formal theorems.\nWithin ‚Äúpure‚Äù mathematics, if you kept digging into the foundations of things like calculus or algebra, you would eventually arrive at Alfred North Whitehead and Bertrand Russell‚Äôs Principia Mathematica‚ÄîPearl (2000) is that but for causality: the foundational axioms and core theorems are all in here.\n Hume (1748), An Enquiry Concerning Human Understanding\nThis book serves as the philosophical ‚Äújumping off point‚Äù for causality: you can think of it like, there‚Äôs a nice progress-narrative of the human study of causality, that starts with the uncomfortable questions raised in Hume (1748) about the possibility (or impossibility!) of inferring knowledge of causality via inductive reasoning, and culminates in Pearl (2000).\n Sperber (1996), Explaining Culture: A Naturalistic Approach\nMentioned in a footnote in Week 1, this is a surprisingly-old book that made waves in certain communities (like, e.g., among people like me who geek about studying culture quantitatively), by essentially proposing a ‚Äúresearch program‚Äù for the rigorous quantitative/empirical study of culture. In Jeff‚Äôs perfect world, this book would spark a progress-narrative in the same way that Hume (1748)",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#applied-examples",
    "href": "resources.html#applied-examples",
    "title": "Resources",
    "section": "Applied Examples",
    "text": "Applied Examples\n\nThe Holy Grail (But, Field = Comparative Politics)\n Kalyvas (2006), The Logic of Violence in Civil War\nThis book is essentially‚Ä¶ like, when you read stories about people spending decades hand-carving pathways through a mountain using only a hammer and chisel, this is the social science equivalent of that. A painstaking labor-of-love book that checks every single box I can think of in terms of causally-focused computational social science. It‚Äôs my model for any research I try to carry out.\n\n\nHistory\n Barron et al. (2018), ‚ÄúIndividuals, Institutions, and Innovation in the Debates of the French Revolution‚Äù\n Blaydes, Grimmer, and McQueen (2018), ‚ÄúMirrors for Princes and Sultans: Advice on the Art of Governance in the Medieval Christian and Islamic Worlds‚Äù\n\n\nEconometric Policy Evaluation\n Bj√∂rkegren, Blumenstock, and Knight (2025), ‚ÄúWhat Do Policies Value?‚Äù, Review of Economic Studies [PDF]\nAnother ‚Äúholy grail‚Äù paper, but in this case more for the Data Ethics and Policy course than this course. However, it does touch substantially on the issue of associational vs.¬†causal inference, especially in terms of how going towards causality is a ‚Äúhigh stakes‚Äù endeavor here, since we‚Äôre inferring normative ethical values from data (as opposed to descriptive statistics like the magnitude of the causal effect \\(X \\rightarrow Y\\))",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#video-resources",
    "href": "resources.html#video-resources",
    "title": "Resources",
    "section": "Video Resources",
    "text": "Video Resources\nTo me (as in, given how my brain works), there are certain topics which I‚Äôve spent hours trying to understand via reading, only to realize that there‚Äôs some simple diagram or animation out there which ‚Äúclicks‚Äù it in my mind 10000 times more effectively than the reading ever would.\nSo, to that end, these video resources are just as important as (for some topics far more important than) the resources above!1\n Ahrens (2024), ‚ÄúRobust Causal Inference using Double/Debiased Machine Learning: A Guide for Empirical Research‚Äù, MZES Methods Bites Seminar Link\nHere I literally added it to the resources folder before I realized that it has a discussion of a paper I did during the PhD. So, perhaps leaving it in here is some sort of humble brag, but finding it originally was not! (It‚Äôs what came up for me when I searched ‚ÄúDouble/Debiased Machine Learning‚Äù on YouTube in May 2025 üòú)",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#miscellaneous-commonly-used-books-that-were-not-really-using",
    "href": "resources.html#miscellaneous-commonly-used-books-that-were-not-really-using",
    "title": "Resources",
    "section": "Miscellaneous / Commonly-Used Books That We‚Äôre Not Really Using",
    "text": "Miscellaneous / Commonly-Used Books That We‚Äôre Not Really Using\n King, Keohane, and Verba (1994), Designing Social Inquiry\nI don‚Äôt really have a substantive critique of this book, or a deep reason for not using it, other than that I find myself getting really bored whenever I try to read it üôà",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#footnotes",
    "href": "resources.html#footnotes",
    "title": "Resources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor similar reasons, audiobooks may provide more effective ways to digest some topics in the course!‚Ü©Ô∏é",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "midterm.html",
    "href": "midterm.html",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "",
    "text": "The DSAN 5650 in-class midterm will start at 6:30pm EDT on Wednesday, July 2nd. It is intended to take only 1.5 hours, but you will have 3 hours to take it just in case, so that it will be due by 9:30pm EDT.\nWe‚Äôve covered a lot of‚Ä¶ sometimes-disjointed topics thus far, so our goal for the midterm is to emphasize the most important takeaways from across the first six weeks of the class, by putting them in your brain‚Äôs short-term memory one more time before we dive into the second, computationally-focused half of the course when you come back from July 4th weekend! As a reminder, there is a very good reason to continually try and re-remember the same course topics, coming from studies of long-term memory retention:"
  },
  {
    "objectID": "midterm.html#overview",
    "href": "midterm.html#overview",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "",
    "text": "The DSAN 5650 in-class midterm will start at 6:30pm EDT on Wednesday, July 2nd. It is intended to take only 1.5 hours, but you will have 3 hours to take it just in case, so that it will be due by 9:30pm EDT.\nWe‚Äôve covered a lot of‚Ä¶ sometimes-disjointed topics thus far, so our goal for the midterm is to emphasize the most important takeaways from across the first six weeks of the class, by putting them in your brain‚Äôs short-term memory one more time before we dive into the second, computationally-focused half of the course when you come back from July 4th weekend! As a reminder, there is a very good reason to continually try and re-remember the same course topics, coming from studies of long-term memory retention:"
  },
  {
    "objectID": "midterm.html#part-1-pgms-the-language-of-dgps",
    "href": "midterm.html#part-1-pgms-the-language-of-dgps",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "Part 1: PGMs, the Language of DGPs",
    "text": "Part 1: PGMs, the Language of DGPs"
  },
  {
    "objectID": "midterm.html#part-2-statistical-matching",
    "href": "midterm.html#part-2-statistical-matching",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "Part 2: Statistical Matching",
    "text": "Part 2: Statistical Matching"
  },
  {
    "objectID": "midterm.html#references",
    "href": "midterm.html#references",
    "title": "DSAN 5650 Midterm Study Guide",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5650: Causal Inference for Computational Social Science",
    "section": "",
    "text": "Welcome to the homepage for DSAN 5650: Causal Inference for Computational Social Science at Georgetown University, for the Summer 2025 session!\nThe course meets on Wednesdays from 6:30pm to 9:00pm online, via the Zoom Link provided in the sidebar.\nCheck out the syllabus (or any other link in the sidebar) for more info! Or, use the following links to view notes for individual weeks:\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1: Science from Particles to People\n\n\nMay 21\n\n\n\n\nWeek 2: Probabilistic Graphical Models (PGMs)\n\n\nMay 28\n\n\n\n\nWeek 3: From PGMs to Causal Diagrams\n\n\nJune 4\n\n\n\n\nWeek 4: Clearing the Path from Cause to Effect\n\n\nJune 11\n\n\n\n\nWeek 5: Multilevel Madness, Closing Backdoor Paths\n\n\nJune 18\n\n\n\n\n\nNo matching items\n\n\nCourse Description:\nThis course provides students with the opportunity to take the analytical skills, machine learning algorithms, and statistical methods learned throughout their first year in the program and explore how they can be employed towards carrying out rigorous, original research in the behavioral and social sciences. With a particular emphasis on tackling the additional challenges which arise when moving from associational to causal inference, particularly when only observational (as opposed to experimental) data is available, students will become proficient in cutting-edge causal Machine Learning techniques such as propensity score matching, synthetic controls, causal program evaluation, inverse social welfare function estimation from panel data, and Double-Debiased Machine Learning.\nIn-class examples will cover continuous, discrete-choice, and textual data from a wide swath of social and behavioral sciences: economics, political science, sociology, anthropology, quantitative history, and digital humanities. After gaining experience through in-class labs and homework assignments focused on reproducing key findings from recent journal articles in each of these disciplines, students will spend the final weeks of the course on a final project demonstrating their ability to develop, evaluate, and test the robustness of a causal hypothesis.\nPrerequisites: DSAN 5000, DSAN 5100 (DSAN 5300 recommended but not required)",
    "crumbs": [
      "<i class='bi bi-house pe-1'></i> Home"
    ]
  },
  {
    "objectID": "writeups/pgm-intro/index.html",
    "href": "writeups/pgm-intro/index.html",
    "title": "A Quick Introduction to Probabilistic Graphical Models",
    "section": "",
    "text": "A Probabilistic Graphical Model (PGM) is just a formal mathematical representation of a data-generating process. So, if we wanted to model the relationship between weather and a person‚Äôs choice of whether to go out and party or stay in and watch a movie on a given Saturday evening, we could begin by proposing the following data-generating process:\nNow, given the description of a PGM given above (nodes as variables, edges as relationships between variables), we can perform the move alluded to in the previous section: we can convert our data-generating process into a PGM, by defining nodes (variables) and edges (relationships) as follows:\nThe resulting PGM, in graphical form1, is presented below, followed by the Conditional Probability Table describing the edge from the \\(W\\) node to the \\(Y\\) node.\nPGMs can help us make inferences about the world in the face of incomplete information, which is the situation in nearly every real-world problem. The key tool here is the separation of nodes into two categories: observed (represented graphically as a shaded node) and latent (represented graphically as an unshaded node).\nThus we can now use our model as a weather-inference machine: if we observe that the person we‚Äôre modeling is out at a party with us, what can we infer from this information about the weather outside? We can draw this situation as a PGM with shaded and unshaded nodes, as in the figure below, and then use Bayes‚Äô Rule to perform calculations over the network, to see how the observed information about the person at the party ‚Äúflows‚Äù back into the node representing the weather.\nKeeping in mind that Bayes‚Äô Rule tells us, for any two events \\(A\\) and \\(B\\), how to use information about \\(\\Pr(B \\mid A)\\) to obtain information about \\(\\Pr(A \\mid B)\\):\n\\[\n\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)},\n\\]\nWe can now apply this rule to obtain our new probability distribution over the weather, taking into account the new information that the person has chosen to go out:\n\\[\n\\begin{align*}\n&\\Pr(W = \\textsf{Sunny} \\mid Y = \\textsf{Go Out})\n= \\frac{\\Pr(Y = \\textsf{Go Out} \\mid W = \\textsf{Sunny})}{\\Pr(Y = \\textsf{Go Out})} \\\\\n= &\\frac{\\Pr(Y = \\textsf{Go Out} \\mid W = \\textsf{Sunny})}{\\Pr(Y = \\textsf{Go Out} \\mid W = \\textsf{Sunny}) + \\Pr(Y = \\textsf{Go Out} \\mid W = \\textsf{Rainy})}\n\\end{align*}\n\\]\nAnd now we simply plug in the information we already have from our conditional probability table to obtain our new (conditional) probability of interest:\n\\[\n\\begin{align*}\n\\Pr(W = \\textsf{Sunny} \\mid Y = \\textsf{Go Out}) &= \\frac{(0.8)(0.5)}{(0.8)(0.5) + (0.1)(0.5)} \\\\\n&= \\frac{0.4}{0.4 + 0.05} = \\frac{0.4}{0.45} \\approx 0.89.\n\\end{align*}\n\\]\nWe have learned something interesting: now that we‚Äôve observed the person out at a party, the probability that it is sunny out jumps from \\(0.5\\) (called the ‚Äúprior‚Äù estimate of \\(W\\), i.e., our best guess without any other relevant information) to \\(0.89\\) (called the ‚Äúposterior‚Äù estimate of \\(W\\), i.e., our best guess after incorporating relevant information)."
  },
  {
    "objectID": "writeups/pgm-intro/index.html#footnotes",
    "href": "writeups/pgm-intro/index.html#footnotes",
    "title": "A Quick Introduction to Probabilistic Graphical Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe term ‚ÄúGraphical‚Äù in Probabilistic Graphical Model is not used in the same sense as the ‚Äúgraphical‚Äù we‚Äôre used to from vernacular English. Capital-G Graphical denotes that the Probabilistic Model is represented as a Graph, a well-defined mathematical object consisting of nodes and edges, which does not have to be represented graphically (though it could be, like in our example here with circles and arrows). In fact, when a computer program is estimating a PGM, it is by definition not in a graphical form‚Äîit‚Äôs in the form of 0s and 1s, stored in the computer‚Äôs memory.‚Ü©Ô∏é"
  },
  {
    "objectID": "w05/slides.html#pooling-none-full-and-adaptive",
    "href": "w05/slides.html#pooling-none-full-and-adaptive",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pooling: None, Full, and Adaptive",
    "text": "Pooling: None, Full, and Adaptive\n\nFrom Gelman and Hill (2007)\\[\n\\hat{\\alpha}_j^{\\text {multilevel }} = \\frac{\\frac{n_j}{\\sigma_y^2} \\bar{y}_j+\\frac{1}{\\sigma_\\alpha^2} \\bar{y}_{\\text {all }}}{\\frac{n_j}{\\sigma_y^2}+\\frac{1}{\\sigma_\\alpha^2}}\n\\]"
  },
  {
    "objectID": "w05/slides.html#why-adaptive-none-or-full",
    "href": "w05/slides.html#why-adaptive-none-or-full",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Why Adaptive >> None or Full?",
    "text": "Why Adaptive &gt;&gt; None or Full?\n\n\\[\n\\alpha_j \\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha), \\text{ for }j = 1, \\ldots, J\n\\]\n\nIn the limit of \\(\\sigma_\\alpha \\rightarrow \\infty\\), the soft constraints do nothing, and there is no pooling;\nAs \\(\\sigma_\\alpha \\rightarrow 0\\), they pull the estimates all the way to zero, yielding the complete-pooling estimate"
  },
  {
    "objectID": "w05/slides.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/slides.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag_closed, Z=\"Z\", lwd=5)\nadj_sets_closed &lt;- adjustmentSets(\n    pipe_dag_closed\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_closed\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}"
  },
  {
    "objectID": "w05/slides.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/slides.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w05/slides.html#conditioning-on-a-proxy-for-z",
    "href": "w05/slides.html#conditioning-on-a-proxy-for-z",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Conditioning on a Proxy for \\(Z\\)",
    "text": "Conditioning on a Proxy for \\(Z\\)\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A \\Rightarrow\\) some (not all!) info about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cprox_a0_slope,\"&lt;/span&gt;\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cprox_a1_slope,\"&lt;/span&gt;\")\ncprox_a_texlabel &lt;- paste0(cprox_a0_label, \" | \", cprox_a1_label)\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.5*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_markdown(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )"
  },
  {
    "objectID": "w05/slides.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "href": "w05/slides.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening",
    "text": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\ncoll_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(coll_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(coll_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(coll_dag, lwd=5)\nadj_sets_coll &lt;- adjustmentSets(\n    coll_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_coll\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w05/slides.html#is-this-just-a-corner-case",
    "href": "w05/slides.html#is-this-just-a-corner-case",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Is This Just a Corner Case?",
    "text": "Is This Just a Corner Case?\n\nGrant applications are regularly judged on two criteria: novelty and rigor\nJudges grade proposal separately on the two criteria, grant is awarded to top \\(N\\) applications based on combined scores‚Ä¶\n\n\nCode\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = runif(n_c, 0, 10),\n    rigorous = runif(n_c, 0, 10),\n    awarded = ifelse(newsworthy + rigorous &gt; 10, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = rnorm(n_c, 5, 1),\n    rigorous = rnorm(n_c, 5, 1),\n    awarded = ifelse(newsworthy + rigorous &gt; 12, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )"
  },
  {
    "objectID": "w05/slides.html#blocking-backdoor-paths",
    "href": "w05/slides.html#blocking-backdoor-paths",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Blocking Backdoor Paths",
    "text": "Blocking Backdoor Paths\ndagitty: R interface to dagitty.net\n\n\n\nCode\nrt_dag &lt;- dagitty(\"dag{\nX [exposure]\nY [outcome]\nU [unobserved]\nX -&gt; Y\nX &lt;- U &lt;- A -&gt; C -&gt; Y\nU -&gt; B &lt;- C\n}\")\ncoordinates(rt_dag) &lt;- list(\n    x=c(U=0, X=0, A=0.5, B=0.5, C=1, Y=1),\n    y=c(X=0.75, Y=0.75, B=0.5, U=0.25, C=0.25, A=0)\n)\ndrawdag(rt_dag, cex=2, lwd=4, radius=6)\n\n\n\n\n\n\n\n\n\n\n\n\nTwo backdoor paths!\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\leftarrow A \\rightarrow C \\rightarrow Y\\): Open or closed?\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\rightarrow B \\leftarrow C \\rightarrow Y\\): Open or closed?\n\n\n\nCode\nadjustmentSets(rt_dag)\n\n\n{ C }\n{ A }"
  },
  {
    "objectID": "w05/slides.html#references",
    "href": "w05/slides.html#references",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. https://www.dropbox.com/scl/fi/asbumi3g0gqa4xl9va7wp/Andrew-Gelman-Jennifer-Hill-Data-Analysis-Using-Regression-and-Multilevel_Hierarchical-Models.pdf?rlkey=zf8icjhm7rswvxrpm7d10m65o&dl=1."
  },
  {
    "objectID": "w05/index.html",
    "href": "w05/index.html",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#pooling-none-full-and-adaptive",
    "href": "w05/index.html#pooling-none-full-and-adaptive",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pooling: None, Full, and Adaptive",
    "text": "Pooling: None, Full, and Adaptive\n\n\n\nFrom Gelman and Hill (2007)\n\n\n\\[\n\\hat{\\alpha}_j^{\\text {multilevel }} = \\frac{\\frac{n_j}{\\sigma_y^2} \\bar{y}_j+\\frac{1}{\\sigma_\\alpha^2} \\bar{y}_{\\text {all }}}{\\frac{n_j}{\\sigma_y^2}+\\frac{1}{\\sigma_\\alpha^2}}\n\\]",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#why-adaptive-none-or-full",
    "href": "w05/index.html#why-adaptive-none-or-full",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Why Adaptive >> None or Full?",
    "text": "Why Adaptive &gt;&gt; None or Full?\n\n\n\n\n\n\\[\n\\alpha_j \\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha), \\text{ for }j = 1, \\ldots, J\n\\]\n\nIn the limit of \\(\\sigma_\\alpha \\rightarrow \\infty\\), the soft constraints do nothing, and there is no pooling;\nAs \\(\\sigma_\\alpha \\rightarrow 0\\), they pull the estimates all the way to zero, yielding the complete-pooling estimate",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/index.html#pipes-x-rightarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Pipes \\(X \\rightarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag_closed, Z=\"Z\", lwd=5)\nadj_sets_closed &lt;- adjustmentSets(\n    pipe_dag_closed\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_closed\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cpipe_z0_slope,\"&lt;/span&gt;\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cpipe_z1_slope,\"&lt;/span&gt;\")\ncpipe_z_texlabel &lt;- paste0(cpipe_z0_label, \" | \", cpipe_z1_label)\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "href": "w05/index.html#forks-x-leftarrow-z-rightarrow-y-conditioning-blocking",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking",
    "text": "Forks \\(X \\leftarrow Z \\rightarrow Y\\): Conditioning = Blocking\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\npipe_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(pipe_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(pipe_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(pipe_dag, lwd=5)\nadj_sets &lt;- adjustmentSets(\n    pipe_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n{ Z }\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nZ -&gt; X\nZ -&gt; Y\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nlibrary(ggtext)\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",z0_slope,\"&lt;/span&gt;\")\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",z1_slope,\"&lt;/span&gt;\")\nz_texlabel &lt;- paste0(z0_label, \" | \", z1_label)\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_text(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#conditioning-on-a-proxy-for-z",
    "href": "w05/index.html#conditioning-on-a-proxy-for-z",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Conditioning on a Proxy for \\(Z\\)",
    "text": "Conditioning on a Proxy for \\(Z\\)\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A \\Rightarrow\\) some (not all!) info about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",cprox_a0_slope,\"&lt;/span&gt;\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",cprox_a1_slope,\"&lt;/span&gt;\")\ncprox_a_texlabel &lt;- paste0(cprox_a0_label, \" | \", cprox_a1_label)\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.5*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_markdown(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "href": "w05/index.html#colliders-x-rightarrow-z-leftarrow-y-conditioning-opening",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening",
    "text": "‚ö†Ô∏èColliders‚ö†Ô∏è \\(X \\rightarrow Z \\leftarrow Y\\): Conditioning = Opening\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\ncoll_dag &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(coll_dag) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\ndrawdag(coll_dag, cex=4, lwd=5, radius=10)\ndrawopenpaths(coll_dag, lwd=5)\nadj_sets_coll &lt;- adjustmentSets(\n    coll_dag, effect=\"direct\"\n)\nwriteLines(\"Adjustment sets (direct effect):\")\nadj_sets_coll\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 3, color='white'\n  ) +\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustment sets (direct effect):\n {}\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(rethinking)\nlibrary(dagitty)\nlibrary(ggdag)\nfork_dag_closed &lt;-dagitty(\"dag{\nX[exposure]\nY[outcome]\nZ[adjustedNode]\nX -&gt; Y\nX -&gt; Z\nY -&gt; Z\n}\")\ncoordinates(fork_dag_closed) &lt;- list(\n    x=c(X=0, Z=0.5, Y=1),\n    y=c(X=1, Z=0.5, Y=1)\n)\nfork_dag_closed &lt;- setVariableStatus(fork_dag_closed, \"adjustedNode\", \"Z\")\ndrawdag(fork_dag_closed, cex=4, lwd=5, radius=10)\ndrawopenpaths(fork_dag_closed, Z=\"Z\", lwd=5)\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"&lt;span style='color: #e69f00;'&gt;Slope&lt;sub&gt;Z=0&lt;/sub&gt; = \",ccoll_z0_slope,\"&lt;/span&gt;\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;Z=1&lt;/sub&gt; = \",ccoll_z1_slope,\"&lt;/span&gt;\")\nccoll_z_texlabel &lt;- paste0(ccoll_z0_label, \" | \", ccoll_z1_label)\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#is-this-just-a-corner-case",
    "href": "w05/index.html#is-this-just-a-corner-case",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Is This Just a Corner Case?",
    "text": "Is This Just a Corner Case?\n\nGrant applications are regularly judged on two criteria: novelty and rigor\nJudges grade proposal separately on the two criteria, grant is awarded to top \\(N\\) applications based on combined scores‚Ä¶\n\n\nCode\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = runif(n_c, 0, 10),\n    rigorous = runif(n_c, 0, 10),\n    awarded = ifelse(newsworthy + rigorous &gt; 10, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )\nset.seed(5650)\ngrant_df &lt;- tibble(\n    newsworthy = rnorm(n_c, 5, 1),\n    rigorous = rnorm(n_c, 5, 1),\n    awarded = ifelse(newsworthy + rigorous &gt; 12, 1, 0)\n)\ngrant_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df)\ngrant_slope &lt;- round(grant_lm$coef['newsworthy'], 3)\ngrant_z0_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 0))\ngrant_z0_slope &lt;- round(grant_z0_lm$coef['newsworthy'], 2)\ngrant_z1_lm &lt;- lm(rigorous ~ newsworthy, data=grant_df |&gt; filter(awarded == 1))\ngrant_z1_slope &lt;- round(grant_z1_lm$coef['newsworthy'], 2)\ngrant_z1_label &lt;- paste0(\"&lt;span style='color: #56b4e9;'&gt;Slope&lt;sub&gt;+&lt;/sub&gt; = \",grant_z1_slope,\"&lt;/span&gt;\")\ngrant_z_texlabel &lt;- grant_z1_label\ngrant_xmin &lt;- min(grant_df$newsworthy)\ngrant_xmax &lt;- max(grant_df$newsworthy)\nggplot() +\n  # Points\n  geom_point(\n    data=grant_df |&gt; filter(rigorous &gt; -3),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    size=0.4*g_pointsize,\n    alpha=0.8\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, group=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=grant_df |&gt; filter(awarded == 1),\n    aes(x=newsworthy, y=rigorous, color=factor(awarded)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=18) +\n  theme(\n    plot.title = element_markdown(size=18),\n    plot.subtitle = element_markdown(size=16)\n  ) +\n  coord_equal() +\n  labs(\n    title=grant_z_texlabel,\n    x = \"Newsworthy?\", y = \"Rigorous?\", color = \"awarded\"\n  )\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#blocking-backdoor-paths",
    "href": "w05/index.html#blocking-backdoor-paths",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "Blocking Backdoor Paths",
    "text": "Blocking Backdoor Paths\ndagitty: R interface to dagitty.net\n\n\nCode\nrt_dag &lt;- dagitty(\"dag{\nX [exposure]\nY [outcome]\nU [unobserved]\nX -&gt; Y\nX &lt;- U &lt;- A -&gt; C -&gt; Y\nU -&gt; B &lt;- C\n}\")\ncoordinates(rt_dag) &lt;- list(\n    x=c(U=0, X=0, A=0.5, B=0.5, C=1, Y=1),\n    y=c(X=0.75, Y=0.75, B=0.5, U=0.25, C=0.25, A=0)\n)\ndrawdag(rt_dag, cex=2, lwd=4, radius=6)\n\n\n\n\n\n\n\n\n\n\n\nTwo backdoor paths!\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\leftarrow A \\rightarrow C \\rightarrow Y\\): Open or closed?\n \\(X \\leftarrow \\require{enclose}\\enclose{circle}{U} \\rightarrow B \\leftarrow C \\rightarrow Y\\): Open or closed?\n\n\n\nCode\nadjustmentSets(rt_dag)\n\n\n{ C }\n{ A }",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w05/index.html#references",
    "href": "w05/index.html#references",
    "title": "Week 5: Multilevel Madness, Closing Backdoor Paths",
    "section": "References",
    "text": "References\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. https://www.dropbox.com/scl/fi/asbumi3g0gqa4xl9va7wp/Andrew-Gelman-Jennifer-Hill-Data-Analysis-Using-Regression-and-Multilevel_Hierarchical-Models.pdf?rlkey=zf8icjhm7rswvxrpm7d10m65o&dl=1.",
    "crumbs": [
      "Week 5: {{< var w05.date-md >}}"
    ]
  },
  {
    "objectID": "w04/slides.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "href": "w04/slides.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?",
    "text": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?\n\nMy answer: allows you to adapt to specifics/idiosyncrasies of your problem!\nLanguage metaphor: Learning models vs.¬†learning modeling language \\(\\Leftrightarrow\\) Learning phrases in a language vs.¬†learning to speak the language\n‚ÄúHello, one hamburger please‚Äù is good, but what if you‚Ä¶\n Are allergic to ketchup and need to make sure it‚Äôs removed\n Want to replace sesame seed bun with poppy seed bun, if they have it\n Prefer spicy, but not too spicy, mustard  Bun only  Animal style  ‚Ä¶\n\n\n\n\n\n\n\n\nLanguages give us a syntax‚Ä¶\n\n\n\nS\n\\(\\rightarrow\\)\nNP VP\n\n\nNP\n\\(\\rightarrow\\)\nDetP N | AdjP NP\n\n\nVP\n\\(\\rightarrow\\)\nV NP\n\n\nAdjP\n\\(\\rightarrow\\)\nAdj | Adv AdjP\n\n\nN\n\\(\\rightarrow\\)\nfrog | tadpole\n\n\nV\n\\(\\rightarrow\\)\nsees | likes\n\n\nAdj\n\\(\\rightarrow\\)\nbig | small\n\n\nAdv\n\\(\\rightarrow\\)\nvery\n\n\nDetP\n\\(\\rightarrow\\)\na | the\n\n\n\n\n\n\n\n‚Ä¶For expressing arbitrary (infinitely many!) sentences"
  },
  {
    "objectID": "w04/slides.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "href": "w04/slides.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)",
    "text": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)\nNeed a language that can communicate the following info to estimation algorithm:\n\n Unit of observation is tadpole, but unit of analysis is tank\n Ultimately, I care about \\(Y =\\) survival rate (dependent var), as function of \\(X =\\) tank properties (independent var)\n ‚Ä¶But the \\(n_i = 48\\) tanks actually come in \\(n_j = 3\\) types: small (10 bois), medium (25), large (35) (Bonus: What if there are different numbers of tanks per type?)\n I need it to account for impact of tank size, then  pool info across tank sizes\n\n\nFrom McElreath (2020)"
  },
  {
    "objectID": "w04/slides.html#example-2-dissertation-nightmare",
    "href": "w04/slides.html#example-2-dissertation-nightmare",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 2: Dissertation Nightmare",
    "text": "Example 2: Dissertation Nightmare\n\n\n\n\n\n\n\n\n\nAbove: Data from Soviet archives; Above Right: US Military archives; Below Right: NATO archives"
  },
  {
    "objectID": "w04/slides.html#nightmarish-without-a-modeling-language",
    "href": "w04/slides.html#nightmarish-without-a-modeling-language",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Nightmarish Without a Modeling Language!",
    "text": "Nightmarish Without a Modeling Language!\n\nModeling language \\(\\Rightarrow\\) Unambiguously ‚Äúencode‚Äù idiosyncratic domain knowledge\nDissertation: Cold War \\(\\times\\) ‚ÄúThird World‚Äù \\(\\leadsto\\) Cuban üá®üá∫ trans-continental operations1\nMain narrative (for estimation): 1975 (South Africa invades Angola, 14 Oct ‚Üí üá®üá∫ intervention, 4 Nov) to 1979 (USSR requests üá®üá∫ troops to Ethiopia for Ogaden War)\n[Ontology] Fix 1979 geographic entities at National level (as modeling choice, like fixing 2000 USD to measure inflation): \\(\\textsf{Cuba}_{1979}\\), \\(\\textsf{Angola}_{1979}\\), \\(\\textsf{PDRY}_{1979}\\), \\(\\textsf{YAR}_{1979}\\)\nDifferent tokens (Think NLP: \"Congo\", \"DRC\", \"Republic of Congo\") can then be contextualized: can ‚Äútrack‚Äù and link data appropriately despite splits, merges, name changes\nSay we have data on ‚ÄúNumber of Communist Militants in \\(X\\)‚Äù (Hoover Yearbook)‚Ä¶\n\n\n\n\nEntity\nData from 1947-1971 at...\nData from 1971-Present at...\n\n\n\n\n\\(\\textsf{Pakistan}_{1979}\\)\nNational Level:\n\\(\\frac{62}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúPakistan‚Äù\n\n\nSubnational Level:\n‚ÄúWest Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)\n\n\n\\(\\textsf{Bangladesh}_{1979}\\)\nNational Level:\n\\(\\frac{70}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúBangladesh‚Äù\n\n\nSubnational Level:\n‚ÄúEast Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)\n\n\n\nHelpful metaphor (Gleijeses 2013): Cuba \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for USSR (Soviet $ but Cuban training of PAIGC ‚Üí MPLA), as Israel \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for US (US $ but Israeli training of SAVAK ‚Üí SADF)"
  },
  {
    "objectID": "w04/slides.html#the-fork-x-leftarrow-z-rightarrow-y",
    "href": "w04/slides.html#the-fork-x-leftarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)",
    "text": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\nfork_df &lt;- tibble(\n    Z = rbern(n_d),\n    X = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\nplot_freqs &lt;- function(df, plot_title, y_lab=TRUE) {\n  df_cor &lt;- cor(df$X, df$Y)\n  df_label &lt;- paste0(\"Cor(X,Y) = \",round(df_cor,3))\n  freq_df &lt;- df |&gt;\n    group_by(X, Y) |&gt;\n    summarize(count=n())\n  freq_plot &lt;- freq_df |&gt;\n    ggplot(\n      aes(x=factor(X), y=factor(Y), fill=count)\n    ) +\n    geom_tile() +\n    coord_equal() +\n    scale_fill_distiller(\n      palette=\"Greens\", direction=1,\n      limits=c(0,5000)\n    ) +\n    geom_label(\n      aes(label=count),\n      fill=\"white\", color=\"black\", size=7\n    ) +\n    labs(\n      title = plot_title,\n      subtitle = df_label,\n      x=\"X\", y=\"Y\"\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      plot.title = element_text(size=21),\n      plot.subtitle = element_text(size=18)\n    ) +\n    remove_legend()\n  if (!y_lab) {\n    freq_plot &lt;- freq_plot + theme(\n      axis.title.y = element_blank()\n    )\n  }\n  return(freq_plot)\n}\n# The full df\nfull_label &lt;- paste0(\"Raw Data (n = 10K)\")\nfull_plot &lt;- plot_freqs(fork_df, full_label)\n# Conditioning on Z = 0\nz0_df &lt;- fork_df |&gt; filter(Z == 0)\nz0_n &lt;- nrow(z0_df)\nz0_label &lt;- paste0(\"Z == 0 (\",z0_n,\" obs)\")\nz0_plot &lt;- plot_freqs(z0_df, z0_label, y_lab=FALSE)\n# Conditioning on Z = 1\nz1_df &lt;- fork_df |&gt; filter(Z == 1)\nz1_n &lt;- nrow(z1_df)\nz1_label &lt;- paste0(\"Z == 1 (\",z1_n,\" obs)\")\nz1_plot &lt;- plot_freqs(z1_df, z1_label, y_lab=FALSE)\nfull_plot | z0_plot | z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"$Slope_{Z=0} = \",z0_slope,\"$\")\nz0_leg_label &lt;- TeX(paste0(\"0 $(m=\",z0_slope,\")$\"))\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"$Slope_{Z=1} = \",z1_slope,\"$\")\nz_texlabel &lt;- TeX(paste0(z0_label, \" | \", z1_label))\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w04/slides.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "href": "w04/slides.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)",
    "text": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\npipe_df &lt;- tibble(\n    X = rbern(n_d),\n    Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\n# The full df\npipe_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\npipe_full_plot &lt;- plot_freqs(pipe_df, pipe_full_label)\n# Conditioning on Z = 0\npipe_z0_df &lt;- pipe_df |&gt; filter(Z == 0)\npipe_z0_n &lt;- nrow(pipe_z0_df)\npipe_z0_label &lt;- paste0(\"Z == 0 (\",pipe_z0_n,\" obs)\")\npipe_z0_plot &lt;- plot_freqs(pipe_z0_df, pipe_z0_label, y_lab=FALSE)\n# Conditioning on Z = 1\npipe_z1_df &lt;- pipe_df |&gt; filter(Z == 1)\npipe_z1_n &lt;- nrow(pipe_z1_df)\npipe_z1_label &lt;- paste0(\"Z == 1 (\",pipe_z1_n,\" obs)\")\npipe_z1_plot &lt;- plot_freqs(pipe_z1_df, pipe_z1_label, y_lab=FALSE)\npipe_full_plot | pipe_z0_plot | pipe_z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",cpipe_z0_slope,\"$\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",cpipe_z1_slope,\"$\")\ncpipe_z_texlabel &lt;- TeX(paste0(cpipe_z0_label, \" | \", cpipe_z1_label))\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    subtitle=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )"
  },
  {
    "objectID": "w04/slides.html#the-collider-x-rightarrow-z-leftarrow-y",
    "href": "w04/slides.html#the-collider-x-rightarrow-z-leftarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)",
    "text": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)\n\n\n\n\n\nCode\nset.seed(5650)\ncoll_df &lt;- tibble(\n    X = rbern(n_d),\n    Y = rbern(n_d),\n    Z = rbern(n_d, ifelse(X + Y &gt; 0, 0.9, 0.2)),\n)\n\n\n\n\nCode\n# The full df\ncoll_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\ncoll_full_plot &lt;- plot_freqs(coll_df, coll_full_label)\n# Conditioning on Z = 0\ncoll_z0_df &lt;- coll_df |&gt; filter(Z == 0)\ncoll_z0_n &lt;- nrow(coll_z0_df)\ncoll_z0_label &lt;- paste0(\"Z == 0 (\",coll_z0_n,\" obs)\")\ncoll_z0_plot &lt;- plot_freqs(coll_z0_df, coll_z0_label, y_lab=FALSE)\n# Conditioning on Z = 1\ncoll_z1_df &lt;- coll_df |&gt; filter(Z == 1)\ncoll_z1_n &lt;- nrow(coll_z1_df)\ncoll_z1_label &lt;- paste0(\"Z == 1 (\",coll_z1_n,\" obs)\")\ncoll_z1_plot &lt;- plot_freqs(coll_z1_df, coll_z1_label, y_lab=FALSE)\ncoll_full_plot | coll_z0_plot | coll_z1_plot\n\n\n\n\n\n\n\n\n\n\nConditioning on colliders induces correlation where there previously was none ‚ò†Ô∏è\n\n\n\n\n\nCode\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\n\n\n\n\nCode\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",ccoll_z0_slope,\"$\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",ccoll_z1_slope,\"$\")\nccoll_z_texlabel &lt;- TeX(paste0(ccoll_z0_label, \" | \", ccoll_z1_label))\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",ccoll_slope\n    ),\n    subtitle=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶This is why we have to think, rather than just ‚Äúcontrol for everything‚Äù! üò≠"
  },
  {
    "objectID": "w04/slides.html#proxies-for-z",
    "href": "w04/slides.html#proxies-for-z",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Proxies for \\(Z\\)",
    "text": "Proxies for \\(Z\\)\n\n\n\n\n\nCode\nset.seed(5650)\nprox_df &lt;- tibble(\n  X = rbern(n_d),\n  Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n  Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n  A = rbern(n_d, (1-Z)*0.1 + Z*0.9)\n)\n\n\n\n\nCode\n# The full df\nprox_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\nprox_full_plot &lt;- plot_freqs(prox_df, prox_full_label)\n# Conditioning on A == 0\nprox_a0_df &lt;- prox_df |&gt; filter(A == 0)\nprox_a0_n &lt;- nrow(prox_a0_df)\nprox_a0_label &lt;- paste0(\"A == 0 (\",prox_a0_n,\" obs)\")\nprox_a0_plot &lt;- plot_freqs(prox_a0_df, prox_a0_label, y_lab=FALSE)\n# Conditioning on A == 1\nprox_a1_df &lt;- prox_df |&gt; filter(A == 1)\nprox_a1_n &lt;- nrow(prox_a1_df)\nprox_a1_label &lt;- paste0(\"A == 1 (\",prox_a1_n,\" obs)\")\nprox_a1_plot &lt;- plot_freqs(prox_a1_df, prox_a1_label, y_lab=FALSE)\nprox_full_plot | prox_a0_plot | prox_a1_plot\n\n\n\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A\\) gives us some (not all!) information about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\nn_c &lt;- 300\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"$Slope_{A=0} = \",cprox_a0_slope,\"$\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"$Slope_{A=1} = \",cprox_a1_slope,\"$\")\ncprox_a_texlabel &lt;- TeX(paste0(cprox_a0_label, \" | \", cprox_a1_label))\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_text(size=18)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )"
  },
  {
    "objectID": "w04/slides.html#references",
    "href": "w04/slides.html#references",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "References",
    "text": "References\n\n\nBezuidenhout, Dana, Sarah Forthal, Kara Rudolph, and Matthew R Lamb. 2024. ‚ÄúSingle World Intervention Graphs (SWIGs): A Practical Guide.‚Äù American Journal of Epidemiology, September, kwae353. https://doi.org/10.1093/aje/kwae353.\n\n\nGleijeses, Piero. 2013. Visions of Freedom: Havana, Washington, Pretoria, and the Struggle for Southern Africa, 1976-1991. UNC Press Books.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and STAN. CRC Press."
  },
  {
    "objectID": "w04/index.html",
    "href": "w04/index.html",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "href": "w04/index.html#why-take-the-time-to-learn-a-modeling-language-vs.-individual-models",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?",
    "text": "Why Take the Time to Learn a Modeling Language (vs.¬†Individual Models)?\n\nMy answer: allows you to adapt to specifics/idiosyncrasies of your problem!\nLanguage metaphor: Learning models vs.¬†learning modeling language \\(\\Leftrightarrow\\) Learning phrases in a language vs.¬†learning to speak the language\n‚ÄúHello, one hamburger please‚Äù is good, but what if you‚Ä¶\n Are allergic to ketchup and need to make sure it‚Äôs removed\n Want to replace sesame seed bun with poppy seed bun, if they have it\n Prefer spicy, but not too spicy, mustard  Bun only  Animal style  ‚Ä¶\n\n\n\n\n\n\n\n\nLanguages give us a syntax‚Ä¶\n\n\n\nS\n\\(\\rightarrow\\)\nNP VP\n\n\nNP\n\\(\\rightarrow\\)\nDetP N | AdjP NP\n\n\nVP\n\\(\\rightarrow\\)\nV NP\n\n\nAdjP\n\\(\\rightarrow\\)\nAdj | Adv AdjP\n\n\nN\n\\(\\rightarrow\\)\nfrog | tadpole\n\n\nV\n\\(\\rightarrow\\)\nsees | likes\n\n\nAdj\n\\(\\rightarrow\\)\nbig | small\n\n\nAdv\n\\(\\rightarrow\\)\nvery\n\n\nDetP\n\\(\\rightarrow\\)\na | the\n\n\n\n\n\n\n\n‚Ä¶For expressing arbitrary (infinitely many!) sentences",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "href": "w04/index.html#example-1-multilevel-tadpoles-mcelreath-ch.-13",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)",
    "text": "Example 1: Multilevel Tadpoles (McElreath, Ch. 13)\nNeed a language that can communicate the following info to estimation algorithm:\n\n Unit of observation is tadpole, but unit of analysis is tank\n Ultimately, I care about \\(Y =\\) survival rate (dependent var), as function of \\(X =\\) tank properties (independent var)\n ‚Ä¶But the \\(n_i = 48\\) tanks actually come in \\(n_j = 3\\) types: small (10 bois), medium (25), large (35) (Bonus: What if there are different numbers of tanks per type?)\n I need it to account for impact of tank size, then  pool info across tank sizes\n\n\n\n\nFrom McElreath (2020)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#example-2-dissertation-nightmare",
    "href": "w04/index.html#example-2-dissertation-nightmare",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Example 2: Dissertation Nightmare",
    "text": "Example 2: Dissertation Nightmare\n\n\n\n\n\n\n\n\n\nAbove: Data from Soviet archives; Above Right: US Military archives; Below Right: NATO archives",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#nightmarish-without-a-modeling-language",
    "href": "w04/index.html#nightmarish-without-a-modeling-language",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Nightmarish Without a Modeling Language!",
    "text": "Nightmarish Without a Modeling Language!\n\nModeling language \\(\\Rightarrow\\) Unambiguously ‚Äúencode‚Äù idiosyncratic domain knowledge\nDissertation: Cold War \\(\\times\\) ‚ÄúThird World‚Äù \\(\\leadsto\\) Cuban üá®üá∫ trans-continental operations1\nMain narrative (for estimation): 1975 (South Africa invades Angola, 14 Oct ‚Üí üá®üá∫ intervention, 4 Nov) to 1979 (USSR requests üá®üá∫ troops to Ethiopia for Ogaden War)\n[Ontology] Fix 1979 geographic entities at National level (as modeling choice, like fixing 2000 USD to measure inflation): \\(\\textsf{Cuba}_{1979}\\), \\(\\textsf{Angola}_{1979}\\), \\(\\textsf{PDRY}_{1979}\\), \\(\\textsf{YAR}_{1979}\\)\nDifferent tokens (Think NLP: \"Congo\", \"DRC\", \"Republic of Congo\") can then be contextualized: can ‚Äútrack‚Äù and link data appropriately despite splits, merges, name changes\nSay we have data on ‚ÄúNumber of Communist Militants in \\(X\\)‚Äù (Hoover Yearbook)‚Ä¶\n\n\n\n\nEntity\nData from 1947-1971 at...\nData from 1971-Present at...\n\n\n\n\n\\(\\textsf{Pakistan}_{1979}\\)\nNational Level:\n\\(\\frac{62}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúPakistan‚Äù\n\n\nSubnational Level:\n‚ÄúWest Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)\n\n\n\\(\\textsf{Bangladesh}_{1979}\\)\nNational Level:\n\\(\\frac{70}{62+70} \\times\\) ‚ÄúPakistan‚Äù\nNational Level:\n‚ÄúBangladesh‚Äù\n\n\nSubnational Level:\n‚ÄúEast Pakistan‚Äù\nSubnational Level:\n\\(\\sum_{i \\in \\text{Regions}}\\text{data}_i\\)",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#the-fork-x-leftarrow-z-rightarrow-y",
    "href": "w04/index.html#the-fork-x-leftarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)",
    "text": "The Fork: \\(X \\leftarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\nfork_df &lt;- tibble(\n    Z = rbern(n_d),\n    X = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\nplot_freqs &lt;- function(df, plot_title, y_lab=TRUE) {\n  df_cor &lt;- cor(df$X, df$Y)\n  df_label &lt;- paste0(\"Cor(X,Y) = \",round(df_cor,3))\n  freq_df &lt;- df |&gt;\n    group_by(X, Y) |&gt;\n    summarize(count=n())\n  freq_plot &lt;- freq_df |&gt;\n    ggplot(\n      aes(x=factor(X), y=factor(Y), fill=count)\n    ) +\n    geom_tile() +\n    coord_equal() +\n    scale_fill_distiller(\n      palette=\"Greens\", direction=1,\n      limits=c(0,5000)\n    ) +\n    geom_label(\n      aes(label=count),\n      fill=\"white\", color=\"black\", size=7\n    ) +\n    labs(\n      title = plot_title,\n      subtitle = df_label,\n      x=\"X\", y=\"Y\"\n    ) +\n    theme_dsan(base_size=24) +\n    theme(\n      plot.title = element_text(size=21),\n      plot.subtitle = element_text(size=18)\n    ) +\n    remove_legend()\n  if (!y_lab) {\n    freq_plot &lt;- freq_plot + theme(\n      axis.title.y = element_blank()\n    )\n  }\n  return(freq_plot)\n}\n# The full df\nfull_label &lt;- paste0(\"Raw Data (n = 10K)\")\nfull_plot &lt;- plot_freqs(fork_df, full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 0\nz0_df &lt;- fork_df |&gt; filter(Z == 0)\nz0_n &lt;- nrow(z0_df)\nz0_label &lt;- paste0(\"Z == 0 (\",z0_n,\" obs)\")\nz0_plot &lt;- plot_freqs(z0_df, z0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 1\nz1_df &lt;- fork_df |&gt; filter(Z == 1)\nz1_n &lt;- nrow(z1_df)\nz1_label &lt;- paste0(\"Z == 1 (\",z1_n,\" obs)\")\nz1_plot &lt;- plot_freqs(z1_df, z1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\nfull_plot | z0_plot | z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncfork_df &lt;- tibble(\n    Z = rbern(n_c),\n    X = rnorm(n_c, 2 * Z - 1),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\nlibrary(latex2exp)\noverall_lm &lt;- lm(Y ~ X, data=cfork_df)\noverall_slope &lt;- round(overall_lm$coef['X'], 3)\nz0_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 0))\nz0_slope &lt;- round(z0_lm$coef['X'], 2)\nz0_label &lt;- paste0(\"$Slope_{Z=0} = \",z0_slope,\"$\")\nz0_leg_label &lt;- TeX(paste0(\"0 $(m=\",z0_slope,\")$\"))\nz1_lm &lt;- lm(Y ~ X, data=cfork_df |&gt; filter(Z == 1))\nz1_slope &lt;- round(z1_lm$coef['X'], 2)\nz1_label &lt;- paste0(\"$Slope_{Z=1} = \",z1_slope,\"$\")\nz_texlabel &lt;- TeX(paste0(z0_label, \" | \", z1_label))\ncfork_xmin &lt;- min(cfork_df$X)\ncfork_xmax &lt;- max(cfork_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cfork_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cfork_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",overall_slope\n    ),\n    subtitle=z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "href": "w04/index.html#the-pipe-x-rightarrow-z-rightarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)",
    "text": "The Pipe: \\(X \\rightarrow Z \\rightarrow Y\\)\n\n\n\n\nCode\nset.seed(5650)\npipe_df &lt;- tibble(\n    X = rbern(n_d),\n    Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n    Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n)\n\n\n\n\nCode\n# The full df\npipe_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\npipe_full_plot &lt;- plot_freqs(pipe_df, pipe_full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 0\npipe_z0_df &lt;- pipe_df |&gt; filter(Z == 0)\npipe_z0_n &lt;- nrow(pipe_z0_df)\npipe_z0_label &lt;- paste0(\"Z == 0 (\",pipe_z0_n,\" obs)\")\npipe_z0_plot &lt;- plot_freqs(pipe_z0_df, pipe_z0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 1\npipe_z1_df &lt;- pipe_df |&gt; filter(Z == 1)\npipe_z1_n &lt;- nrow(pipe_z1_df)\npipe_z1_label &lt;- paste0(\"Z == 1 (\",pipe_z1_n,\" obs)\")\npipe_z1_plot &lt;- plot_freqs(pipe_z1_df, pipe_z1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\npipe_full_plot | pipe_z0_plot | pipe_z1_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(5650)\ncpipe_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1)\n)\n\n\n\n\nCode\ncpipe_lm &lt;- lm(Y ~ X, data=cpipe_df)\ncpipe_slope &lt;- round(cpipe_lm$coef['X'], 3)\ncpipe_z0_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 0))\ncpipe_z0_slope &lt;- round(cpipe_z0_lm$coef['X'], 2)\ncpipe_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",cpipe_z0_slope,\"$\")\ncpipe_z1_lm &lt;- lm(Y ~ X, data=cpipe_df |&gt; filter(Z == 1))\ncpipe_z1_slope &lt;- round(cpipe_z1_lm$coef['X'], 2)\ncpipe_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",cpipe_z1_slope,\"$\")\ncpipe_z_texlabel &lt;- TeX(paste0(cpipe_z0_label, \" | \", cpipe_z1_label))\ncpipe_xmin &lt;- min(cpipe_df$X)\ncpipe_xmax &lt;- max(cpipe_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cpipe_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cpipe_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cpipe_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cpipe_slope\n    ),\n    subtitle=cpipe_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#the-collider-x-rightarrow-z-leftarrow-y",
    "href": "w04/index.html#the-collider-x-rightarrow-z-leftarrow-y",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)",
    "text": "‚ö†Ô∏èThe Collider‚ö†Ô∏è: \\(X \\rightarrow Z \\leftarrow Y\\)\n\n\n\n\n\nCode\nset.seed(5650)\ncoll_df &lt;- tibble(\n    X = rbern(n_d),\n    Y = rbern(n_d),\n    Z = rbern(n_d, ifelse(X + Y &gt; 0, 0.9, 0.2)),\n)\n\n\n\n\nCode\n# The full df\ncoll_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\ncoll_full_plot &lt;- plot_freqs(coll_df, coll_full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 0\ncoll_z0_df &lt;- coll_df |&gt; filter(Z == 0)\ncoll_z0_n &lt;- nrow(coll_z0_df)\ncoll_z0_label &lt;- paste0(\"Z == 0 (\",coll_z0_n,\" obs)\")\ncoll_z0_plot &lt;- plot_freqs(coll_z0_df, coll_z0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on Z = 1\ncoll_z1_df &lt;- coll_df |&gt; filter(Z == 1)\ncoll_z1_n &lt;- nrow(coll_z1_df)\ncoll_z1_label &lt;- paste0(\"Z == 1 (\",coll_z1_n,\" obs)\")\ncoll_z1_plot &lt;- plot_freqs(coll_z1_df, coll_z1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\ncoll_full_plot | coll_z0_plot | coll_z1_plot\n\n\n\n\n\n\n\n\n\n\nConditioning on colliders induces correlation where there previously was none ‚ò†Ô∏è\n\n\n\n\n\nCode\nset.seed(5650)\nccoll_df &lt;- tibble(\n    X = rnorm(n_c),\n    Y = rnorm(n_c),\n    Z = rbern(n_c, plogis(2 * (X + Y - 1)))\n)\n\n\n\n\nCode\nccoll_lm &lt;- lm(Y ~ X, data=ccoll_df)\nccoll_slope &lt;- round(ccoll_lm$coef['X'], 3)\nccoll_z0_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 0))\nccoll_z0_slope &lt;- round(ccoll_z0_lm$coef['X'], 2)\nccoll_z0_label &lt;- paste0(\"$Slope_{Z=0} = \",ccoll_z0_slope,\"$\")\nccoll_z1_lm &lt;- lm(Y ~ X, data=ccoll_df |&gt; filter(Z == 1))\nccoll_z1_slope &lt;- round(ccoll_z1_lm$coef['X'], 2)\nccoll_z1_label &lt;- paste0(\"$Slope_{Z=1} = \",ccoll_z1_slope,\"$\")\nccoll_z_texlabel &lt;- TeX(paste0(ccoll_z0_label, \" | \", ccoll_z1_label))\nccoll_xmin &lt;- min(ccoll_df$X)\nccoll_xmax &lt;- max(ccoll_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=ccoll_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(Z)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=ccoll_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, group=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=ccoll_df,\n    aes(x=X, y=Y, color=factor(Z)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=24) +\n  theme(\n    plot.title = element_text(size=24),\n    plot.subtitle = element_text(size=20)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",ccoll_slope\n    ),\n    subtitle=ccoll_z_texlabel,\n    x = \"X\", y = \"Y\", color = \"Z\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶This is why we have to think, rather than just ‚Äúcontrol for everything‚Äù! üò≠",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#proxies-for-z",
    "href": "w04/index.html#proxies-for-z",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Proxies for \\(Z\\)",
    "text": "Proxies for \\(Z\\)\n\n\n\n\n\nCode\nset.seed(5650)\nprox_df &lt;- tibble(\n  X = rbern(n_d),\n  Z = rbern(n_d, (1-X)*0.1 + X*0.9),\n  Y = rbern(n_d, (1-Z)*0.1 + Z*0.9),\n  A = rbern(n_d, (1-Z)*0.1 + Z*0.9)\n)\n\n\n\n\nCode\n# The full df\nprox_full_label &lt;- paste0(\"Raw Data (n = 10K)\")\nprox_full_plot &lt;- plot_freqs(prox_df, prox_full_label)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on A == 0\nprox_a0_df &lt;- prox_df |&gt; filter(A == 0)\nprox_a0_n &lt;- nrow(prox_a0_df)\nprox_a0_label &lt;- paste0(\"A == 0 (\",prox_a0_n,\" obs)\")\nprox_a0_plot &lt;- plot_freqs(prox_a0_df, prox_a0_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\n# Conditioning on A == 1\nprox_a1_df &lt;- prox_df |&gt; filter(A == 1)\nprox_a1_n &lt;- nrow(prox_a1_df)\nprox_a1_label &lt;- paste0(\"A == 1 (\",prox_a1_n,\" obs)\")\nprox_a1_plot &lt;- plot_freqs(prox_a1_df, prox_a1_label, y_lab=FALSE)\n\n\n`summarise()` has grouped output by 'X'. You can override using the `.groups`\nargument.\n\n\nCode\nprox_full_plot | prox_a0_plot | prox_a1_plot\n\n\n\n\n\n\n\n\n\n\nWith just \\(X \\rightarrow Z \\rightarrow Y\\), we‚Äôd have a pipe\nObserving \\(A\\) gives us some (not all!) information about \\(Z\\)\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(extraDistr)\nlibrary(latex2exp)\nset.seed(5650)\nn_c &lt;- 300\ncprox_df &lt;- tibble(\n    X = rnorm(n_c),\n    Z = rbern(n_c, plogis(X)),\n    Y = rnorm(n_c, 2 * Z - 1),\n    A = rbern(n_c, (1-Z)*0.86 + Z*0.14)\n)\ncprox_lm &lt;- lm(Y ~ X, data=cprox_df)\ncprox_slope &lt;- round(cprox_lm$coef['X'], 3)\ncprox_a0_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 0))\ncprox_a0_slope &lt;- round(cprox_a0_lm$coef['X'], 2)\ncprox_a0_label &lt;- paste0(\"$Slope_{A=0} = \",cprox_a0_slope,\"$\")\n# A == 1 lm\ncprox_a1_lm &lt;- lm(Y ~ X, data=cprox_df |&gt; filter(A == 1))\ncprox_a1_slope &lt;- round(cprox_a1_lm$coef['X'], 2)\ncprox_a1_label &lt;- paste0(\"$Slope_{A=1} = \",cprox_a1_slope,\"$\")\ncprox_a_texlabel &lt;- TeX(paste0(cprox_a0_label, \" | \", cprox_a1_label))\ncprox_xmin &lt;- min(cprox_df$X)\ncprox_xmax &lt;- max(cprox_df$X)\nggplot() +\n  # Points\n  geom_point(\n    data=cprox_df |&gt; filter(Y &gt; -3),\n    aes(x=X, y=Y, color=factor(A)),\n    size=0.6*g_pointsize,\n    alpha=0.8\n  ) +\n  # Overall lm\n  geom_smooth(\n    data=cprox_df, aes(x=X, y=Y),\n    method = lm, se = FALSE,\n    linewidth = 2.5, color='black'\n  ) +\n  # Stratified lm\n  # (slightly larger black lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, group=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2.75, color='black'\n  ) +\n  # (Colored lines)\n  geom_smooth(\n    data=cprox_df,\n    aes(x=X, y=Y, color=factor(A)),\n    method=lm, se=FALSE, fullrange=TRUE,\n    linewidth=2\n  ) +\n  theme_dsan(base_size=22) +\n  theme(\n    plot.title = element_text(size=22),\n    plot.subtitle = element_text(size=18)\n  ) +\n  coord_equal() +\n  labs(\n    title = paste0(\n      \"Unstratified Slope = \",cprox_slope\n    ),\n    subtitle=cprox_a_texlabel,\n    x = \"X\", y = \"Y\", color = \"A\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#references",
    "href": "w04/index.html#references",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "References",
    "text": "References\n\n\nBezuidenhout, Dana, Sarah Forthal, Kara Rudolph, and Matthew R Lamb. 2024. ‚ÄúSingle World Intervention Graphs (SWIGs): A Practical Guide.‚Äù American Journal of Epidemiology, September, kwae353. https://doi.org/10.1093/aje/kwae353.\n\n\nGleijeses, Piero. 2013. Visions of Freedom: Havana, Washington, Pretoria, and the Struggle for Southern Africa, 1976-1991. UNC Press Books.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and STAN. CRC Press.",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w04/index.html#footnotes",
    "href": "w04/index.html#footnotes",
    "title": "Week 4: Clearing the Path from Cause to Effect",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHelpful metaphor (Gleijeses 2013): Cuba \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for USSR (Soviet $ but Cuban training of PAIGC ‚Üí MPLA), as Israel \\(\\approx\\) Forward-deployed ‚Äú3rd World Outpost‚Äù for US (US $ but Israeli training of SAVAK ‚Üí SADF)‚Ü©Ô∏é",
    "crumbs": [
      "Week 4: {{< var w04.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html",
    "href": "w03/index.html",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#so-whats-the-problem",
    "href": "w03/index.html#so-whats-the-problem",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-fundamental-problem-of-causal-inference",
    "href": "w03/index.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#what-is-to-be-done",
    "href": "w03/index.html#what-is-to-be-done",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#probability",
    "href": "w03/index.html#probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring.",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#beyond-conditional-probability",
    "href": "w03/index.html#beyond-conditional-probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#preview-do-calculus",
    "href": "w03/index.html#preview-do-calculus",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w03/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#importance-of-observed-vs.-latent-distinction",
    "href": "w03/index.html#importance-of-observed-vs.-latent-distinction",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Importance of Observed vs.¬†Latent Distinction!",
    "text": "Importance of Observed vs.¬†Latent Distinction!\n\nAcross many different fields, hidden stumbling-block in your project may be failure to model this distinction and pursue its implications!\n\n\n\n\n\n\n\n\n\nFigure¬†1: Your model failing to achieve its goal bc you haven‚Äôt yet distinguished observed vs.¬†latent variables",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#example-from-cognitive-neuroscience-visual-perception",
    "href": "w03/index.html#example-from-cognitive-neuroscience-visual-perception",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Example from Cognitive Neuroscience: Visual Perception",
    "text": "Example from Cognitive Neuroscience: Visual Perception\n\n\n\n\n\n\n\nWe ‚Äúsee‚Äù 3D objects like a basketballs, but our eyes are (curved) 2D surfaces!\n\\(\\Rightarrow\\) Our brains construct 3D environment by combining 2D info (observed photons-hitting-light-cones) with latent heuristic info:\n\nInstantaneous Binocular Disparity, fusing info from two slightly-offset eyes,\nShort-term Motion Parallax: How does object shift over short temporal ‚Äúwindows‚Äù of movement?\nLong-term mental models (orange-ish circle with this line pattern is usually a basketball, which is usually this big, etc.)\n\n\n\n\n\n\n\nImage source (a very cool article!)\n\n\n\n\n\n\nSimilar examples in many other fields \\(\\leadsto\\) science is a strange waltz of general models vs.¬†field-specific details, but there‚Äôs one model that is infinitely helpful imo‚Ä¶",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "href": "w03/index.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!",
    "text": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!\n\nUsing ‚ÄúUr‚Äù in the same sense as ‚ÄúAmerica‚Äôs Ur-Choropleths‚Äù‚Ä¶\nHMMs are our ‚ÄúUr-Models‚Äù for Computational Social Science specifically\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs consider an extremely currently-popular strand of CSS research, and step through why (a) it may be harder than it initially seems, but (b) we can use HMMs to ‚Äúorganize‚Äù/manage/visualize the complexity!\n\n\nAs in, how America‚Äôs Ur-Choropleths are two visualizations you can keep at hand before launching into a specific nation-wide choropleth about a specific issue!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#studying-fake-news",
    "href": "w03/index.html#studying-fake-news",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Studying ‚ÄúFake News‚Äù",
    "text": "Studying ‚ÄúFake News‚Äù\n\n\n\n\n\n\n\n\n\n\nStudying ‚Äúfake news‚Äù with ML and/or Deep Learning and/or Big Data is very popular in Computational Social Science: let‚Äôs use HMMs to see why it might be more‚Ä¶ difficult/complicated than it seems at first üôà\n\n\n\n\n\nThe (implicit) model in studies like Iyengar and Kinder (2010) is something like:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThus allowing results to be summarized in a table like:\n\n\n\n\n\n\n\n\n\n\n\nIyengar and Kinder (2010)",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-devil-in-the-details-i",
    "href": "w03/index.html#the-devil-in-the-details-i",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details I",
    "text": "The Devil in the Details I\n\nResidents of the New Haven, Connecticut area participated in one of two experiments, each of which spanned six consecutive days [‚Ä¶] took place in November 1980, shortly after the presidential election\n\n\nWe measured problem importance with four questions that appeared in both the pretreatment and posttreatment questionnaires:\n\nPlease indicate how important you consider these problems to be.\nShould the federal government do more to develop solutions to these problems, even if it means raising taxes?\nHow much do you yourself care about these problems?\nThese days how much do you talk about these problems?",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-devil-in-the-details-ii",
    "href": "w03/index.html#the-devil-in-the-details-ii",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details II",
    "text": "The Devil in the Details II",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#randomization-and-fine-tuned-treatment",
    "href": "w03/index.html#randomization-and-fine-tuned-treatment",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Randomization and Fine-Tuned Treatment",
    "text": "Randomization and Fine-Tuned Treatment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶These are the types of things we usually don‚Äôt have control over as data scientists (we‚Äôre just handed a .csv!)",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#lets-model-it",
    "href": "w03/index.html#lets-model-it",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Let‚Äôs Model It!",
    "text": "Let‚Äôs Model It!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#the-final-piece-plate-notation",
    "href": "w03/index.html#the-final-piece-plate-notation",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Final Piece: Plate Notation",
    "text": "The Final Piece: Plate Notation\n\nFor describing general distributions, there is often a ‚Äúsingle node generating a bunch of nodes‚Äù structure:\n\n\n\n\n\n\n\nPGM notation has a built-in tool for this: plates!",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#crucial-css-model-we-can-now-dive-into",
    "href": "w03/index.html#crucial-css-model-we-can-now-dive-into",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Crucial CSS Model We Can Now Dive Into!",
    "text": "Crucial CSS Model We Can Now Dive Into!\n\n\n\nImage source",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#what-does-this-give-us",
    "href": "w03/index.html#what-does-this-give-us",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Does This Give Us?",
    "text": "What Does This Give Us?",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#before-we-branch-off-of-pgms",
    "href": "w03/index.html#before-we-branch-off-of-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Before We Branch Off Of PGMs",
    "text": "Before We Branch Off Of PGMs\n(Even in non-causal settings)\n\n\n\nImage source\n\n\n\nWe don‚Äôt exactly think ‚ÄúShakespeare decided on a set of topics,‚Äù",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#getting-shot-by-a-firing-squad",
    "href": "w03/index.html#getting-shot-by-a-firing-squad",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Getting Shot By A Firing Squad",
    "text": "Getting Shot By A Firing Squad\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(O\\)\n\\(C\\)\n\\(A\\)\n\\(B\\)\n\\(D\\)\n\n\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n\n\\[\n\\begin{align*}\n\\Pr(D) &= 0.4 \\\\\n\\Pr(D \\mid A) &= 1\n\\end{align*}\n\\]",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#references",
    "href": "w03/index.html#references",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nIyengar, Shanto, and Donald R. Kinder. 2010. News That Matters: Television & American Opinion. University of Chicago Press.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1.",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/index.html#footnotes",
    "href": "w03/index.html#footnotes",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Appendix Slide‚Ü©Ô∏é",
    "crumbs": [
      "Week 3: {{< var w03.date-md >}}"
    ]
  },
  {
    "objectID": "w03/slides.html#so-whats-the-problem",
    "href": "w03/slides.html#so-whats-the-problem",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often\n\nSee Appendix Slide"
  },
  {
    "objectID": "w03/slides.html#the-fundamental-problem-of-causal-inference",
    "href": "w03/slides.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠"
  },
  {
    "objectID": "w03/slides.html#what-is-to-be-done",
    "href": "w03/slides.html#what-is-to-be-done",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?"
  },
  {
    "objectID": "w03/slides.html#probability",
    "href": "w03/slides.html#probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring."
  },
  {
    "objectID": "w03/slides.html#beyond-conditional-probability",
    "href": "w03/slides.html#beyond-conditional-probability",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects"
  },
  {
    "objectID": "w03/slides.html#preview-do-calculus",
    "href": "w03/slides.html#preview-do-calculus",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events"
  },
  {
    "objectID": "w03/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w03/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!"
  },
  {
    "objectID": "w03/slides.html#importance-of-observed-vs.-latent-distinction",
    "href": "w03/slides.html#importance-of-observed-vs.-latent-distinction",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Importance of Observed vs.¬†Latent Distinction!",
    "text": "Importance of Observed vs.¬†Latent Distinction!\n\nAcross many different fields, hidden stumbling-block in your project may be failure to model this distinction and pursue its implications!\n\n\n\n\n\n\n\n\n\nFigure¬†1: Your model failing to achieve its goal bc you haven‚Äôt yet distinguished observed vs.¬†latent variables"
  },
  {
    "objectID": "w03/slides.html#example-from-cognitive-neuroscience-visual-perception",
    "href": "w03/slides.html#example-from-cognitive-neuroscience-visual-perception",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Example from Cognitive Neuroscience: Visual Perception",
    "text": "Example from Cognitive Neuroscience: Visual Perception\n\n\n\n\n\n\n\nWe ‚Äúsee‚Äù 3D objects like a basketballs, but our eyes are (curved) 2D surfaces!\n\\(\\Rightarrow\\) Our brains construct 3D environment by combining 2D info (observed photons-hitting-light-cones) with latent heuristic info:\n\nInstantaneous Binocular Disparity, fusing info from two slightly-offset eyes,\nShort-term Motion Parallax: How does object shift over short temporal ‚Äúwindows‚Äù of movement?\nLong-term mental models (orange-ish circle with this line pattern is usually a basketball, which is usually this big, etc.)\n\n\n\n\n\n\n\nImage source (a very cool article!)\n\n\n\n\n\n\nSimilar examples in many other fields \\(\\leadsto\\) science is a strange waltz of general models vs.¬†field-specific details, but there‚Äôs one model that is infinitely helpful imo‚Ä¶"
  },
  {
    "objectID": "w03/slides.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "href": "w03/slides.html#hidden-markov-models-hmms-are-our-ur-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!",
    "text": "Hidden Markov Models (HMMs) Are Our Ur-PGMs!\n\nUsing ‚ÄúUr‚Äù in the same sense as ‚ÄúAmerica‚Äôs Ur-Choropleths‚Äù‚Ä¶\nHMMs are our ‚ÄúUr-Models‚Äù for Computational Social Science specifically\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs consider an extremely currently-popular strand of CSS research, and step through why (a) it may be harder than it initially seems, but (b) we can use HMMs to ‚Äúorganize‚Äù/manage/visualize the complexity!\n\n\nAs in, how America‚Äôs Ur-Choropleths are two visualizations you can keep at hand before launching into a specific nation-wide choropleth about a specific issue!"
  },
  {
    "objectID": "w03/slides.html#studying-fake-news",
    "href": "w03/slides.html#studying-fake-news",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Studying ‚ÄúFake News‚Äù",
    "text": "Studying ‚ÄúFake News‚Äù\n\n\n\n\n\n\n\nStudying ‚Äúfake news‚Äù with ML and/or Deep Learning and/or Big Data is very popular in Computational Social Science: let‚Äôs use HMMs to see why it might be more‚Ä¶ difficult/complicated than it seems at first üôà\n\n\n\n\n\nThe (implicit) model in studies like Iyengar and Kinder (2010) is something like:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThus allowing results to be summarized in a table like:\n\n\n\n\n\n\n\n\n\n\n\nIyengar and Kinder (2010)"
  },
  {
    "objectID": "w03/slides.html#the-devil-in-the-details-i",
    "href": "w03/slides.html#the-devil-in-the-details-i",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details I",
    "text": "The Devil in the Details I\n\nResidents of the New Haven, Connecticut area participated in one of two experiments, each of which spanned six consecutive days [‚Ä¶] took place in November 1980, shortly after the presidential election\n\n\nWe measured problem importance with four questions that appeared in both the pretreatment and posttreatment questionnaires:\n\nPlease indicate how important you consider these problems to be.\nShould the federal government do more to develop solutions to these problems, even if it means raising taxes?\nHow much do you yourself care about these problems?\nThese days how much do you talk about these problems?"
  },
  {
    "objectID": "w03/slides.html#the-devil-in-the-details-ii",
    "href": "w03/slides.html#the-devil-in-the-details-ii",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Devil in the Details II",
    "text": "The Devil in the Details II"
  },
  {
    "objectID": "w03/slides.html#randomization-and-fine-tuned-treatment",
    "href": "w03/slides.html#randomization-and-fine-tuned-treatment",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Randomization and Fine-Tuned Treatment",
    "text": "Randomization and Fine-Tuned Treatment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Ä¶These are the types of things we usually don‚Äôt have control over as data scientists (we‚Äôre just handed a .csv!)"
  },
  {
    "objectID": "w03/slides.html#lets-model-it",
    "href": "w03/slides.html#lets-model-it",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Let‚Äôs Model It!",
    "text": "Let‚Äôs Model It!"
  },
  {
    "objectID": "w03/slides.html#the-final-piece-plate-notation",
    "href": "w03/slides.html#the-final-piece-plate-notation",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "The Final Piece: Plate Notation",
    "text": "The Final Piece: Plate Notation\n\nFor describing general distributions, there is often a ‚Äúsingle node generating a bunch of nodes‚Äù structure:\n\n\n\n\n\n\n\nPGM notation has a built-in tool for this: plates!"
  },
  {
    "objectID": "w03/slides.html#crucial-css-model-we-can-now-dive-into",
    "href": "w03/slides.html#crucial-css-model-we-can-now-dive-into",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Crucial CSS Model We Can Now Dive Into!",
    "text": "Crucial CSS Model We Can Now Dive Into!\n\nImage source"
  },
  {
    "objectID": "w03/slides.html#what-does-this-give-us",
    "href": "w03/slides.html#what-does-this-give-us",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "What Does This Give Us?",
    "text": "What Does This Give Us?"
  },
  {
    "objectID": "w03/slides.html#before-we-branch-off-of-pgms",
    "href": "w03/slides.html#before-we-branch-off-of-pgms",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Before We Branch Off Of PGMs",
    "text": "Before We Branch Off Of PGMs\n(Even in non-causal settings)\n\nImage source\nWe don‚Äôt exactly think ‚ÄúShakespeare decided on a set of topics,‚Äù"
  },
  {
    "objectID": "w03/slides.html#getting-shot-by-a-firing-squad",
    "href": "w03/slides.html#getting-shot-by-a-firing-squad",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "Getting Shot By A Firing Squad",
    "text": "Getting Shot By A Firing Squad\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(O\\)\n\\(C\\)\n\\(A\\)\n\\(B\\)\n\\(D\\)\n\n\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n1\n\n\n\n\\[\n\\begin{align*}\n\\Pr(D) &= 0.4 \\\\\n\\Pr(D \\mid A) &= 1\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w03/slides.html#references",
    "href": "w03/slides.html#references",
    "title": "Week 3: From PGMs to Causal Diagrams",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nIyengar, Shanto, and Donald R. Kinder. 2010. News That Matters: Television & American Opinion. University of Chicago Press.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignment Point Distributions",
    "section": "",
    "text": "Use the tabs below to view the point distributions for different assignments.\nThe distributions are imported from Google Sheets mainly for transparency: so that you can see exactly how totals are computed as a sum of the individual points allocated for each test!\n\nHW1\n\n\n\n\n\n\n\n\n\npart\nqid\npoints\nq_total\npart_total\n\n\n\n\n1\nQ1.1a\n4\n\n\n\n\n\nQ1.1b\n2\n6\n\n\n\n\nQ1.2\n8\n8\n\n\n\n\nQ1.3a\n3\n\n\n\n\n\nQ1.3b\n3\n\n\n\n\n\nQ1.3c\n3\n9\n\n\n\n\nQ1.4a\n3\n\n\n\n\n\nQ1.4b\n3\n6\n29\n\n\n2\nQ2.1\n4\n\n\n\n\n\nQ2.2\n4\n\n\n\n\n\nQ2.3\n4\n\n\n\n\n\nQ2.4\n4\n\n16\n\n\n3\nQ3.1\n4\n\n\n\n\n\nQ3.2\n4\n\n\n\n\n\nQ3.3\n4\n\n12\n\n\n4\nQ4.1\n3\n3\n\n\n\n\nQ4.2a\n4\n\n\n\n\n\nQ4.2b\n2\n6\n\n\n\n\nQ4.3a\n2\n\n\n\n\n\nQ4.3b\n2\n4\n\n\n\n\nQ4.4a\n2\n\n\n\n\n\nQ4.4b\n2\n4\n\n\n\n\nQ4.5a\n2\n\n\n\n\n\nQ4.5b\n2\n4\n\n\n\n\nQ4.6a\n2\n\n\n\n\n\nQ4.6b\n2\n4\n25\n\n\n5\nQ5.1\n4\n4\n\n\n\n\nQ5.2\n2\n2\n\n\n\n\nQ5.3\n2\n\n\n\n\n\nQ5.4a\n4\n\n\n\n\n\nQ5.4b\n3\n\n\n\n\n\nQ5.4c\n3\n10\n18\n\n\n\nTotal\n100",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#courtney-green",
    "href": "w02/index.html#courtney-green",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Courtney Green",
    "text": "Courtney Green\nBackground: BA in Public Policy & Leadership, now interested in using data to examine structural disparities.\nInterests & what I can help with: How sociodemographic factors (e.g., race, gender, income, immigration status, education level) shape policy outcomes and institutional practices in areas like criminal justice, housing, healthcare, education, and environment.\n\nAsk me about: Counterfactual balancing, handling messy or privacy-limited datasets, and framing causal questions around inequality.\nReach out if you‚Äôre thinking about a final project that touches social systems, systemic inequity, or fairness!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#what-got-me-interested-in-causal-inference",
    "href": "w02/index.html#what-got-me-interested-in-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Got Me Interested in Causal Inference",
    "text": "What Got Me Interested in Causal Inference\n5000 Project: Over-Policing and Wrongful Convictions in Illinois\n\nMotivation: Explore what demographic factors predict wrongful convictions using real exoneration data.\nChallenge: No data on ‚Äúnon-exonerated innocents,‚Äù so I (with prof‚Äôs help) created a counterfactual dataset by sampling from the incarcerated population\nMethod: Trained models (logistic regression, random forest, KNN) on a balanced dataset of 548 exonerated and 548 non-exonerated individuals\nFinding: Race and geography (esp.¬†Cook Count/Chicago) were the strongest predictors of exoneration\nTakeaway: Questions about systemic bias can‚Äôt be answered with just descriptive stats, you need counterfactual reasoning and causal inference\n\n(Optional) View the counterfactual setup",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#wendy-hu",
    "href": "w02/index.html#wendy-hu",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Wendy Hu",
    "text": "Wendy Hu\n\nBackground: BS in Social Work, passionate about advancing social equity through computational tools\nInterest: I bring a background in social welfare, public health, and NGO data, with experience analyzing behavioral and institutional outcomes. My work spans Indigenous health, gender equity, and trauma-informed service design‚Äîalways with a focus on equity, system change, and policy relevance.\nAsk me about: Framing social justice questions in data terms, working with community or clinical datasets and interpreting model results in context.\nIf you‚Äôre designing a project around health, inequality, nonprofit impact, or any socially embedded issue‚ÄîI‚Äôd love to help bridge rigor and relevance.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#jeffs-hw1-updateapology",
    "href": "w02/index.html#jeffs-hw1-updateapology",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Jeff‚Äôs HW1 Update/Apology",
    "text": "Jeff‚Äôs HW1 Update/Apology\n\nBasically‚Ä¶ I goofed by trying to shove a homework into the first two weeks of class üò≠\nLast week was ‚Äúsetting the table‚Äù, so there are a few (multiple-choice) questions on that\nBut, really only makes sense to have HW starting from end of today, once we‚Äôve introduced PGMs, the language we need to actually do Causal Computational Social Science!\n\n\n Due date pushed to Friday, June 6, 5:59pm",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#hw1-detail-plz-dont-hate-me",
    "href": "w02/index.html#hw1-detail-plz-dont-hate-me",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)",
    "text": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)\n(The main reason this is taking me so long)\n\nTwo key libraries for ‚Äúmain‚Äù course content (HW2 onwards):\n\nIn R: rethinking, ‚Äúlite‚Äù version of Stan\nIn Python: PyMC\n\nBoth are overkill for current unit: lots of work required to ‚Äúturn off‚Äù fancy features and implement basic PGMs\n\\(\\leadsto\\) Only way I know to achieve two goals:\n\n Learning simple PGMs first, as tools for thinking, before adding in additional full-on coding baggage of Stan/PyMC\n Having an entire textbook on these base PGMs that you can use as reference\n\nIs to use [small, restricted] subset of JavaScript, called WebPPL, because‚Ä¶\n\n\nhttp://probmods.org/",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "href": "w02/index.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality",
    "text": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality\n\n\n You‚Äôll no longer be able to read ‚Äúscientific‚Äù writing without striking this expression (involuntarily):\n\n ‚ÄúScientific‚Äù talks will begin to sound like the following:",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#blasting-off-into-causality",
    "href": "w02/index.html#blasting-off-into-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Blasting Off Into Causality!",
    "text": "Blasting Off Into Causality!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#data-generating-processes-dgps",
    "href": "w02/index.html#data-generating-processes-dgps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Data-Generating Processes (DGPs)",
    "text": "Data-Generating Processes (DGPs)\n\n\n\n\n\n\n\nYou saw this in DSAN 5100!\n¬´\\(X_1, \\ldots, X_n\\) drawn i.i.d. Normal, mean \\(\\mu\\) variance \\(\\sigma^2\\)¬ª characterizes DGP of \\((X_1, \\ldots, X_n)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n5650: Dive into DGPs, rather than treating as black box/footnote to Law of Large Numbers, so we can move [asymptotically!]‚Ä¶\nFrom associational statements:\n\n¬´\\(\\underbrace{\\text{An increase}}_{\\small\\text{noun}}\\) in \\(X\\) by 1 is associated with increase in \\(Y\\) by \\(\\beta\\)¬ª\n\nTo causal ones: ¬´\\(\\underbrace{\\text{Increasing}}_{\\small\\text{verb}}\\) \\(X\\) by 1 causes \\(Y\\) to increase by \\(\\beta\\)¬ª",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#dgps-and-the-emergence-of-order",
    "href": "w02/index.html#dgps-and-the-emergence-of-order",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "DGPs and the Emergence of Order",
    "text": "DGPs and the Emergence of Order\n\n\n\n\n\n\n\nWho can guess the state of this process after 10 steps, with 1 person?\n10 people? 50? 100? (If they find themselves on the same spot, they stand on each other‚Äôs heads)\n100 steps? 1000?",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-result-16-steps",
    "href": "w02/index.html#the-result-16-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 16 Steps",
    "text": "The Result: 16 Steps",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-result-64-steps",
    "href": "w02/index.html#the-result-64-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 64 Steps",
    "text": "The Result: 64 Steps",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#mathematicalscientific-modeling",
    "href": "w02/index.html#mathematicalscientific-modeling",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "‚ÄúMathematical/Scientific Modeling‚Äù",
    "text": "‚ÄúMathematical/Scientific Modeling‚Äù\n\nThing we observe (poking out of water): data\nHidden but possibly discoverable via deeper dive (ecosystem under surface): DGP",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#so-whats-the-problem",
    "href": "w02/index.html#so-whats-the-problem",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-shallow-problem-of-causal-inference",
    "href": "w02/index.html#the-shallow-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Shallow Problem of Causal Inference",
    "text": "The Shallow Problem of Causal Inference\n\n\n\n\n\n\n\n\ncor(ski_df$value, law_df$value)\n[1] 0.9921178\n\n(Data from Vigen, Spurious Correlations)\n\nThis, however, is only a mini-boss. Beyond it lies the truly invincible FINAL BOSS‚Ä¶ üôÄ",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#the-fundamental-problem-of-causal-inference",
    "href": "w02/index.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#what-is-to-be-done",
    "href": "w02/index.html#what-is-to-be-done",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#probability",
    "href": "w02/index.html#probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#beyond-conditional-probability",
    "href": "w02/index.html#beyond-conditional-probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#preview-do-calculus",
    "href": "w02/index.html#preview-do-calculus",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w02/index.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#ulysses-and-the-computational-sirens",
    "href": "w02/index.html#ulysses-and-the-computational-sirens",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Ulysses and the [Computational] Sirens",
    "text": "Ulysses and the [Computational] Sirens",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#bayesian-inference-but-with-pictures",
    "href": "w02/index.html#bayesian-inference-but-with-pictures",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Bayesian Inference but with Pictures",
    "text": "Bayesian Inference but with Pictures\nA Probabilistic Graphical Model (PGM) provides us with:\n\nA formal-mathematical‚Ä¶\nBut also easily visualizable (by construction)‚Ä¶\nRepresentation of a data-generating process (DGP)!\n\nExample: Let‚Äôs model how weather \\(W\\) affects evening plans \\(Y\\): the choice between going to a party or staying in to watch movies\n\n\n\n\n\n\n The Partier‚Äôs Dilemma\n\n\n\n\nA person \\(i\\) wakes up with some initial affinity for partying: \\(\\Pr(Y_i = \\textsf{Go})\\)\n\\(i\\) then goes to their window and observes the weather \\(W_i\\) outside:\n\nIf the weather is sunny, \\(i\\)‚Äôs affinity increases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Sun}) &gt; \\Pr(Y = \\textsf{Go})\\)\nOtherwise, if it is rainy, \\(i\\)‚Äôs affinity decreases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Rain}) &lt; \\Pr(Y = \\textsf{Go})\\)",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#two-main-building-blocks",
    "href": "w02/index.html#two-main-building-blocks",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Two Main ‚ÄúBuilding Blocks‚Äù",
    "text": "Two Main ‚ÄúBuilding Blocks‚Äù\n\nNodes like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}\\) denote Random Variables\n\n\\[\n\\boxed{\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}} \\simeq \\boxed{ \\begin{array}{c|cc}x & \\textsf{Tails} & \\textsf{Heads} \\\\\\hline \\Pr(X = x) & 0.5 & 0.5\\end{array}}\n\\]\n\nEdges like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) denote relationships between RVs\n\nWhat an edge ‚Äúmeans‚Äù can get [ontologically] tricky!\nRetain sanity by just remembering: an edge \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) is included in our PGM if we ‚Äúcare about‚Äù modeling the conditional probability table (CPT) of \\(Y\\) w.r.t. \\(X\\)\n\n\n\\[\n\\require{enclose}\\boxed{ \\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em} } \\simeq \\boxed{\n  \\begin{array}{c|cc}\n  x & \\Pr(Y = \\textsf{Lose} \\mid X = x) & \\Pr(Y = \\textsf{Win} \\mid X = x) \\\\\\hline\n  \\textsf{Tails} & 0.8 & 0.2 \\\\\n  \\textsf{Heads} & 0.5 & 0.5\n  \\end{array}\n}\n\\]",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#pgm-for-the-partiers-dilemma",
    "href": "w02/index.html#pgm-for-the-partiers-dilemma",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "PGM for the Partier‚Äôs Dilemma",
    "text": "PGM for the Partier‚Äôs Dilemma\n\nA node \\(\\pnode{W}\\) denoting RV \\(W\\), which can take on values in \\(\\mathcal{R}_W = \\{\\textsf{Sun}, \\textsf{Rain}\\}\\),\nA node \\(\\pnode{Y}\\) denoting RV \\(Y\\), which can take on values in \\(\\mathcal{R}_Y = \\{\\textsf{Go}, \\textsf{Stay}\\}\\), and\nAn edge \\(\\pedge{W}{Y}\\) representing the following relationship between \\(W\\) and \\(Y\\):\n\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) = 0.8\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Sun}) = 0.2\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) = 0.1\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Rain}) = 0.9\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Our PGM of the Partier‚Äôs Dilemma\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\Pr(Y = \\textsf{Stay} \\mid W)\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W)\\)\n\n\n\n\n\\(W = \\textsf{Sun}\\)\n0.2\n0.8\n\n\n\\(W = \\textsf{Rain}\\)\n0.9\n0.1\n\n\n\n\n\nFigure¬†2: The Conditional Probability Table (CPT) for the edge \\(\\pedge{W}{Y}\\) in Figure¬†1",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#observed-vs.-latent-nodes",
    "href": "w02/index.html#observed-vs.-latent-nodes",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed vs.¬†Latent Nodes",
    "text": "Observed vs.¬†Latent Nodes\n\nPGMs help us make valid (Bayesian) inferences about the world in the face of incomplete information!\nKey remaining tool: separation of nodes into two categories:\n\nObserved nodes (shaded)\nLatent nodes (unshaded)\n\n\\(\\Rightarrow\\) Can use our PGM as a weather-inference machine!\nIf we observe \\(i\\) at a party, what can we infer about the weather outside?",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#observed-partier-latent-weather",
    "href": "w02/index.html#observed-partier-latent-weather",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed Partier, Latent Weather",
    "text": "Observed Partier, Latent Weather\n\nWe can draw this situation as a PGM with shaded and unshaded nodes, distinguishing what we know from what we‚Äôd like to infer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùì\n¬†\n‚úÖ\n\n\n\n\nAnd we can now use Bayes‚Äô Rule to compute how observed information (\\(i\\) at party \\(\\Rightarrow [Y = \\textsf{Go}]\\)) ‚Äúflows‚Äù back into \\(W\\)",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#computation-via-bayes-rule",
    "href": "w02/index.html#computation-via-bayes-rule",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Computation via Bayes‚Äô Rule",
    "text": "Computation via Bayes‚Äô Rule\n\nBayes‚Äô Rule, \\(\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}\\), tells us how to use info about \\(\\Pr(B \\mid A)\\) to obtain info about \\(\\Pr(A \\mid B)\\)!\nWe use it to obtain a distribution for \\(W\\) updated to incorporate new info \\([Y = \\textsf{Go}]\\):\n\n\\[\n\\begin{align*}\n&\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go})\n= \\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go})} \\\\\n=\\, &\\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun}) + \\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) \\Pr(W = \\textsf{Rain})}\n\\end{align*}\n\\]\n\nPlug in info from CPT to obtain our new (conditional) probability of interest:\n\n\\[\n\\begin{align*}\n\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go}) &= \\frac{(0.8)(0.5)}{(0.8)(0.5) + (0.1)(0.5)} = \\frac{0.4}{0.4 + 0.05} \\approx 0.89\n\\end{align*}\n\\]\n\nWe‚Äôve learned something interesting! Observing \\(i\\) at the party \\(\\leadsto\\) probability of sun jumps from \\(0.5\\) (‚Äúprior‚Äù estimate of \\(W\\), best guess without any other relevant info) to \\(0.89\\) (‚Äúposterior‚Äù estimate of \\(W\\), best guess after incorporating relevant info).",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#references",
    "href": "w02/index.html#references",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nKoller, Daphne, and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press. https://www.dropbox.com/scl/fi/6qmnr9feizngaq4005isj/Probabilistic-Graphical-Models.pdf?rlkey=4803nmsffh4xtsdfiji8jppb1&dl=1.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1.",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#appendix-zero-probabilities",
    "href": "w02/index.html#appendix-zero-probabilities",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Appendix: Zero Probabilities",
    "text": "Appendix: Zero Probabilities\nFrom Koller and Friedman (2009), pp.¬†66-67:\n\nZero probabilities: A common mistake is to assign a probability of zero to an event that is extremely unlikely, but not impossible. The problem is that one can never condition away a zero probability, no matter how much evidence we get. When an event is unlikely but not impossible, giving it probability zero is guaranteed to lead to irrecoverable errors. For example, in one of the early versions of the the Pathfinder system (box 3.D), 10 percent of the misdiagnoses were due to zero probability estimates given by the expert to events that were unlikely but not impossible.\n\n‚Üê Back to slide",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/index.html#footnotes",
    "href": "w02/index.html#footnotes",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Appendix Slide‚Ü©Ô∏é",
    "crumbs": [
      "Week 2: {{< var w02.date-md >}}"
    ]
  },
  {
    "objectID": "w02/slides.html#courtney-green",
    "href": "w02/slides.html#courtney-green",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Courtney Green",
    "text": "Courtney Green\nBackground: BA in Public Policy & Leadership, now interested in using data to examine structural disparities.\nInterests & what I can help with: How sociodemographic factors (e.g., race, gender, income, immigration status, education level) shape policy outcomes and institutional practices in areas like criminal justice, housing, healthcare, education, and environment.\n\nAsk me about: Counterfactual balancing, handling messy or privacy-limited datasets, and framing causal questions around inequality.\nReach out if you‚Äôre thinking about a final project that touches social systems, systemic inequity, or fairness!"
  },
  {
    "objectID": "w02/slides.html#what-got-me-interested-in-causal-inference",
    "href": "w02/slides.html#what-got-me-interested-in-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Got Me Interested in Causal Inference",
    "text": "What Got Me Interested in Causal Inference\n5000 Project: Over-Policing and Wrongful Convictions in Illinois\n\nMotivation: Explore what demographic factors predict wrongful convictions using real exoneration data.\nChallenge: No data on ‚Äúnon-exonerated innocents,‚Äù so I (with prof‚Äôs help) created a counterfactual dataset by sampling from the incarcerated population\nMethod: Trained models (logistic regression, random forest, KNN) on a balanced dataset of 548 exonerated and 548 non-exonerated individuals\nFinding: Race and geography (esp.¬†Cook Count/Chicago) were the strongest predictors of exoneration\nTakeaway: Questions about systemic bias can‚Äôt be answered with just descriptive stats, you need counterfactual reasoning and causal inference\n\n(Optional) View the counterfactual setup"
  },
  {
    "objectID": "w02/slides.html#wendy-hu",
    "href": "w02/slides.html#wendy-hu",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Wendy Hu",
    "text": "Wendy Hu\n\nBackground: BS in Social Work, passionate about advancing social equity through computational tools\nInterest: I bring a background in social welfare, public health, and NGO data, with experience analyzing behavioral and institutional outcomes. My work spans Indigenous health, gender equity, and trauma-informed service design‚Äîalways with a focus on equity, system change, and policy relevance.\nAsk me about: Framing social justice questions in data terms, working with community or clinical datasets and interpreting model results in context.\nIf you‚Äôre designing a project around health, inequality, nonprofit impact, or any socially embedded issue‚ÄîI‚Äôd love to help bridge rigor and relevance."
  },
  {
    "objectID": "w02/slides.html#jeffs-hw1-updateapology",
    "href": "w02/slides.html#jeffs-hw1-updateapology",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Jeff‚Äôs HW1 Update/Apology",
    "text": "Jeff‚Äôs HW1 Update/Apology\n\nBasically‚Ä¶ I goofed by trying to shove a homework into the first two weeks of class üò≠\nLast week was ‚Äúsetting the table‚Äù, so there are a few (multiple-choice) questions on that\nBut, really only makes sense to have HW starting from end of today, once we‚Äôve introduced PGMs, the language we need to actually do Causal Computational Social Science!\n\n\n Due date pushed to Friday, June 6, 5:59pm"
  },
  {
    "objectID": "w02/slides.html#hw1-detail-plz-dont-hate-me",
    "href": "w02/slides.html#hw1-detail-plz-dont-hate-me",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)",
    "text": "HW1 Detail (Plz Don‚Äôt Hate Me üôà)\n(The main reason this is taking me so long)\n\nTwo key libraries for ‚Äúmain‚Äù course content (HW2 onwards):\n\nIn R: rethinking, ‚Äúlite‚Äù version of Stan\nIn Python: PyMC\n\nBoth are overkill for current unit: lots of work required to ‚Äúturn off‚Äù fancy features and implement basic PGMs\n\\(\\leadsto\\) Only way I know to achieve two goals:\n\n Learning simple PGMs first, as tools for thinking, before adding in additional full-on coding baggage of Stan/PyMC\n Having an entire textbook on these base PGMs that you can use as reference\n\nIs to use [small, restricted] subset of JavaScript, called WebPPL, because‚Ä¶\n\n\nhttp://probmods.org/"
  },
  {
    "objectID": "w02/slides.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "href": "w02/slides.html#disclaimer-unfortunate-side-effects-of-engaging-seriously-with-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality",
    "text": "Disclaimer: Unfortunate Side Effects of Engaging Seriously with Causality\n\n\n You‚Äôll no longer be able to read ‚Äúscientific‚Äù writing without striking this expression (involuntarily):\n\n ‚ÄúScientific‚Äù talks will begin to sound like the following:"
  },
  {
    "objectID": "w02/slides.html#blasting-off-into-causality",
    "href": "w02/slides.html#blasting-off-into-causality",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Blasting Off Into Causality!",
    "text": "Blasting Off Into Causality!"
  },
  {
    "objectID": "w02/slides.html#data-generating-processes-dgps",
    "href": "w02/slides.html#data-generating-processes-dgps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Data-Generating Processes (DGPs)",
    "text": "Data-Generating Processes (DGPs)\n\n\n\n\n\n\n\nYou saw this in DSAN 5100!\n¬´\\(X_1, \\ldots, X_n\\) drawn i.i.d. Normal, mean \\(\\mu\\) variance \\(\\sigma^2\\)¬ª characterizes DGP of \\((X_1, \\ldots, X_n)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n5650: Dive into DGPs, rather than treating as black box/footnote to Law of Large Numbers, so we can move [asymptotically!]‚Ä¶\nFrom associational statements:\n\n¬´\\(\\underbrace{\\text{An increase}}_{\\small\\text{noun}}\\) in \\(X\\) by 1 is associated with increase in \\(Y\\) by \\(\\beta\\)¬ª\n\nTo causal ones: ¬´\\(\\underbrace{\\text{Increasing}}_{\\small\\text{verb}}\\) \\(X\\) by 1 causes \\(Y\\) to increase by \\(\\beta\\)¬ª"
  },
  {
    "objectID": "w02/slides.html#dgps-and-the-emergence-of-order",
    "href": "w02/slides.html#dgps-and-the-emergence-of-order",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "DGPs and the Emergence of Order",
    "text": "DGPs and the Emergence of Order\n\n\n\n\n\n\n\nWho can guess the state of this process after 10 steps, with 1 person?\n10 people? 50? 100? (If they find themselves on the same spot, they stand on each other‚Äôs heads)\n100 steps? 1000?"
  },
  {
    "objectID": "w02/slides.html#the-result-16-steps",
    "href": "w02/slides.html#the-result-16-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 16 Steps",
    "text": "The Result: 16 Steps"
  },
  {
    "objectID": "w02/slides.html#the-result-64-steps",
    "href": "w02/slides.html#the-result-64-steps",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Result: 64 Steps",
    "text": "The Result: 64 Steps"
  },
  {
    "objectID": "w02/slides.html#mathematicalscientific-modeling",
    "href": "w02/slides.html#mathematicalscientific-modeling",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "‚ÄúMathematical/Scientific Modeling‚Äù",
    "text": "‚ÄúMathematical/Scientific Modeling‚Äù\n\nThing we observe (poking out of water): data\nHidden but possibly discoverable via deeper dive (ecosystem under surface): DGP"
  },
  {
    "objectID": "w02/slides.html#so-whats-the-problem",
    "href": "w02/slides.html#so-whats-the-problem",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "So What‚Äôs the Problem?",
    "text": "So What‚Äôs the Problem?\n\nNon-probabilistic models: High potential for being garbage\n\ntldr: even if SUPER certain, using \\(\\Pr(\\mathcal{H}) = 1-\\varepsilon\\) with tiny \\(\\varepsilon\\) has literal life-saving advantages1 (Finetti 1972)\n\nProbabilistic models: Getting there, still looking at ‚Äúsurface‚Äù\n\nOf the \\(N = 100\\) times we observed event \\(X\\) occurring, event \\(Y\\) also occurred \\(90\\) of those times\n\\(\\implies \\Pr(Y \\mid X) = \\frac{\\#[X, Y]}{\\#[X]} = \\frac{90}{100} = 0.9\\)\n\nCausal models: Does \\(Y\\) happen because of \\(X\\) happening? For that, need to start modeling what‚Äôs happening under the surface making \\(X\\) and \\(Y\\) ‚Äúpop up‚Äù together so often\n\nSee Appendix Slide"
  },
  {
    "objectID": "w02/slides.html#the-shallow-problem-of-causal-inference",
    "href": "w02/slides.html#the-shallow-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Shallow Problem of Causal Inference",
    "text": "The Shallow Problem of Causal Inference\n\n\n\n\n\n\n\n\ncor(ski_df$value, law_df$value)\n[1] 0.9921178\n\n(Data from Vigen, Spurious Correlations)\n\nThis, however, is only a mini-boss. Beyond it lies the truly invincible FINAL BOSS‚Ä¶ üôÄ"
  },
  {
    "objectID": "w02/slides.html#the-fundamental-problem-of-causal-inference",
    "href": "w02/slides.html#the-fundamental-problem-of-causal-inference",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nThe only workable definition of ¬´\\(X\\) causes \\(Y\\)¬ª:\n\n\n\n\n Defining Causality (Hume 1739, ruining everything as usual üò§)\n\n\n\\(X\\) causes \\(Y\\) if and only if:\n\n\\(X\\) temporally precedes \\(Y\\) and\n\nIn two worlds \\(W_0\\) and \\(W_1\\) where\neverything is exactly the same except that \\(X = 0\\) in \\(W_0\\) and \\(X = 1\\) in \\(W_1\\),\n\\(Y = 0\\) in \\(W_0\\) and \\(Y = 1\\) in \\(W_1\\)\n\n\n\n\n\n\n\nThe problem? We live in one world, not two identical worlds simultaneously üò≠"
  },
  {
    "objectID": "w02/slides.html#what-is-to-be-done",
    "href": "w02/slides.html#what-is-to-be-done",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "What Is To Be Done?",
    "text": "What Is To Be Done?"
  },
  {
    "objectID": "w02/slides.html#probability",
    "href": "w02/slides.html#probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Probability++",
    "text": "Probability++\n\nTools from prob/stats (RVs, CDFs, Conditional Probability) necessary but not sufficient for causal inference!\nExample: Say we use DSAN 5100 tools to discover:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\)\nProbability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\)\n\nUnfortunately, we still cannot infer that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring."
  },
  {
    "objectID": "w02/slides.html#beyond-conditional-probability",
    "href": "w02/slides.html#beyond-conditional-probability",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Beyond Conditional Probability",
    "text": "Beyond Conditional Probability\n\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶\nRecent decades: researchers have built up an additional ‚Äúlayer‚Äù of modeling tools, augmenting existing machinery of probability to address causality head-on!\nPearl (2000): Formal proofs that (\\(\\Pr\\) axioms) \\(\\cup\\) (\\(\\textsf{do}\\) axioms) \\(\\Rightarrow\\) causal inference procedures successfully recover causal effects"
  },
  {
    "objectID": "w02/slides.html#preview-do-calculus",
    "href": "w02/slides.html#preview-do-calculus",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Preview: do-Calculus",
    "text": "Preview: do-Calculus\n\nExtends core of probability to incorporate causality, via \\(\\textsf{do}\\) operator\n\\(\\textsf{do}(X = 5)\\) is a ‚Äúspecial‚Äù event, representing intervention in DGP to force \\(X \\leftarrow 5\\)‚Ä¶ \\(\\textsf{do}(X = 5)\\) not the same event as \\(X = 5\\)!\n\n\n\n\n\n\n\n\n\n\\(X = 5\\)\n\\(\\neq\\)\n\\(\\textsf{do}(X = 5)\\)\n\n\n\n\nObserving that \\(X\\) took on value 5 (for some possibly-unknown reason)\n\\(\\neq\\)\nIntervening to force \\(X \\leftarrow 5\\), all else in DGP remaining the same (intervention then ‚Äúflows‚Äù through rest of DGP)\n\n\n\n\nProbably the most difficult thing in 5650 to wrap head around\n‚ÄúSpecial‚Äù: \\(\\Pr(\\textsf{do}(X = 5))\\) not well-defined, only \\(\\Pr(Y = 6 \\mid \\textsf{do}(X = 5))\\)\nTo emphasize special-ness, we may use notation like:\n\\[\n\\Pr(Y = 6 \\mid \\textsf{do}(X = 5)) \\equiv \\textstyle \\Pr_{\\textsf{do}(X = 5)}(Y = 6)\n\\]\nto avoid confusion with ‚Äúnormal‚Äù events"
  },
  {
    "objectID": "w02/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "href": "w02/slides.html#causal-world-unlocked-with-great-power-comes-great-responsibility",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)",
    "text": "Causal World Unlocked üòé (With Great Power Comes Great Responsibility‚Ä¶)\n\nWith \\(\\textsf{do}(\\cdot)\\) in hand‚Ä¶ (Alongside DGP satisfying axioms slightly more strict than core probability axioms)\nWe can make causal inferences from similar pair of facts! If:\n\nProbability that event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\),\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nNow we can actually infer that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!"
  },
  {
    "objectID": "w02/slides.html#ulysses-and-the-computational-sirens",
    "href": "w02/slides.html#ulysses-and-the-computational-sirens",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Ulysses and the [Computational] Sirens",
    "text": "Ulysses and the [Computational] Sirens"
  },
  {
    "objectID": "w02/slides.html#bayesian-inference-but-with-pictures",
    "href": "w02/slides.html#bayesian-inference-but-with-pictures",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Bayesian Inference but with Pictures",
    "text": "Bayesian Inference but with Pictures\nA Probabilistic Graphical Model (PGM) provides us with:\n\nA formal-mathematical‚Ä¶\nBut also easily visualizable (by construction)‚Ä¶\nRepresentation of a data-generating process (DGP)!\n\nExample: Let‚Äôs model how weather \\(W\\) affects evening plans \\(Y\\): the choice between going to a party or staying in to watch movies\n\n\n\n\n The Partier‚Äôs Dilemma\n\n\n\nA person \\(i\\) wakes up with some initial affinity for partying: \\(\\Pr(Y_i = \\textsf{Go})\\)\n\\(i\\) then goes to their window and observes the weather \\(W_i\\) outside:\n\nIf the weather is sunny, \\(i\\)‚Äôs affinity increases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Sun}) &gt; \\Pr(Y = \\textsf{Go})\\)\nOtherwise, if it is rainy, \\(i\\)‚Äôs affinity decreases: \\(\\Pr(Y_i = \\textsf{Go} \\mid W_i = \\textsf{Rain}) &lt; \\Pr(Y = \\textsf{Go})\\)"
  },
  {
    "objectID": "w02/slides.html#two-main-building-blocks",
    "href": "w02/slides.html#two-main-building-blocks",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Two Main ‚ÄúBuilding Blocks‚Äù",
    "text": "Two Main ‚ÄúBuilding Blocks‚Äù\n\nNodes like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}\\) denote Random Variables\n\n\\[\n\\boxed{\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em}} \\simeq \\boxed{ \\begin{array}{c|cc}x & \\textsf{Tails} & \\textsf{Heads} \\\\\\hline \\Pr(X = x) & 0.5 & 0.5\\end{array}}\n\\]\n\nEdges like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) denote relationships between RVs\n\nWhat an edge ‚Äúmeans‚Äù can get [ontologically] tricky!\nRetain sanity by just remembering: an edge \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em}\\) is included in our PGM if we ‚Äúcare about‚Äù modeling the conditional probability table (CPT) of \\(Y\\) w.r.t. \\(X\\)\n\n\n\\[\n\\require{enclose}\\boxed{ \\enclose{circle}{\\kern .01em ~X~\\kern .01em} \\rightarrow \\; \\enclose{circle}{\\kern.01em Y~\\kern .01em} } \\simeq \\boxed{\n  \\begin{array}{c|cc}\n  x & \\Pr(Y = \\textsf{Lose} \\mid X = x) & \\Pr(Y = \\textsf{Win} \\mid X = x) \\\\\\hline\n  \\textsf{Tails} & 0.8 & 0.2 \\\\\n  \\textsf{Heads} & 0.5 & 0.5\n  \\end{array}\n}\n\\]"
  },
  {
    "objectID": "w02/slides.html#pgm-for-the-partiers-dilemma",
    "href": "w02/slides.html#pgm-for-the-partiers-dilemma",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "PGM for the Partier‚Äôs Dilemma",
    "text": "PGM for the Partier‚Äôs Dilemma\n\nA node \\(\\pnode{W}\\) denoting RV \\(W\\), which can take on values in \\(\\mathcal{R}_W = \\{\\textsf{Sun}, \\textsf{Rain}\\}\\),\nA node \\(\\pnode{Y}\\) denoting RV \\(Y\\), which can take on values in \\(\\mathcal{R}_Y = \\{\\textsf{Go}, \\textsf{Stay}\\}\\), and\nAn edge \\(\\pedge{W}{Y}\\) representing the following relationship between \\(W\\) and \\(Y\\):\n\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) = 0.8\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Sun}) = 0.2\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) = 0.1\\)\n\\(\\Pr(Y = \\textsf{Stay} \\mid W = \\textsf{Rain}) = 0.9\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Our PGM of the Partier‚Äôs Dilemma\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\Pr(Y = \\textsf{Stay} \\mid W)\\)\n\\(\\Pr(Y = \\textsf{Go} \\mid W)\\)\n\n\n\n\n\\(W = \\textsf{Sun}\\)\n0.2\n0.8\n\n\n\\(W = \\textsf{Rain}\\)\n0.9\n0.1\n\n\n\n\n\nFigure¬†2: The Conditional Probability Table (CPT) for the edge \\(\\pedge{W}{Y}\\) in Figure¬†1"
  },
  {
    "objectID": "w02/slides.html#observed-vs.-latent-nodes",
    "href": "w02/slides.html#observed-vs.-latent-nodes",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed vs.¬†Latent Nodes",
    "text": "Observed vs.¬†Latent Nodes\n\nPGMs help us make valid (Bayesian) inferences about the world in the face of incomplete information!\nKey remaining tool: separation of nodes into two categories:\n\nObserved nodes (shaded)\nLatent nodes (unshaded)\n\n\\(\\Rightarrow\\) Can use our PGM as a weather-inference machine!\nIf we observe \\(i\\) at a party, what can we infer about the weather outside?"
  },
  {
    "objectID": "w02/slides.html#observed-partier-latent-weather",
    "href": "w02/slides.html#observed-partier-latent-weather",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Observed Partier, Latent Weather",
    "text": "Observed Partier, Latent Weather\n\nWe can draw this situation as a PGM with shaded and unshaded nodes, distinguishing what we know from what we‚Äôd like to infer:\n\n\n\n\n\n\n\n\n\n\n‚ùì\n¬†\n‚úÖ\n\n\n\n\nAnd we can now use Bayes‚Äô Rule to compute how observed information (\\(i\\) at party \\(\\Rightarrow [Y = \\textsf{Go}]\\)) ‚Äúflows‚Äù back into \\(W\\)"
  },
  {
    "objectID": "w02/slides.html#computation-via-bayes-rule",
    "href": "w02/slides.html#computation-via-bayes-rule",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Computation via Bayes‚Äô Rule",
    "text": "Computation via Bayes‚Äô Rule\n\nBayes‚Äô Rule, \\(\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}\\), tells us how to use info about \\(\\Pr(B \\mid A)\\) to obtain info about \\(\\Pr(A \\mid B)\\)!\nWe use it to obtain a distribution for \\(W\\) updated to incorporate new info \\([Y = \\textsf{Go}]\\):\n\n\\[\n\\begin{align*}\n&\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go})\n= \\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go})} \\\\\n=\\, &\\frac{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun})}{\\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Sun}) \\Pr(W = \\textsf{Sun}) + \\Pr(Y = \\textsf{Go} \\mid W = \\textsf{Rain}) \\Pr(W = \\textsf{Rain})}\n\\end{align*}\n\\]\n\nPlug in info from CPT to obtain our new (conditional) probability of interest:\n\n\\[\n\\begin{align*}\n\\Pr(W = \\textsf{Sun} \\mid Y = \\textsf{Go}) &= \\frac{(0.8)(0.5)}{(0.8)(0.5) + (0.1)(0.5)} = \\frac{0.4}{0.4 + 0.05} \\approx 0.89\n\\end{align*}\n\\]\n\nWe‚Äôve learned something interesting! Observing \\(i\\) at the party \\(\\leadsto\\) probability of sun jumps from \\(0.5\\) (‚Äúprior‚Äù estimate of \\(W\\), best guess without any other relevant info) to \\(0.89\\) (‚Äúposterior‚Äù estimate of \\(W\\), best guess after incorporating relevant info)."
  },
  {
    "objectID": "w02/slides.html#references",
    "href": "w02/slides.html#references",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "References",
    "text": "References\n\n\nFinetti, Bruno de. 1972. Probability, Induction and Statistics: The Art of Guessing. J. Wiley.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nKoller, Daphne, and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press. https://www.dropbox.com/scl/fi/6qmnr9feizngaq4005isj/Probabilistic-Graphical-Models.pdf?rlkey=4803nmsffh4xtsdfiji8jppb1&dl=1.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press. https://www.dropbox.com/scl/fi/4kfpro4gpx7tg55fwd7qt/Judea-Pearl-Causality_-Models-Reasoning-and-Inference.pdf?rlkey=f0abw0cgmsyiegx6xapq5z7j2&dl=1."
  },
  {
    "objectID": "w02/slides.html#appendix-zero-probabilities",
    "href": "w02/slides.html#appendix-zero-probabilities",
    "title": "Week 2: Probabilistic Graphical Models (PGMs)",
    "section": "Appendix: Zero Probabilities",
    "text": "Appendix: Zero Probabilities\nFrom Koller and Friedman (2009), pp.¬†66-67:\n\nZero probabilities: A common mistake is to assign a probability of zero to an event that is extremely unlikely, but not impossible. The problem is that one can never condition away a zero probability, no matter how much evidence we get. When an event is unlikely but not impossible, giving it probability zero is guaranteed to lead to irrecoverable errors. For example, in one of the early versions of the the Pathfinder system (box 3.D), 10 percent of the misdiagnoses were due to zero probability estimates given by the expert to events that were unlikely but not impossible.\n\n‚Üê Back to slide"
  },
  {
    "objectID": "writeups/index.html",
    "href": "writeups/index.html",
    "title": "Extra Writeups",
    "section": "",
    "text": "Order By\n       Default\n         \n          Last Updated - Oldest\n        \n         \n          Last Updated - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nLast Updated\n\n\nRelevant To\n\n\nCategory\n\n\n\n\n\n\nHigher-Order DAGs via Block Models\n\n\nThursday, May 1, 2025\n\n\nWeeks 5-7 (Causality)\n\n\nExtra Writeups\n\n\n\n\nA Quick Introduction to Probabilistic Graphical Models\n\n\nSunday, February 18, 2024\n\n\nWeeks 5-7 (Causality)\n\n\nExtra Writeups\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "final.html",
    "href": "final.html",
    "title": "Final Project Specifications",
    "section": "",
    "text": "Our goal is to make the final project as open-ended as possible, to give you the space to explore any particular topic that may have piqued your interest throughout the semester! At the same time, we hope to provide you with guidance and mentorship so that you don‚Äôt feel lost as to how to start, how to proceed, and/or what to submit for the final deliverable!1"
  },
  {
    "objectID": "final.html#overview",
    "href": "final.html#overview",
    "title": "Final Project Specifications",
    "section": "",
    "text": "Our goal is to make the final project as open-ended as possible, to give you the space to explore any particular topic that may have piqued your interest throughout the semester! At the same time, we hope to provide you with guidance and mentorship so that you don‚Äôt feel lost as to how to start, how to proceed, and/or what to submit for the final deliverable!1"
  },
  {
    "objectID": "final.html#references",
    "href": "final.html#references",
    "title": "Final Project Specifications",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "final.html#footnotes",
    "href": "final.html#footnotes",
    "title": "Final Project Specifications",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you can‚Äôt tell, my whole educational philosophy here is just the Montessori system‚Äîthis approach was originally developed for younger (primary school) children, but lots and lots of recent educational research indicates that it‚Äôs an actually an extremely effective way to learn, and to motivate self-learning, for people of any age üòé‚Ü©Ô∏é"
  },
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Science from Particles to People",
    "section": "",
    "text": "Open slides in new window ‚Üí",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#prof.-jeff-introduction",
    "href": "w01/index.html#prof.-jeff-introduction",
    "title": "Week 1: Science from Particles to People",
    "section": "Prof.¬†Jeff Introduction!",
    "text": "Prof.¬†Jeff Introduction!\n\nBorn in NW DC ‚Üí high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Econ",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#the-world-outside-of-dc",
    "href": "w01/index.html#the-world-outside-of-dc",
    "title": "Week 1: Science from Particles to People",
    "section": "The World Outside of DC",
    "text": "The World Outside of DC\n Studied abroad in Beijing (Peking University/ÂåóÂ§ß) ‚Üí internship with Huawei in Hong Kong (HKUST)\n\n\n\n Stanford, MS in Computer Science\n Research Economist, UC Berkeley\n Columbia, PhD in Political Economy",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#why-is-georgetown-having-me-teach-this",
    "href": "w01/index.html#why-is-georgetown-having-me-teach-this",
    "title": "Week 1: Science from Particles to People",
    "section": "Why Is Georgetown Having Me Teach This?",
    "text": "Why Is Georgetown Having Me Teach This?\n\n\n\n\n\n\n\nQuanty things \\(\\leadsto\\) PhD in Political Economy\nPhD exam major: Political Philosophy\nPhD exam minor: International Relations\nPhD exam paper: ‚ÄúHow to Do Things with Translations‚Äù\nGame-changing Research Fellowships at‚Ä¶ (Nothing was the same -drake):\nSanta Fe Institute: dedicated to the multidisciplinary study of complex systems: physical, computational, biological, social\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\n\nFigure¬†1: Years spent questing in dungeons of academia\n\n\n\n\n\n\n\nCentre for the Study of the History of Political Thought, Queen Mary University of London (QMUL): New approaches to the history of political thought [Quentin Skinner, ‚ÄúCambridge School‚Äù] have changed how we study ideas from the past and their relevance to contemporary politics. The focus of the Centre is to explore [ts].",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#dissertation-nlp-x-history",
    "href": "w01/index.html#dissertation-nlp-x-history",
    "title": "Week 1: Science from Particles to People",
    "section": "Dissertation (NLP x History)",
    "text": "Dissertation (NLP x History)\n‚ÄúOur Word is Our Weapon‚Äù: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#ir-part-wars-of-ideas-i-cold-war",
    "href": "w01/index.html#ir-part-wars-of-ideas-i-cold-war",
    "title": "Week 1: Science from Particles to People",
    "section": "(IR Part) Wars of Ideas I: Cold War",
    "text": "(IR Part) Wars of Ideas I: Cold War\n\nCold War arms shipments (SIPRI) vs.¬†propaganda (–ü–µ—á–∞—Ç—å –°–°–°–†): here, to üá™üáπ",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "href": "w01/index.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "title": "Week 1: Science from Particles to People",
    "section": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993",
    "text": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#research-nowadays",
    "href": "w01/index.html#research-nowadays",
    "title": "Week 1: Science from Particles to People",
    "section": "Research Nowadays",
    "text": "Research Nowadays\n\nMost cited paper: ‚ÄúMonopsony in Online Labor Markets‚Äù (Uses Double-Debiased ML for Causal Inference!)\nMost recent paper: ‚ÄúOperationalizing Freedom as Non-Domination in the Labor Market‚Äù (Cambridge U Press)\nRarely cited paper but often-thought-about obsession: ‚ÄúHow To Do Things With Translations‚Äù\nRelated thing you can buy in a bookstore: [Editorial board for new translation of] Capital, Vol. 1 by Karl Marx (Princeton U Press)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#but-now-teaching",
    "href": "w01/index.html#but-now-teaching",
    "title": "Week 1: Science from Particles to People",
    "section": "But Now‚Ä¶ Teaching!",
    "text": "But Now‚Ä¶ Teaching!\n\nGrowth mindset: ‚ÄúI can‚Äôt do this‚Äù \\(\\leadsto\\) ‚ÄúI can‚Äôt do this yet!‚Äù\nThink of anything you‚Äôre able to do‚Ä¶ There was a point in life when you didn‚Äôt know how to do it! What happened? Your brain established and/or rearranged neural pathways as you struggled with it!\nHow does this neural re-arrangement work? One ‚Äúcheatcode‚Äù is spaced repetition:\n\n\n\n\nImage source",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#lil-wayne-on-spaced-repetition",
    "href": "w01/index.html#lil-wayne-on-spaced-repetition",
    "title": "Week 1: Science from Particles to People",
    "section": "Lil Wayne on Spaced Repetition",
    "text": "Lil Wayne on Spaced Repetition\n\n\n\n\n\n\nFigure¬†2: From The Carter (Documentary)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#maria-montessori-on-your-final-project",
    "href": "w01/index.html#maria-montessori-on-your-final-project",
    "title": "Week 1: Science from Particles to People",
    "section": "Maria Montessori on Your Final Project",
    "text": "Maria Montessori on Your Final Project\n\nOur teaching should be governed, not by a desire to make students learn things, but by the endeavor to keep burning within them that light which is called curiosity. (Montessori 1916)1\n\n\nTo this end: your final project is to explore some potential causal linkage from \\(X\\) to \\(Y\\) (more later!)\nThe sole requirement is: sufficient curiosity to serve as fuel for your journey from associational world to causal world",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#coarse-graining-units-of-observation",
    "href": "w01/index.html#coarse-graining-units-of-observation",
    "title": "Week 1: Science from Particles to People",
    "section": "Coarse-Graining Units of Observation",
    "text": "Coarse-Graining Units of Observation\n\n\n\nField\nExample Unit of Observation\n\n\n\n\nPhysics\nParticle\n[Particle = Fine-Graining of Molecules]\n\n\nChemistry\nMolecule = \\(\\cup\\)(Particles)\n[Molecule = Coarse-Graining of Particles]\n\n\nBiology\nCell = \\(\\cup\\)(Molecules)\n\n\nNeuro/Cog Sci\nBrain = \\(\\cup\\)(Cells)\n\n\nHuman Physiology\nBody = Brain \\(\\cup\\) Other Organs\n\n\n‚Üë Science ¬†¬† üßê something happens here‚Ä¶ ü§î ¬†¬† ‚Üì Social Science\n\n\nAnthropology\nHuman-Relational System (e.g., Kinship) = \\(\\cup\\)(Brains) \\(\\times\\) Natural Environment\n\n\nEconomics\nEconomy = Specific relational system of exchange w.r.t. scarce resources\n\n\nPolitical Economy\nEconomy-Context = Economic Exchange \\(\\cap\\) Relational Power\n\n\nSociology\nSociety = \\(\\cup\\)(Relational Systems: Kinship, Friendship, Authority, Power, Violence)\n\n\nHistory\nLongue-Dur√©e = Evolution of Societies over Time and Space",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#homo-sapienshomo-arbitratushomo-mischievous",
    "href": "w01/index.html#homo-sapienshomo-arbitratushomo-mischievous",
    "title": "Week 1: Science from Particles to People",
    "section": "Homo sapiens/Homo arbitratus/Homo mischievous",
    "text": "Homo sapiens/Homo arbitratus/Homo mischievous\n\nLatin sapiens denotes being ‚Äúdiscerning‚Äù or ‚Äúwise‚Äù\n\n\n\n\n\n\n\n\nBut‚Ä¶ technically nothing stops us from choosing to be ‚Äúunwise‚Äù whenever we‚Äôd like‚Ä¶ bc free will\n\\(\\Rightarrow\\) For this class, humans are Homo arbitratus: arbitratus denotes choosing what to do, after we‚Äôve sapiently ‚Äúdiscerned‚Äù/thought about it",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#laws-of-physics-vs.-laws-of-social-science",
    "href": "w01/index.html#laws-of-physics-vs.-laws-of-social-science",
    "title": "Week 1: Science from Particles to People",
    "section": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science",
    "text": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science\n\nIf we tell an inanimate object that we‚Äôve discovered a law saying that it will accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n‚Ä¶It will likely2 still accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n\n\n\n\n\n\n\n\nIf we tell a human we‚Äôve discovered a law saying they will quack like a duck at 7:30pm EDT every Wednesday\n\n‚Ä¶They can utilize their free will to violate this ‚Äúlaw‚Äù",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#strangely-relevant-cs-topic-the-halting-problem",
    "href": "w01/index.html#strangely-relevant-cs-topic-the-halting-problem",
    "title": "Week 1: Science from Particles to People",
    "section": "Strangely-Relevant CS Topic: The Halting Problem",
    "text": "Strangely-Relevant CS Topic: The Halting Problem\n\nKurt G√∂del \\(\\leftrightarrow\\) Alan Turing: Entscheidungsproblem\nTheorem: It is not possible to write a computer program \\(P(x)\\) that detects whether or not a computer program \\(x\\) will eventually halt (as opposed to, e.g., looping forever)\nProof: Assume \\(P(x)\\) is possible to write. Run it on mischievous_program.py. Infinite contradiction loop.\n\n\n\n\n\nhalting_problem_solver.py\n\ndef will_it_halt(program, input):\n  # Put code you think will work here\n  # Return True if program will halt, False otherwise\n\n\n\nmischievous_program.py\n\ndef my_mischievous_function(input):\n  if will_it_halt(my_mischevious_function, input):\n    while True: pass # Infinite loop\n  else:\n    return # Halt\n\n\n\n\n\n\n\n\nInput‚Üí‚ÜìProgram\n0\n1\n2\n\\(\\cdots\\)\n\n\n\n\n0\nHalt\nLoop\nLoop\n\\(\\cdots\\)\n\n\n1\nLoop\nLoop\nHalt\n\\(\\cdots\\)\n\n\n2\nLoop\nLoop\nLoop\n\\(\\cdots\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\mathbf{\\ddots}\\)\n\n\n\nLoop\nHalt\nHalt\n\\(\\mathbf{\\cdots}\\)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#the-takeaway-bayesian-humility",
    "href": "w01/index.html#the-takeaway-bayesian-humility",
    "title": "Week 1: Science from Particles to People",
    "section": "The Takeaway: [Bayesian] Humility!",
    "text": "The Takeaway: [Bayesian] Humility!\n\nSocial science, with ‚Äúscience‚Äù used in the same sense as for physics, may be a quixotic endeavor3\nInstead, we‚Äôll do social science, where we use data to‚Ä¶\n Infer tendencies: \\(\\mathcal{H}\\) = ¬´\\(X\\) tends to cause \\(Y\\)¬ª\n With some degree of veracity: \\(\\Pr(\\mathcal{H}) \\approx 0.7\\)\n Construct models that we can update with new evidence: Bayes‚Äô rule! \\(\\Pr(\\mathcal{H} \\mid E) = \\frac{\\Pr(E \\mid \\mathcal{H} ) \\Pr(\\mathcal{H})}{\\Pr(E)} \\approx 0.8\\)\nPls notice ‚Äúslippage‚Äù between aleatory probability within \\(\\mathcal{H}\\) (‚Äútends to‚Äù) vs.¬†epistemic probability ‚Äúoutside of‚Äù, talking about \\(\\mathcal{H}\\) (‚ÄúI‚Äôm 70% confident about \\(\\mathcal{H}\\)‚Äù)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#the-logic-of-violence-in-civil-war",
    "href": "w01/index.html#the-logic-of-violence-in-civil-war",
    "title": "Week 1: Science from Particles to People",
    "section": "The Logic of Violence in Civil War",
    "text": "The Logic of Violence in Civil War\n\n\n\nKalyvas (2006)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#matching-estimators",
    "href": "w01/index.html#matching-estimators",
    "title": "Week 1: Science from Particles to People",
    "section": "Matching Estimators",
    "text": "Matching Estimators\n\n\n\nImage Source",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#case-study-military-inequality-leadsto-military-success",
    "href": "w01/index.html#case-study-military-inequality-leadsto-military-success",
    "title": "Week 1: Science from Particles to People",
    "section": "Case Study: Military Inequality \\(\\leadsto\\) Military Success",
    "text": "Case Study: Military Inequality \\(\\leadsto\\) Military Success\n\nLyall (2020): ‚ÄúTreating certain ethnic groups as second-class citizens [‚Ä¶] leads victimized soldiers to subvert military authorities once war begins. The higher an army‚Äôs inequality, the greater its rates of desertion, side-switching, and casualties‚Äù\n\n\nMatching constructs pairs of belligerents that are similar across a wide range of traits thought to dictate battlefield performance but that vary in levels of prewar inequality. The more similar the belligerents, the better our estimate of inequality‚Äôs effects, as all other traits are shared and thus cannot explain observed differences in performance, helping assess how battlefield performance would have improved (declined) if the belligerent had a lower (higher) level of prewar inequality.\nSince [non-matched] cases are dropped [‚Ä¶] selected cases are more representative of average belligerents/wars than outliers with few or no matches, [providing] surer ground for testing generalizability of the book‚Äôs claims than focusing solely on canonical but unrepresentative usual suspects (Germany, the United States, Israel)",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#does-inequality-cause-poor-military-performance",
    "href": "w01/index.html#does-inequality-cause-poor-military-performance",
    "title": "Week 1: Science from Particles to People",
    "section": "Does Inequality Cause Poor Military Performance?",
    "text": "Does Inequality Cause Poor Military Performance?\n\n\n\n\n\n\n\n\nCovariates\nSultanate of Morocco Spanish-Moroccan War, 1859-60\nKhanate of Kokand War with Russia, 1864-65\n\n\n\n\n\\(X\\): Military Inequality\nLow (0.01)\nExtreme (0.70)\n\n\n\\(\\mathbf{Z}\\): Matched Covariates:\n\n\n\n\nInitial relative power\n66%\n66%\n\n\nTotal fielded force\n55,000\n50,000\n\n\nRegime type\nAbsolutist Monarchy (‚àí6)\nAbsolute Monarchy (‚àí7)\n\n\nDistance from capital\n208km\n265km\n\n\nStanding army\nYes\nYes\n\n\nComposite military\nYes\nYes\n\n\nInitiator\nNo\nNo\n\n\nJoiner\nNo\nNo\n\n\nDemocratic opponent\nNo\nNo\n\n\nGreat Power\nNo\nNo\n\n\nCivil war\nNo\nNo\n\n\nCombined arms\nYes\nYes\n\n\nDoctrine\nOffensive\nOffensive\n\n\nSuperior weapons\nNo\nNo\n\n\nFortifications\nYes\nYes\n\n\nForeign advisors\nYes\nYes\n\n\nTerrain\nSemiarid coastal plain\nSemiarid grassland plain\n\n\nTopography\nRugged\nRugged\n\n\nWar duration\n126 days\n378 days\n\n\nRecent war history w/opp\nYes\nYes\n\n\nFacing colonizer\nYes\nYes\n\n\nIdentity dimension\nSunni Islam/Christian\nSunni Islam/Christian\n\n\nNew leader\nYes\nYes\n\n\nPopulation\n8‚Äì8.5 million\n5‚Äì6 million\n\n\nEthnoling fractionalization (ELF)\nHigh\nHigh\n\n\nCiv-mil relations\nRuler as commander\nRuler as commander\n\n\n\\(Y\\): Battlefield Performance:\n\n\n\n\nLoss-exchange ratio\n0.43\n0.02\n\n\nMass desertion\nNo\nYes\n\n\nMass defection\nNo\nNo\n\n\nFratricidal violence\nNo\nYes",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#particularly-fun-non-standard-examples",
    "href": "w01/index.html#particularly-fun-non-standard-examples",
    "title": "Week 1: Science from Particles to People",
    "section": "Particularly Fun Non-‚ÄúStandard‚Äù Examples",
    "text": "Particularly Fun Non-‚ÄúStandard‚Äù Examples\n\nDeDeo, French Revolution\nGrimmer, Mirrors for Sultans and Princes",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#jupyterhub",
    "href": "w01/index.html#jupyterhub",
    "title": "Week 1: Science from Particles to People",
    "section": "JupyterHub",
    "text": "JupyterHub\n\nhttps://guhub.io\n\n\n\n\nImage source",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#references",
    "href": "w01/index.html#references",
    "title": "Week 1: Science from Particles to People",
    "section": "References",
    "text": "References\n\n\nKalyvas, Stathis N. 2006. The Logic of Violence in Civil War. Cambridge University Press. https://www.dropbox.com/scl/fi/4es1r44ldnqtcroonr5g7/The-Logic-of-Violence-in-Civil-War.pdf?rlkey=uonfmw5zkx1ja5l8emv2bzfmg&dl=1.\n\n\nLyall, Jason. 2020. Divided Armies: Inequality and Battlefield Performance in Modern War. Princeton University Press.\n\n\nMontessori, Maria. 1916. Spontaneous Activity in Education: A Basic Guide to the Montessori Methods of Learning in the Classroom. Lulu Press.\n\n\nSperber, Dan. 1996. Explaining Culture: A Naturalistic Approach. Cambridge: Blackwell. https://www.dropbox.com/scl/fi/he6idaajr6vopybv13qhd/Explaining-Culture-A-Naturalistic-Approach.pdf?rlkey=8oyhnsu5ycet4osstft252jaw&dl=1.",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/index.html#footnotes",
    "href": "w01/index.html#footnotes",
    "title": "Week 1: Science from Particles to People",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúCouldn‚Äôt do classes, them lessons used to bore me, I was tryna school heads like Waldorf and Montessori‚Äù -Bar dropped by local rapper one day in freestyle that I never forgot‚Ü©Ô∏é\nObligatory quantum mechanics footnote: human agency [maybe] plays a role, in a quirky way, in physics at tiny subatomic scales‚Ä¶ but once we coarse grain to atoms (and avoid speed of light), Newton‚Äôs Laws accurate to many decimals ü§Ø‚Ü©Ô∏é\nAt least, for the time being‚Ä¶ BUT see Sperber (1996), which will come up later‚Ü©Ô∏é",
    "crumbs": [
      "Week 1: {{< var w01.date-md >}}"
    ]
  },
  {
    "objectID": "w01/slides.html#prof.-jeff-introduction",
    "href": "w01/slides.html#prof.-jeff-introduction",
    "title": "Week 1: Science from Particles to People",
    "section": "Prof.¬†Jeff Introduction!",
    "text": "Prof.¬†Jeff Introduction!\n\nBorn in NW DC ‚Üí high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Econ"
  },
  {
    "objectID": "w01/slides.html#the-world-outside-of-dc",
    "href": "w01/slides.html#the-world-outside-of-dc",
    "title": "Week 1: Science from Particles to People",
    "section": "The World Outside of DC",
    "text": "The World Outside of DC\n Studied abroad in Beijing (Peking University/ÂåóÂ§ß) ‚Üí internship with Huawei in Hong Kong (HKUST)\n\n\n\n Stanford, MS in Computer Science\n Research Economist, UC Berkeley\n Columbia, PhD in Political Economy"
  },
  {
    "objectID": "w01/slides.html#why-is-georgetown-having-me-teach-this",
    "href": "w01/slides.html#why-is-georgetown-having-me-teach-this",
    "title": "Week 1: Science from Particles to People",
    "section": "Why Is Georgetown Having Me Teach This?",
    "text": "Why Is Georgetown Having Me Teach This?\n\n\n\n\n\n\n\nQuanty things \\(\\leadsto\\) PhD in Political Economy\nPhD exam major: Political Philosophy\nPhD exam minor: International Relations\nPhD exam paper: ‚ÄúHow to Do Things with Translations‚Äù\nGame-changing Research Fellowships at‚Ä¶ (Nothing was the same -drake):\nSanta Fe Institute: dedicated to the multidisciplinary study of complex systems: physical, computational, biological, social\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\n\nFigure¬†1: Years spent questing in dungeons of academia\n\n\n\n\n\n\n\nCentre for the Study of the History of Political Thought, Queen Mary University of London (QMUL): New approaches to the history of political thought [Quentin Skinner, ‚ÄúCambridge School‚Äù] have changed how we study ideas from the past and their relevance to contemporary politics. The focus of the Centre is to explore [ts]."
  },
  {
    "objectID": "w01/slides.html#dissertation-nlp-x-history",
    "href": "w01/slides.html#dissertation-nlp-x-history",
    "title": "Week 1: Science from Particles to People",
    "section": "Dissertation (NLP x History)",
    "text": "Dissertation (NLP x History)\n‚ÄúOur Word is Our Weapon‚Äù: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada"
  },
  {
    "objectID": "w01/slides.html#ir-part-wars-of-ideas-i-cold-war",
    "href": "w01/slides.html#ir-part-wars-of-ideas-i-cold-war",
    "title": "Week 1: Science from Particles to People",
    "section": "(IR Part) Wars of Ideas I: Cold War",
    "text": "(IR Part) Wars of Ideas I: Cold War\n\nCold War arms shipments (SIPRI) vs.¬†propaganda (–ü–µ—á–∞—Ç—å –°–°–°–†): here, to üá™üáπ"
  },
  {
    "objectID": "w01/slides.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "href": "w01/slides.html#middle-east-part-wars-of-ideas-ii-first-intifada-1987-1993",
    "title": "Week 1: Science from Particles to People",
    "section": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993",
    "text": "(Middle East Part) Wars of Ideas II: First Intifada, 1987-1993"
  },
  {
    "objectID": "w01/slides.html#research-nowadays",
    "href": "w01/slides.html#research-nowadays",
    "title": "Week 1: Science from Particles to People",
    "section": "Research Nowadays",
    "text": "Research Nowadays\n\nMost cited paper: ‚ÄúMonopsony in Online Labor Markets‚Äù (Uses Double-Debiased ML for Causal Inference!)\nMost recent paper: ‚ÄúOperationalizing Freedom as Non-Domination in the Labor Market‚Äù (Cambridge U Press)\nRarely cited paper but often-thought-about obsession: ‚ÄúHow To Do Things With Translations‚Äù\nRelated thing you can buy in a bookstore: [Editorial board for new translation of] Capital, Vol. 1 by Karl Marx (Princeton U Press)"
  },
  {
    "objectID": "w01/slides.html#but-now-teaching",
    "href": "w01/slides.html#but-now-teaching",
    "title": "Week 1: Science from Particles to People",
    "section": "But Now‚Ä¶ Teaching!",
    "text": "But Now‚Ä¶ Teaching!\n\nGrowth mindset: ‚ÄúI can‚Äôt do this‚Äù \\(\\leadsto\\) ‚ÄúI can‚Äôt do this yet!‚Äù\nThink of anything you‚Äôre able to do‚Ä¶ There was a point in life when you didn‚Äôt know how to do it! What happened? Your brain established and/or rearranged neural pathways as you struggled with it!\nHow does this neural re-arrangement work? One ‚Äúcheatcode‚Äù is spaced repetition:\n\n\nImage source"
  },
  {
    "objectID": "w01/slides.html#lil-wayne-on-spaced-repetition",
    "href": "w01/slides.html#lil-wayne-on-spaced-repetition",
    "title": "Week 1: Science from Particles to People",
    "section": "Lil Wayne on Spaced Repetition",
    "text": "Lil Wayne on Spaced Repetition\n\n\n\n\n\n\nFigure¬†2: From The Carter (Documentary)"
  },
  {
    "objectID": "w01/slides.html#maria-montessori-on-your-final-project",
    "href": "w01/slides.html#maria-montessori-on-your-final-project",
    "title": "Week 1: Science from Particles to People",
    "section": "Maria Montessori on Your Final Project",
    "text": "Maria Montessori on Your Final Project\n\nOur teaching should be governed, not by a desire to make students learn things, but by the endeavor to keep burning within them that light which is called curiosity. (Montessori 1916)1\n\n\nTo this end: your final project is to explore some potential causal linkage from \\(X\\) to \\(Y\\) (more later!)\nThe sole requirement is: sufficient curiosity to serve as fuel for your journey from associational world to causal world\n\n‚ÄúCouldn‚Äôt do classes, them lessons used to bore me, I was tryna school heads like Waldorf and Montessori‚Äù -Bar dropped by local rapper one day in freestyle that I never forgot"
  },
  {
    "objectID": "w01/slides.html#coarse-graining-units-of-observation",
    "href": "w01/slides.html#coarse-graining-units-of-observation",
    "title": "Week 1: Science from Particles to People",
    "section": "Coarse-Graining Units of Observation",
    "text": "Coarse-Graining Units of Observation\n\n\n\nField\nExample Unit of Observation\n\n\n\n\nPhysics\nParticle\n[Particle = Fine-Graining of Molecules]\n\n\nChemistry\nMolecule = \\(\\cup\\)(Particles)\n[Molecule = Coarse-Graining of Particles]\n\n\nBiology\nCell = \\(\\cup\\)(Molecules)\n\n\nNeuro/Cog Sci\nBrain = \\(\\cup\\)(Cells)\n\n\nHuman Physiology\nBody = Brain \\(\\cup\\) Other Organs\n\n\n‚Üë Science ¬†¬† üßê something happens here‚Ä¶ ü§î ¬†¬† ‚Üì Social Science\n\n\nAnthropology\nHuman-Relational System (e.g., Kinship) = \\(\\cup\\)(Brains) \\(\\times\\) Natural Environment\n\n\nEconomics\nEconomy = Specific relational system of exchange w.r.t. scarce resources\n\n\nPolitical Economy\nEconomy-Context = Economic Exchange \\(\\cap\\) Relational Power\n\n\nSociology\nSociety = \\(\\cup\\)(Relational Systems: Kinship, Friendship, Authority, Power, Violence)\n\n\nHistory\nLongue-Dur√©e = Evolution of Societies over Time and Space"
  },
  {
    "objectID": "w01/slides.html#homo-sapienshomo-arbitratushomo-mischievous",
    "href": "w01/slides.html#homo-sapienshomo-arbitratushomo-mischievous",
    "title": "Week 1: Science from Particles to People",
    "section": "Homo sapiens/Homo arbitratus/Homo mischievous",
    "text": "Homo sapiens/Homo arbitratus/Homo mischievous\n\nLatin sapiens denotes being ‚Äúdiscerning‚Äù or ‚Äúwise‚Äù\n\n\n\n\n\n\n\n\nBut‚Ä¶ technically nothing stops us from choosing to be ‚Äúunwise‚Äù whenever we‚Äôd like‚Ä¶ bc free will\n\\(\\Rightarrow\\) For this class, humans are Homo arbitratus: arbitratus denotes choosing what to do, after we‚Äôve sapiently ‚Äúdiscerned‚Äù/thought about it"
  },
  {
    "objectID": "w01/slides.html#laws-of-physics-vs.-laws-of-social-science",
    "href": "w01/slides.html#laws-of-physics-vs.-laws-of-social-science",
    "title": "Week 1: Science from Particles to People",
    "section": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science",
    "text": "Laws of Physics vs.¬†‚ÄúLaws‚Äù of Social Science\n\nIf we tell an inanimate object that we‚Äôve discovered a law saying that it will accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n‚Ä¶It will likely1 still accelerate towards Earth at \\(9.8~\\textrm{m}/\\textrm{s}^2\\)\n\n\n\n\n\n\n\n\n\nIf we tell a human we‚Äôve discovered a law saying they will quack like a duck at 7:30pm EDT every Wednesday\n\n‚Ä¶They can utilize their free will to violate this ‚Äúlaw‚Äù\n\n\n\n\n\n\n\n\n\n\n\n\nObligatory quantum mechanics footnote: human agency [maybe] plays a role, in a quirky way, in physics at tiny subatomic scales‚Ä¶ but once we coarse grain to atoms (and avoid speed of light), Newton‚Äôs Laws accurate to many decimals ü§Ø"
  },
  {
    "objectID": "w01/slides.html#strangely-relevant-cs-topic-the-halting-problem",
    "href": "w01/slides.html#strangely-relevant-cs-topic-the-halting-problem",
    "title": "Week 1: Science from Particles to People",
    "section": "Strangely-Relevant CS Topic: The Halting Problem",
    "text": "Strangely-Relevant CS Topic: The Halting Problem\n\nKurt G√∂del \\(\\leftrightarrow\\) Alan Turing: Entscheidungsproblem\nTheorem: It is not possible to write a computer program \\(P(x)\\) that detects whether or not a computer program \\(x\\) will eventually halt (as opposed to, e.g., looping forever)\nProof: Assume \\(P(x)\\) is possible to write. Run it on mischievous_program.py. Infinite contradiction loop.\n\n\n\n\n\nhalting_problem_solver.py\n\ndef will_it_halt(program, input):\n  # Put code you think will work here\n  # Return True if program will halt, False otherwise\n\n\n\nmischievous_program.py\n\ndef my_mischievous_function(input):\n  if will_it_halt(my_mischevious_function, input):\n    while True: pass # Infinite loop\n  else:\n    return # Halt\n\n\n\n\n\n\n\n\nInput‚Üí‚ÜìProgram\n0\n1\n2\n\\(\\cdots\\)\n\n\n\n\n0\nHalt\nLoop\nLoop\n\\(\\cdots\\)\n\n\n1\nLoop\nLoop\nHalt\n\\(\\cdots\\)\n\n\n2\nLoop\nLoop\nLoop\n\\(\\cdots\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\mathbf{\\ddots}\\)\n\n\n\nLoop\nHalt\nHalt\n\\(\\mathbf{\\cdots}\\)"
  },
  {
    "objectID": "w01/slides.html#the-takeaway-bayesian-humility",
    "href": "w01/slides.html#the-takeaway-bayesian-humility",
    "title": "Week 1: Science from Particles to People",
    "section": "The Takeaway: [Bayesian] Humility!",
    "text": "The Takeaway: [Bayesian] Humility!\n\nSocial science, with ‚Äúscience‚Äù used in the same sense as for physics, may be a quixotic endeavor1\nInstead, we‚Äôll do social science, where we use data to‚Ä¶\n Infer tendencies: \\(\\mathcal{H}\\) = ¬´\\(X\\) tends to cause \\(Y\\)¬ª\n With some degree of veracity: \\(\\Pr(\\mathcal{H}) \\approx 0.7\\)\n Construct models that we can update with new evidence: Bayes‚Äô rule! \\(\\Pr(\\mathcal{H} \\mid E) = \\frac{\\Pr(E \\mid \\mathcal{H} ) \\Pr(\\mathcal{H})}{\\Pr(E)} \\approx 0.8\\)\nPls notice ‚Äúslippage‚Äù between aleatory probability within \\(\\mathcal{H}\\) (‚Äútends to‚Äù) vs.¬†epistemic probability ‚Äúoutside of‚Äù, talking about \\(\\mathcal{H}\\) (‚ÄúI‚Äôm 70% confident about \\(\\mathcal{H}\\)‚Äù)\n\nAt least, for the time being‚Ä¶ BUT see Sperber (1996), which will come up later"
  },
  {
    "objectID": "w01/slides.html#the-logic-of-violence-in-civil-war",
    "href": "w01/slides.html#the-logic-of-violence-in-civil-war",
    "title": "Week 1: Science from Particles to People",
    "section": "The Logic of Violence in Civil War",
    "text": "The Logic of Violence in Civil War\n\nKalyvas (2006)"
  },
  {
    "objectID": "w01/slides.html#matching-estimators",
    "href": "w01/slides.html#matching-estimators",
    "title": "Week 1: Science from Particles to People",
    "section": "Matching Estimators",
    "text": "Matching Estimators\n\nImage Source"
  },
  {
    "objectID": "w01/slides.html#case-study-military-inequality-leadsto-military-success",
    "href": "w01/slides.html#case-study-military-inequality-leadsto-military-success",
    "title": "Week 1: Science from Particles to People",
    "section": "Case Study: Military Inequality \\(\\leadsto\\) Military Success",
    "text": "Case Study: Military Inequality \\(\\leadsto\\) Military Success\n\nLyall (2020): ‚ÄúTreating certain ethnic groups as second-class citizens [‚Ä¶] leads victimized soldiers to subvert military authorities once war begins. The higher an army‚Äôs inequality, the greater its rates of desertion, side-switching, and casualties‚Äù\n\n\nMatching constructs pairs of belligerents that are similar across a wide range of traits thought to dictate battlefield performance but that vary in levels of prewar inequality. The more similar the belligerents, the better our estimate of inequality‚Äôs effects, as all other traits are shared and thus cannot explain observed differences in performance, helping assess how battlefield performance would have improved (declined) if the belligerent had a lower (higher) level of prewar inequality.\nSince [non-matched] cases are dropped [‚Ä¶] selected cases are more representative of average belligerents/wars than outliers with few or no matches, [providing] surer ground for testing generalizability of the book‚Äôs claims than focusing solely on canonical but unrepresentative usual suspects (Germany, the United States, Israel)"
  },
  {
    "objectID": "w01/slides.html#does-inequality-cause-poor-military-performance",
    "href": "w01/slides.html#does-inequality-cause-poor-military-performance",
    "title": "Week 1: Science from Particles to People",
    "section": "Does Inequality Cause Poor Military Performance?",
    "text": "Does Inequality Cause Poor Military Performance?\n\n\n\n\n\n\n\n\nCovariates\nSultanate of Morocco Spanish-Moroccan War, 1859-60\nKhanate of Kokand War with Russia, 1864-65\n\n\n\n\n\\(X\\): Military Inequality\nLow (0.01)\nExtreme (0.70)\n\n\n\\(\\mathbf{Z}\\): Matched Covariates:\n\n\n\n\nInitial relative power\n66%\n66%\n\n\nTotal fielded force\n55,000\n50,000\n\n\nRegime type\nAbsolutist Monarchy (‚àí6)\nAbsolute Monarchy (‚àí7)\n\n\nDistance from capital\n208km\n265km\n\n\nStanding army\nYes\nYes\n\n\nComposite military\nYes\nYes\n\n\nInitiator\nNo\nNo\n\n\nJoiner\nNo\nNo\n\n\nDemocratic opponent\nNo\nNo\n\n\nGreat Power\nNo\nNo\n\n\nCivil war\nNo\nNo\n\n\nCombined arms\nYes\nYes\n\n\nDoctrine\nOffensive\nOffensive\n\n\nSuperior weapons\nNo\nNo\n\n\nFortifications\nYes\nYes\n\n\nForeign advisors\nYes\nYes\n\n\nTerrain\nSemiarid coastal plain\nSemiarid grassland plain\n\n\nTopography\nRugged\nRugged\n\n\nWar duration\n126 days\n378 days\n\n\nRecent war history w/opp\nYes\nYes\n\n\nFacing colonizer\nYes\nYes\n\n\nIdentity dimension\nSunni Islam/Christian\nSunni Islam/Christian\n\n\nNew leader\nYes\nYes\n\n\nPopulation\n8‚Äì8.5 million\n5‚Äì6 million\n\n\nEthnoling fractionalization (ELF)\nHigh\nHigh\n\n\nCiv-mil relations\nRuler as commander\nRuler as commander\n\n\n\\(Y\\): Battlefield Performance:\n\n\n\n\nLoss-exchange ratio\n0.43\n0.02\n\n\nMass desertion\nNo\nYes\n\n\nMass defection\nNo\nNo\n\n\nFratricidal violence\nNo\nYes"
  },
  {
    "objectID": "w01/slides.html#particularly-fun-non-standard-examples",
    "href": "w01/slides.html#particularly-fun-non-standard-examples",
    "title": "Week 1: Science from Particles to People",
    "section": "Particularly Fun Non-‚ÄúStandard‚Äù Examples",
    "text": "Particularly Fun Non-‚ÄúStandard‚Äù Examples\n\nDeDeo, French Revolution\nGrimmer, Mirrors for Sultans and Princes"
  },
  {
    "objectID": "w01/slides.html#jupyterhub",
    "href": "w01/slides.html#jupyterhub",
    "title": "Week 1: Science from Particles to People",
    "section": "JupyterHub",
    "text": "JupyterHub\n\nhttps://guhub.io\n\n\nImage source"
  },
  {
    "objectID": "w01/slides.html#references",
    "href": "w01/slides.html#references",
    "title": "Week 1: Science from Particles to People",
    "section": "References",
    "text": "References\n\n\nKalyvas, Stathis N. 2006. The Logic of Violence in Civil War. Cambridge University Press. https://www.dropbox.com/scl/fi/4es1r44ldnqtcroonr5g7/The-Logic-of-Violence-in-Civil-War.pdf?rlkey=uonfmw5zkx1ja5l8emv2bzfmg&dl=1.\n\n\nLyall, Jason. 2020. Divided Armies: Inequality and Battlefield Performance in Modern War. Princeton University Press.\n\n\nMontessori, Maria. 1916. Spontaneous Activity in Education: A Basic Guide to the Montessori Methods of Learning in the Classroom. Lulu Press.\n\n\nSperber, Dan. 1996. Explaining Culture: A Naturalistic Approach. Cambridge: Blackwell. https://www.dropbox.com/scl/fi/he6idaajr6vopybv13qhd/Explaining-Culture-A-Naturalistic-Approach.pdf?rlkey=8oyhnsu5ycet4osstft252jaw&dl=1."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to DSAN 5650: Causal Inference for Computational Social Science at Georgetown University!\nThe course meets on Wednesdays from 6:30-9pm online via Zoom",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-staff",
    "href": "syllabus.html#course-staff",
    "title": "Syllabus",
    "section": "Course Staff",
    "text": "Course Staff\n\nProf.¬†Jeff Jacobs, jj1088@georgetown.edu\n\nOffice hours (Click to schedule): Tuesdays, 3:30-6pm\n\nTA Courtney Green, crg123@georgetown.edu\n\nOffice hours by appointment\n\nTA Wendy Hu, lh1078@georgetown.edu\n\nOffice hours by appointment",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThis course provides students with the opportunity to take the analytical skills, machine learning algorithms, and statistical methods learned throughout their first year in the program and explore how they can be employed to carry out rigorous, original research in the behavioral and social sciences. With a particular emphasis on tackling the additional challenges which arise when moving from associational to causal inference, particularly when only observational (as opposed to experimental) data is available, students will become proficient in cutting-edge causal Machine Learning techniques such as propensity score matching, synthetic controls, causal program evaluation, inverse social welfare function estimation from panel data, and Double-Debiased Machine Learning.\nIn-class examples will cover continuous, discrete-choice, and textual data from a wide swath of social and behavioral sciences: economics, political science, sociology, anthropology, quantitative history, and digital humanities. After gaining experience through in-class labs and homework assignments focused on reproducing key findings from recent journal articles in each of these disciplines, students will spend the final weeks of the course on a final project demonstrating their ability to develop, evaluate, and test the robustness of a causal hypothesis.\nPrerequisites: DSAN 5000, DSAN 5100 (DSAN 5300 recommended but not required)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "Syllabus",
    "section": "Course Overview",
    "text": "Course Overview\nThe fundamental building block for the course is the idea of a Data-Generating Process (DGP). You may have encountered this concept in passing during other DSAN courses (for example, in DSAN 5100, a phrase like ‚ÄúAssume \\(X\\) is drawn i.i.d. from a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\)‚Äù is a statement characterizing the DGP of a Random Variable \\(X\\)), but in this course we will ‚Äúzoom in‚Äù on this concept rather than treating it like a black box or a footnote to e.g.¬†a theorem like the Law of Large Numbers.\nThis deep dive into DGPs is necessary for us here, since our goal in the course is to move from associational statements like ‚Äúan increase of \\(X\\) by one unit is associated with an increase of \\(Y\\) by \\(\\beta\\) units‚Äù to causal statements like ‚Äúincreasing \\(X\\) by one unit causes \\(Y\\) to increase by \\(\\beta\\) units‚Äù. As you‚Äôll see in Week 1, the tools from probability theory and statistics that you learned in DSAN 5100‚ÄîRandom Variables, Cumulative Distribution Functions, Conditional Probability, and so on‚Äîare necessary but not sufficient to analyze data from a causal perspective.\nFor example, if we use our tools from DSAN 5000 and DSAN 5100 on some dataset to discover that:\n\nThe probability that some event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\), and\nThe probability that \\(E_1\\) occurs conditional on another event \\(E_0\\) occurring is \\(\\Pr(E_1 \\mid E_0) = 0.75\\),\n\nwe unfortunately cannot infer from these two pieces of information that the occurrence of \\(E_0\\) causes an increase in the likelihood of \\(E_1\\) occurring.\nThis issue (that conditional probabilities could not be interpreted causally) at first represented a kind of dead end for scientists interested in employing probability theory to study causal relationships‚Ä¶ In recent decades, however, researchers have built up what amounts to an additional ‚Äúlayer‚Äù of modeling tools which augment the existing machinery of probability theory to address causality head-on!1\nFor instance, a modeling approach called ‚Äú\\(\\textsf{do}\\)-Calculus‚Äù, that we will learn in this class, extends the core operations and definitions of probability theory to allow such a move towards inferring causality! It does this by introducing a \\(\\textsf{do}(\\cdot)\\) operator that can be applied to Random Variables like \\(X\\), with e.g.¬†\\(\\textsf{do}(X = 5)\\) representing the event wherein someone has intervened in a Data-Generating Process to force the value of \\(X\\) to be 5.\nWith this operator in hand (that is, used alongside an explicit model of a DGP satisfying a set of underlying axioms which are slightly more strict than the axioms of probability theory), it turns out that we can make causal inferences using a very similar pair of facts! If we know that:\n\nThe probability that some event \\(E_1\\) occurs is \\(\\Pr(E_1) = 0.5\\), and\nThe probability that \\(E_1\\) occurs conditional on the event \\(\\textsf{do}(E_0)\\) occurring is \\(\\Pr(E_1 \\mid \\textsf{do}(E_0)) = 0.75\\),\n\nnow we can actually draw the inference that the occurrence of \\(E_0\\) caused an increase in the likelihood of \\(E_1\\) occurring!\nThis stylized comparison (between what‚Äôs possible using ‚Äúcore‚Äù probability theory and what‚Äôs possible when we augment it with additional causal modeling tools) serves as our basic motivation for the course, so that from Week 2 onwards we build upon this foundation to reach the three learning goals described in the next section!",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#main-textbooks-resources",
    "href": "syllabus.html#main-textbooks-resources",
    "title": "Syllabus",
    "section": "Main Textbooks / Resources",
    "text": "Main Textbooks / Resources\nUnlike the case for topics like calculus or statistical learning, this field is too new (and exciting! with new methods being developed month-to-month) to have a single set of ‚Äúestablished‚Äù textbooks. Thus, the main collection of resources (books, papers, and explanatory videos) we‚Äôll draw on for this class are available on the resources page. However, there are three ‚Äúcore‚Äù textbooks you can draw on which best align with the topics in this course:\n\nMorgan and Winship, Counterfactuals and Causal Inference: Methods and Principles for Social Research (Morgan and Winship 2015) [PDF]\n\nThe book which comes closest to being an all-encompassing, single textbook for the class. It brings together different ‚Äústrands‚Äù of causal modeling research (since each field‚Äîeconomics, bioinformatics, sociology, etc.‚Äîtends to use its own notation and vocabulary), unifying them into a single approach. The only reason we can‚Äôt use it as the main textbook is because it hasn‚Äôt been updated since 2015, and most of the assignments in this class use computational tools from 2018 onwards!\n\nAngrist and Pischke, Mastering ‚ÄôMetrics: The Path from Cause to Effect (Angrist and Pischke 2014) [PDF]\n\nThis book is included as the second of the three ‚Äúcore‚Äù texts mainly because, it uses the language of causality specific to Econometrics, the language that is most familiar to me from my PhD training in Political Economy. However, if you tend to learn better by example, it also does a good job of foregrounding specific examples (like evaluating charter schools and community policing policies), so that the methods emerging naturally from attempts to solve these puzzles when association methods like linear regression fail to capture their causal linkages.\n\nPearl and Mackenzie, The Book of Why: The New Science of Cause and Effect (Pearl and Mackenzie 2018) [EPUB]\n\nThis book contrasts with the Angrist and Pischke book in using the language of causality formed within Computer Science rather than Economics. It can be a good starting point especially if you‚Äôre unfamiliar with the heavy use of diagrams for scientific modeling‚Äîbasically, whereas Angrist and Pischke‚Äôs first instinct is to use (sometimes informal) equations like \\(y = mx + b\\) to explain steps in the procedures, Pearl and Mackenzie‚Äôs instinct would be to instead use something like \\(\\require{enclose}\\enclose{circle}{\\kern .01em ~x~\\kern .01em} \\overset{\\small m, b}{\\longrightarrow} \\enclose{circle}{\\kern.01em y~\\kern .01em}\\) to represent the same concept (in this case, a line with slope \\(m\\) and intercept \\(b\\)!).",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule\nThe following is a rough map of what we will work through together throughout the semester; given that everyone learns at a different pace, my aim is to leave us with a good amount of flexibility in terms of how much time we spend on each topic: if I find that it takes me longer than a week to convey a certain topic in sufficient depth, for example, then I view it as a strength rather than a weakness of the course that we can then rearrange the calendar below by adding an extra week on that particular topic! Similarly, if it seems like I am spending too much time on a topic, to the point that students seem bored or impatient to move onto the next topic, we can move a topic intended for the next week to the current week!\n\n\n\nUnit\nWeek\nDate\nTopic\n\n\n\n\nUnit 1: The Language of Causal Modeling\n1\nMay 21\nFrom a Science of Particles to a Science of PeopleRelevant Supplementary Material:Santa Fe Institute online course: Introduction to Renormalization, up through Video 5, ‚ÄúCoarse Graining II: Entropy‚Äù2\n\n\n\n2\nMay 28\nProbabilistic Graphical Models (PGMs) as Data-Generating Processes (DGPs)Relevant Reading:Koller and Friedman (2009), Chapter 2: ‚ÄúFoundations‚Äù. Especially pp.¬†15-34, as a DSAN 5100 refresher!\n\n\nUnit 2: Doin Thangs\n3\nJun 4\nFrom PGMs to Causal Diagrams[Deliverable, 11:59pm EDT] HW1: Causal Modeling with DAGs and PGMs: Direct and Indirect Effects\n\n\n\n4\nJun 11\nClearing the Path from Cause to Effect\n\n\nUnit 3: Matching Apples to Apples\n5\nJun 18\nMultilevel Modeling, Closing Backdoor Paths\n\n\n\n6\nJun 25\nMidterm Review (feat. Example Problems)\n\n\nMidterm\n7\nJul 2\nIn-Class Midterm: Causal Models and Statistical Matching\n\n\nUnit 4: Machine Learning for Causal Inference\n8\nJul 9\nPropensity Score Matching\n\n\n\n9\nJul 16\nDouble-Debiased Machine Learning\n\n\n\n10\nJul 23\nCausal Forests for Heterogeneous Treatment Effects\n\n\n\n11\nJul 30\nFancier Causal Inference at the Fancy Computational Methods Frontier\n\n\nFinal Project Zone\n12\nAug 6\nFinal Project Presentations\n\n\n\n\nAug 8 (Friday), 5:59pm EDT\n[Deliverable] Final Project",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assignments-and-grading",
    "href": "syllabus.html#assignments-and-grading",
    "title": "Syllabus",
    "section": "Assignments and Grading",
    "text": "Assignments and Grading\nThe main assignment in the course will be your final project, submitted at the end of the semester. However, there will also be a (virtual) in-class midterm exam and a series of assignments which exist to let you explore each of the modules of the course, in turn.\n\n\n\n\n\n\n\n\nAssignment\nDue Date\n% of Grade\n\n\n\n\nHW1: Causal Modeling with DAGs and PGMs: Direct and Indirect Effects\nMonday, June 9\n9%\n\n\nHW2: Traversing the Sea of Causality with the do-Harpoon in Hand: Chains, Forks, and Colliders\nFriday, June 13\n9%\n\n\nHW3: Statistical Matching: Apples to Apples and Oranges to Oranges\nFriday, June 27\n9%\n\n\nIn-Class Midterm\nWednesday, July 2\n25%\n\n\nHW4: Basic Computational Causal Methods\nFriday, July 11\n9%\n\n\nHW5: Fancy Computational Causal Methods\nFriday, July 25\n9%\n\n\nFinal Project\nFriday, August 15\n30%\n\n\n\n\nHomework Lateness Policy\nAfter the due date, for each assignment besides the midterm, you will have a grace period of 24 hours to submit the assignment without a lateness penalty. After this 24-hour grace period, late penalties will be applied based on the following scale (unless you obtain an excused lateness from one of the instructional staff!):\n\n0 to 24 hours late: no penalty\n24 to 30 hours late: 2.5% penalty\n30 to 42 hours late: 5% penalty\n42 to 54 hours late: 10% penalty\n54 to 66 hours late: 20% penalty\nMore than 66 hours late: Assignment submissions no longer accepted (without instructor approval)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPearl (2000) represents a key work in this field of research, as it essentially brought together different pieces of causal models into one unified, rigorous framework.‚Ü©Ô∏é\nChallenge yourself to keep watching to the end of the 5th video here, if you can! Even if you feel frustrated/scared! Because, the examples (e.g., macro vs.¬†microeconomics) are what we mainly care about here, more so than e.g.¬†the mathematical definition of entropy (which we will go towards at a slower pace!)‚Ü©Ô∏é",
    "crumbs": [
      "Syllabus"
    ]
  }
]